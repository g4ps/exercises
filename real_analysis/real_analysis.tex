\documentclass[11pt,oneside,titlepage]{book}
\title{My real analysis exercises}
\usepackage{amsmath,amssymb}
\usepackage{geometry}
\usepackage{hyperref}
\author{Evgeny Markin}
\date{2022}

\begin{document}
\maketitle
\tableofcontents

\chapter*{Preface}

Exercises are from UTM-040 Understanding analysis by Stephen Abbott, 1st
edition. I don't own any rights
for the book and I am not planning to aquire them anytime soon. 

Also it wouldn't hurt to mention, that some (or maybe even most) of exercises
were not proof-read after they were written,  and that none of them were
evaluated by anyone who matters. There are a lot of mistakes out  there, both
in mathematical and logical parts, and in the language they were written as
well. I am doing this kind of stuff for fun, so cut me some slack.

\chapter{Real Numbers}

\section*{1.2.1}
\textit{(a) Prove that $\sqrt{3}$ is irrational. Does a simular argument work
  to show $\sqrt{6}$ is irrational?}

Suppose that $\sqrt{3}$ is a rational number; then it is true that
$$\exists m \in \textbf{Z}, n \in \textbf{N}: \frac{m}{n} = \sqrt{3}$$
where $m$ and $n$ are at their lowest possible terms. Then
$$\sqrt{3}n = m$$
$$3n^2 = m^2$$

Therefore we can state, that $m \% 3 = 0$. Therefore $\exists k: 3k = m$.
Thus we can reformulate formula as
$$3n^2 = (3k)^2$$
$$n^2 = 3k^2$$

Therefore $n\%3 = 0$ as well. Therefore $n$ and $k$ are not in their possible
terms, which conradicts our initial assumtions. Therefore we can state that
$\sqrt{3} \notin \textbf{Q}$.

Let's try the same argument for $\sqrt{6}$.
$$\exists m \in \textbf{Z}, n \in \textbf{N}: \frac{m}{n} = \sqrt{6}$$
$$\sqrt{6}n = m$$
$$6n^2 = m^2$$
then m has as their dividers both 2 and 3. Therefore $m\%2 = 0$ and $m\%3 = 0$.
Therefore we can proceed with the same argument as earlier
$$6n^2 = (6k)^2$$
$$n^2 = 6k^2$$

Therefore n is divided by 6, etc., etc., $\sqrt{6} \notin \textbf{Q}$.

\textit{(b) Where does the proof of Theorem 1.1.1 break down if we try to use
  it to prove $\sqrt{4}$ is irrational? }

Suppose that $\sqrt{4}$ is a rational number; then it is true that
$$\exists m \in \textbf{Z}, n \in \textbf{N}: \frac{m}{n} = \sqrt{2}$$
where $m$ and $n$ are at their lowest possible terms. Then
$$\sqrt{4}n = m$$
$$4n^2 = m^2$$

n can still be odd and m can still be even. In other words, m is divisible by
a prime, and the number under the radical consists of two primes. Therefore
if a number decomposes to two equal sets of primes, then its square root is
a rational number. Otherwise it isn't.

\section*{1.2.2}

\textit{Decide which of the following represent true statements about the
  nature of sets. For any that are false, provide a specific exapmple where the
  statement in question does not hold.}

\textit{(a) if $A_1 \supseteq A_2 \supseteq A_3 \supseteq ... $ are all sets
  containing an infinite number of elements, then the intersection
  $\cap_{n = 1}^{\infty} A_n$ is infinite as well.} -

$\cap_{n = 1}^{\infty} A_n = (0, 1/n)$ has no numbers in it.

Proof is easy -

$$\forall x \in \textbf{R} > 0: \exists n \in N: 1/n < x$$

\textit{(b) if $A_1 \supseteq A_2 \supseteq A_3 \supseteq ... $ are all finite,
  nonempty sets of real numbers, then the intersection
  $\cap_{n = 1}^{\infty} A_n$ is finite and nonempty.} -

True.

There is no need for the proof, but I'll supply one anyways. If all $A_n$ are
finite and nonempty, then $\exists j \in \textbf{N} : |A_1| = j$. Therefore,
because of the same reasons, there are only $j - 1$ times when

$$A_k \supset A_{k + 1}$$

can happen, because after $j - 1$ times the set will be empty. Therefore,
because it is finite, their intersection will have finite number of
elements and will be non-empty.

\textit{(c) $A \cap (B \cup C) = (A \cap B) \cup C$}

False: let

$$x \notin A, x \notin B, x \in C$$
Then
$$x \in A \cap (B \cup C); x \notin = (A \cap B) \cup C$$

\textit{(c) $A \cap (B \cap C) = (A \cap B) \cap C$}

True. Kinda goes without a proof; if you imagine a Vien diagram, then it's
obvious.

\textit{(c) $A \cap (B \cup C) = (A \cap B)  \cup (A \cap C)$}

True. For the same reason as before.

I'm sure that there exist more concrete versions of those proofs, but I'm not
required to provide any. My suspition on why is it so, is because it's a
little more complicated and requires more knowlege in set theory and/or logic.

\section*{1.2.3 (De Morgan's Laws).}

\textit{Let A and B be subsets of \textbf{R}}

\textit{(a) If $x \in (A \cap B)^c$, explain why $x \in A^c \cup B^c$. This
  shows that $(A \cap B)^c \subseteq (A \cap B)^c$.}

If we have two sets $A$ and $B$, then \textbf{R} desintegrates into 4 different
sets: $A$, $B$, $A^c$, $B^c$.

Therefore there must exists sets
$$S_1 = A \cap B$$
$$S_2 = A^c \cap B$$
$$S_3 = A \cap B^c$$
$$S_4 = A^c \cap B^c$$

An element cannot be in the set and not in the set at the same time. Therefore,
there does not exist an element, which is in two of $S_n$'s.

For any $x \in \textbf{R} \to x \in A$ or $x \notin A$. Therefore an element
of $\textbf{R}$ needs to be in at least one of those sets. It is easily seen by
$$A \cap \textbf{R} = A$$
$$A \cap (B \cup B^c) = A$$
$$(A \cap B) \cup (A \cap B^c)) = A$$

Therefore $\cup_{n = 1}^4 S_n = \textbf{R}$ and
$\cap_{n = 1}^4 S_n = \emptyset$.

Suppose $x \in (A \cap B)^c$. Then $x \notin A \cap B$. Therefore
$x \in S_2 \cup S_3 \cup S_4$.

Suppose that $x \in A^c \cup B^c$. Then $x \in S_2 \cup S_3 \cup S_4$.

Therefore $ (A \cap B)^c \subseteq A^c \cup B^c$.

\textit{(b) Prove the reverse inclusion}


As seen in part (a)

$$(A \cap B)^c = S_2 \cup S_3 \cup S_4 = A^c \cup B^c$$

\textit{(c) Show $(A \cup B)^c = A^c \cap B^c$ by demonstrating inclusion both
  ways.}

No need to do both ways.
$$(A \cup B)^c = S_4 = A^c \cap B^c$$


\section*{1.2.4}
\textit{Verify the triangle inequality in the special cases where }

\textit{(a) $a$ and $b$ have the same sign}

Suppose $a \geq 0$, $b \geq 0$. Then $|a| = a$ and $|b| = b$. Therefore
$$|a + b| = a + b = |a| + |b| \leq  |a| + |b|$$

Suppose $a < 0$, $b < 0$. Then $|a| = -a$ and $|b| = -b$; also $a + b < 0 \to
|a + b| = -(a + b) = -a - b$. Therefore 
$$|a + b| = -a + (-b) = |a| + |b| \leq  |a| + |b|$$.

\textit{(b) $a \geq 0$, $b < 0$ and $a + b \geq 0$.}

$$a + b \geq 0 \to a + b = |a + b|$$

Also, $|a| = a$ and $|b| = -b$. Therefore
$$a + b \geq 0 \to a \geq -b \to a \geq |b| \to |a| \geq |b|$$

$$ b < 0$$
$$ b \leq 0$$
$$ 2b \leq 0$$
$$ b + b \leq 0$$
$$ b \leq (-b)$$
$$a + b \leq a + (-b)$$
$$|a + b| \leq |a| + |b|$$

\section*{1.2.5}
\textit{Use the triangle inequality to establish the inequalities}

\textit{(a) $|a - b| \leq |a| + |b|;$}
$$|a - b| = |a + (-b)| \leq |a| + |-b| = |a| + |b|$$

\textit{(b) $||a| - |b|| \leq |a - b|$;}

let $a = a + b - b$. Then

$$|a| = |a - b + b| \leq |a - b| + |b|$$
$$|a| - |b| \leq |a - b|$$
$$|b| = |b - a + a| \leq |b - a| + |a| = |a - b| + |a|$$
$$|b| - |a| \leq |a - b|$$
$$|a| - |b| \geq -|a - b|$$
$$-|a - b| \leq |a| - |b| \leq |a - b| \to ||a| - |b|| \leq |a - b|$$

\section*{1.2.6}
\textit{Given a function $f$ and a subset $A$ of its domain, let $f(A)$
  represent the range of $f$ over the set $A$; that is,
  $f(A) = \{f(x): x \in A\}$. }

\textit{(a) Let $f(x) = x^2$. if $A=[0,2]$ (the closed interval $\{x \in
  \textbf{R}: 0 \leq x \leq 2\}$) and $B=[1,4]$, find $f(A)$ and $f(B)$. Does
  $f (A \cap B) = f(A) \cap f(B)$ in this case? Does $f(A \cup B) = f(A) \cup
  f(B)$? }

First things first: $f(A) = [0, 4]$; $f(B) = [1, 16]$ (without any proof
because if we don't go with axiomatic stuff, then it is obvious).

$$f(A \cap B) = f([1, 2]) = [1, 4]$$
$$f(A) \cap f(B) = [1, 4]$$

Therefore in this case $f(A) \cap f(B) = f(A \cap B)$.

$$f(A \cup B) = f([0, 4]) = [0, 16] = f(A) \cup f(B)$$

\textit{(b) Find two sets $A$ and $B$ for which $f(A \cap B) \neq f(A) \cap f(B)$.}

Let $A = [-1, 0]$ and $B = [0, 1]$. Then
$$f(A \cap B) = f(\{0\}) = \{0\}$$
$$f(A) \cap f(B) = [0, 1] \cap [0, 1] = [0, 1] \neq f(A \cap B)$$

\textit{(c) Show that, for an arbitrary function $g: \textbf{R} \to \textbf{R}$, it
  is always true that $g(A \cap B) \subseteq g(A) \cap g(B)$ for all sets
  $A,B \subseteq \textbf{R}$.}

$$x \in g(A \cap B) \to x \in g(A)$$
$$x \in g(A \cap B) \to x \in g(B)$$
Therefore
$$x \in g(A \cap B) \to x \in g(A) \cap g(B)$$
Thus
$$g(A \cap B) \subseteq g(A) \cap g(B)$$

\textit{(d) Form and prove a conjecture aout the relationship between $g(A \cup B)$ and $g(A) \cup g(B)$ for an arbitrary function $g$.}

$$x \in g(A) \to x \in g(A \cup B)$$
$$x \in g(B) \to x \in g(A \cup B)$$

Therefore
$$x \in g(A) \cup g(B) \to x \in g(A \cup B)$$
Thus
$$g(A) \cup g(B) \subseteq g(A \cup B)$$

Suppose that

$$\exists y \in \textbf{R}: y \in g(A \cup B); y \notin g(A) \cup g(B)$$

Then $\exists q_1 \in A \cup B: g(q_1) = y$ but
$$\forall q_2 \in A, q_3 \in B: g(q_2) \neq y; g(q_3) \neq y$$

Therefore $q_1 \notin A$ and $q_1 \notin B$. Therefore $q_1 \in A^c \cap B^c$.
Using De Morgan rule
$$q_1 \in A^c \cap B^c \to q_1 \in (A \cup B)^c$$
therefore 
$$q_1 \notin g(A \cup B)$$
which is a contradiction. Therefore

$$y \in g(A \cup B) \to g(A) \cup g(B)$$
Thus
$$g(A \cup B) \subseteq g(A) \cup g(B)$$

Therefore if we take into account previous conclusion

$$g(A \cup B) = g(A) \cup g(B)$$

for any $g$.

\section*{1.2.7}

\textit{Given a function $f: D \to \textbf{R}$ and a subset $B \subseteq \textbf{R}$, let $f^{-1}(B)$ be the set of all points from the domain $D$ that get mapped into $B$; that is, $f^{-1}(B) = \{x \in D: f(x) \in B\}$. This is called
  the preimage of $B$.}

\textit{(a) Let $f(x) = x^2$. If A is the closed interval $[0,4]$ and B is the
  closed interval $[-1, 1]$, find $f^{-1}(A)$ and $f^{-1}(B)$. Does
  $f^{-1}(A \cap B) = f^{-1}(A) \cap f^{-1}(B)$ in this case? Does
  $f^{-1}(A \cup B) = f^{-1}(A) \cup f^{-1}(B)$?
}

$$f^{-1}(A) = [-2, 2]$$
$$f^{-1}(B) = [-1, 1]$$
$$f^{-1}(A \cap B) = f^{-1}([0, 1]) = [-1, 1] = f^{-1}(A) \cap f^{-1}(B)$$
$$f^{-1}(A \cup B) = f^{-1}([-1, 4]) = [-2, 2] = f^{-1}(A) \cup f^{-1}(B)$$

\textit{(b) The good behaviour of preimages demonstated in (a) is completely
  general. Show that for an arbitrary function $g: \textbf{R} \to \textbf{R}$,
  it is always true that $g^{-1}(A \cap B) = g^{-1}(A) \cap g^{-1}(B)$ and
  $g^{-1}(A \cup B) = g^{-1}(A) \cup g^{-1}(B)$ for all sets $A, B \subseteq \textbf{R}$.
}

% $$ x_1 \in g^{-1}(A) \to \exists y_1 \in A: g(y) = x$$
% $$ x_2 \in g^{-1}(B) \to \exists y_2 \in B: g(y) = x$$
By definition
$$ x \in g^{-1}(A \cap B) \to \exists y \in A \cap B: y = g(x)$$
Therefore if we we use the fact  $y \in A \cap B \to y \in A$ and $y \in A \cap B \to y \in B$
$$ x \in g^{-1}(A \cap B) \to \exists y \in A: y = g(x) \to x \in g^{-1}(A)$$
$$ x \in g^{-1}(A \cap B) \to \exists y \in B: y = g(x) \to x \in g^{-1}(B)$$
therefore $x \in g^{-1}(A \cap B)$ implies that $x \in g^{-1}(A)$ and $x \in g^{-1}(B)$, or in other words
$$g^{-1}(A \cap B) \subseteq g^{-1}(A) \cap g^{-1}(B)$$

In other direction: 
$$x \in g^{-1}(A) \to \exists y_1 \in A: y_1 = g(x)$$
$$x \in g^{-1}(B) \to \exists y_2 \in B: y_2 = g(x)$$

$x \in g^{-1}(A) \cap g^{-1}(B)$ implies that $ \exists y_1 \in A: g(x) = y_1$ and $\exists y_2 \in B: y_2 = g(x)$. Because $g$ is a function we know, that
for every $x$ there exists only one $y = g(x)$. Therefore $y_1 = y_2 = g(x)$.
Thus we can state that $y \in A \cap B$.

thus
$$x \in g^{-1}(A) \cap g^{-1}(B) \to \exists y \in A \cap B: y = g(x) \to
x \in g^{-1}(A \cap B)$$

Therefore 

$$ g^{-1}(A) \cap g^{-1}(B) \subseteq   g^{-1}(A \cap B)$$

If we take previous conclusion into account, then it follows that
$$ g^{-1}(A) \cap g^{-1}(B) = g^{-1}(A \cap B)$$
as desired.

Now let's prove that  $g^{-1}(A \cup B) = g^{-1}(A) \cup g^{-1}(B)$:


If $ x \in g^{-1}(A) \cup g^{-1}(B))$ then  $\exists y \in A: y = g(x)$ or
$\exists y \in B: y = g(x)$. If we take into account that $y \in A \to y \in A \cup B$ then we can conclude that

$$ x \in g^{-1}(A) \cup g^{-1}(B)) \to \exists y \in A \cup B : y = g(x) \to
x \in g^{-1}(A \cup B)$$
Thus

$$g^{-1}(A) \cup g^{-1}(B) \subseteq g^{-1}(A \cup B)$$

In other direction: 
$$x \in g^{-1}(A \cup B) \to \exists y \in A \cup B: y = g(x) \to y = g(x)$$

As proven before $g(A \cup B) = g(A) \cup g(B)$ and therefore
$y \in g(A \cup B)$ implies that either $y \in g(A)$ or $y \in g(B)$. Therefore
$$ x \in g^{-1}(A \cup B) \to x \in g^{-1}(A) \cup g^{-1}(B)$$
$$ g^{-1}(A \cup B)  \subseteq g^{-1}(A) \cup g^{-1}(B)$$
And if we combine this fact with previous conclusion:
$$ g^{-1}(A \cup B) =  g^{-1}(A) \cup g^{-1}(B)$$
as desired.

\section*{1.2.8}

\textit{Form the logical negation of each claim. One way to do this is to
  simply add "It is bot the case that ... " in front of each assertion, but for
  each statement, try to embed the word "not" as deeply into the resulting
  sentence as possible (or avoid using it altogether).}

\textit{(a) For all real numbers satisfying $a < b$, there exist an $n \in
  \textbf{N}$ such that $a + 1/n < b$.}
There exist real numbers $a < b$ such that $a + 1/n \geq b$ for all $n \in \textbf{N}$.

\textit{(b) Between every two distinct real numbers, there is a rational number}

There exist two real numbers, such that there are only irrational numbers
between them.

\textit{(c) For all natural numbers $n \in \textbf{N}$, $\sqrt{n}$ is either a
  natural number  or an irrational number}

There exist a natural number $n \in \textbf{N}$, such that $\sqrt{n}$ is a
rational number, that is not a natural number. (This one is a little bit weird
if we try to negate this, but "not" is stuffed as deep as possible)

\textit{(d) Given any real number $x \in \textbf{R}$, there exist $n \in
  \textbf{N}$ satistying $n > x$}

There exist a real number $x \in \textbf{R}$, such that for all $n \in \textbf{N}$ it is true that $n \leq x$.

\section*{1.2.9}
\textit{Show that the sequence $(x_1, x_2, x_3,...)$ defined in Example 1.2.7
  is bounded above by 2; that is, prove that $x_n \leq 2$ for every $n \in
  \textbf{N}$.}

The mentioned sequence is defined by
$$x_1 = 1$$
$$x_{n + 1} = (1/2)x_n + 1$$

A great advice about induction states, that when you hear words "prove" and
"sequence" in the same sentence, then the word "induction" should pop up in
your head. So here we go

Base case: $x_1 \leq 2$.

Inductive proposition: $x_n \leq 2$

Inductive step:
$$x_n \leq 2$$
$$(1/2)x_n \leq 1$$
$$(1/2)x_n + 1 \leq 2$$
$$x_{n + 1} = (1/2)x_n + 1 \leq 2$$
$$x_{n + 1} \leq 2$$
as desired.

\section*{1.2.10}
\textit{Let $y_1 = 1$, and for each $n \in \textbf{N}$ define $y_{n + 1} =
  (3 * y_n + 4) / 4$.}

\textit{(a) Use induction to prove that the sequence satisfies $y_n < 4$ for
  all $n \in \textbf{N}$.}

Base case: $y_1 = 1 < 4$

Inductive proposition: $y_n < 4$

Inductive step:

$$y_n < 4$$
$$3 * y_n < 12$$
$$3 * y_n + 4 < 16$$
$$(3 * y_n + 4) / 4 < 4$$
$$y_{n + 1} =  (3 * y_n + 4) / 4 < 4$$
$$y_{n + 1} < 4$$
as desired.

\textit{(b) Use another induction to show the sequence $(y_1, y_2, y_3,...)$
  is increasing}

We need to show that $y_{n + 1} - y_n \geq 0$

As shown earlier 
$$y_n < 4$$
therefore
$$ y_n < 4$$
$$ \frac{y_n}{4} < 1$$
$$1 > \frac{y_n}{4}$$
$$1 - \frac{y_n}{4} > 0$$
$$1 + (\frac{3 y_n}{4} - y_n) > 0$$
$$\frac{3 y_n}{4} + 1 - y_n > 0$$
$$(3 * y_n + 4) / 4 - y_n  > 0$$
$$y_{n + 1} - y_n  > 0$$
as desired.

\section*{1.2.11}
\textit{If a set $A$ contains $n$ elements, prove that the number of
  different subsets of $A$ is equal to $2^n$. (Keep in mind that the empty set
  $\emptyset$ is considered to be a subset of every set.)}

This proof is dumb, but intuitive:

Every subset is corresponding to a number in binary system: 0 for excluded,
1 for included. Therefore there exist $2^n$ possible combinations.

For a more concrete proof let's resort to induction.

Base case(s): subsets of $\emptyset$ are $\emptyset$ itseft ($2^0 = 1$ in total). Subsets of
set with one element are $\emptyset$ and set itself ($2^1 = 1$ in total).

Proposition is that set with n elements has $2^n$ subsets.

Inductive step is that for set with $n + 1$ elements can either have or hot
have the $n + 1$'th element. Therefore there exist $2^n + 2^n = 2 * 2^n =
2^{n + 1}$ subsets, as desired.

\section*{1.2.12}
\textit{For this exerice, assume Exercise 1.2.3 has been successfully completed
  (as it was)}

\textit{(a) Show how induction can be used to conclude that }
$$ (A_1 \cup A_2 \cup ... \cup A_n) = A^c_1 \cap A^c_2 \cap ... \cap A^c_n$$

First of all, base case

$$ (A_1 \cup A_2)^c = A^c_1 \cap A^c_2 $$

Proposition
$$ (A_1 \cup A_2 \cup ... \cup A_n) = A^c_1 \cap A^c_2 \cap ... \cap A^c_n$$

Step
$$ (A_1 \cup A_2 \cup ... \cup A_n \cup A_{n + 1}) = A^c_1 \cap A^c_2 \cap ... \cap A^c_n \cap A^c_{n + 1}$$

Let us denote $Q = A^c_1 \cap A^c_2 \cap ... \cap A^c_n$. Then by inductive
proposition

$$  A^c_1 \cap A^c_2 \cap ... \cap A^c_n \cap A^c_{n + 1} = Q \cap A^c_{n + 1}
= (Q^c \cup A_{n + 1}) = (A_1 \cup A_2 \cup ... \cup A_n \cup A_{n + 1})
$$
as desired.

\textit{(b) Explain why induction cannot be used to conclude }
$$(\cup^{\infty}_{n = 1}A_n)^c = \cap^{\infty}_{n = 1}A^c_n$$
\textit{It might be useful to consider part (a) of Exercise 1.2.2.}

Induction cannot be used on this one, because for induction we need finite
set of elements. This stands on the fact, that if induction works for
some case, then it works for $n - 1$'th element, and for $n - 2$'th element
and this way all the way down to the base case. As an example of why it doesn't
work, we can take into accound exercise 1.2.2, where all of the elements have
infinite number of elements, and their intersection would have infinite amount
of elements  for any finite number of elements, but it is not true for the
infinite amount.

\textit{(c) Is the statement in part (b) valid? If so, write a proof that does
  not use induction.}

Suppose that
$$x \in (U^{\infty}_{n = 1}A_n)^c$$

Then $x \notin A_n$ for every $n \in \textbf{N}$. Therefore $x \in A^c_n$ for every $n \in \textbf{N}$. Therefore $x \in \cap^{\infty}_{n = 1}A^c_n$. Thus

$$(\cup^{\infty}_{n = 1}A_n)^c \subseteq \cap^{\infty}_{n = 1}A^c_n$$

Suppose that $x \in \cap^{\infty}_{n = 1}A^c_n$. Then $x \notin A_n$ for every $n \in \textbf{N}$. Therefore $x \notin (U^{\infty}_{n = 1}A_n)$ for every $n \in \textbf{N}$. Therefore $x in \cup^{\infty}_{n = 1}A_n)^c$ for every $n \in \textbf{N}$. Therefore
$$ \cap^{\infty}_{n = 1}A^c_n \subseteq  (\cup^{\infty}_{n = 1}A_n)^c$$

Thus if we combine two statements
$$(\cup^{\infty}_{n = 1}A_n)^c = \cap^{\infty}_{n = 1}A^c_n$$
as desired.

\section*{1.3.1}
\textit{Let $\textbf{Z}_5 = \{0, 1, 2, 3, 4, 5\}$ and define addition and
  multiplication modulo 5. In other words, compute the integer remainder when
  $a + b$ and $ab$ are divided by 5, and use this as the value for the sum and
  product, respectively.}

\textit{(a) Show that, given any element $z \neq 0$ in $\textbf{Z}_5$, there
  existsan element $y$ such that $z + y = 0$. The element $y$ is called the
  additive inverse of $z$.}

It is true, that for every of those elements we can set $y = 5 - z$, for which
it is true that $z + y = 5$ and $(z + y)\%5 = 0$ as desired.

\textit{(b) Show that, given any element $z \neq 0$ in $\textbf{Z}_5$, there
  existsan element $x$ such that $zx = 1$. The element $x$ is called the
  multiplicative inverse of $z$.}

$$ (1 * 1) \% 5 = 1 $$
$$ (2 * 3) \% 5 = 1 $$
$$ (3 * 2) \% 5 = 1 $$
$$ (4 * 4) \% 5 = 1 $$
as desired.

\textit{(c) The existence of additive and multiplicative inverses is part of
  the definition of a field. Investigate the set $\textbf{Z}_4 = \{0, 1, 2,
  3\}$ (where addition and multiplication are defined modulo 4) for the
  existense of additive and multiplicative inverses. Make a conjecture about
  the values of n for which additive inverses exist in $\textbf{Z}_n$, and then
  form another conjecture about the existence of multiplicative inverses.}

For $\textbf{Z}_4$ we define additive inverse the same way we defined it
in the  part (a) ($4 - z = y$). For multiplicative inverse we have the way
with $1$, but any for 2 we don't have multiplicative inverse.

Therefore the conjecture about the additive inserse is that every $Z_n$ with
addition defined as addition modulo n we have additive inverse.

Proof of it is that every element of $\textbf{Z}_n$  is less than n, and that
because of this there exists $s = n - x \in \textbf{Z}_n$

For multiplicative inverse the conjecture is that it exists only when
n is a prime number.

\section*{1.3.2}
\textit{(a) Write a formal definition in the style of Definition 1.3.2 for the
  infinum or greatest lower bound of a set}

A real number $s$ is the \textit{greatest lower bound} for a set
$A \subseteq \textbf{R}$ if it meets the following two criteria:

(i) $s$ is a lower bound

(ii) if $b$ is any lower bound for $A$, then $s \geq b$.

\textit{(b) Now, state and prove a version of Lemma 1.3.7 for greatest lower
  bounds.}

\textbf{Lemma for greatest lower bounds}

Assume $s \in \textbf{R}$ is a lower bound for a set  $A \subseteq \textbf{R}$.
Then, $s = inf(A)$ if and only if, for every choice of $\epsilon > 0$, there
exists an element $a \in A$ satisfying $s + \epsilon > a$.

Proof:

In one direction: Suppose $s = inf(A)$. Then, by definition of greatest lower
bound, there does not exist a lower bound, greater than $s$. In other words,
suppose that there exist $\epsilon > 0$ such that there is no element $a \in A$
such that $a < s + \epsilon$. Then $s + \epsilon$ is a lower bound, which is
greater than s, therefore s is not a greatest lower bound, which is a
contradiction. Therefore there does not exist $\epsilon > 0$ for which
there exist no $a \in A$ such that $a < s + \epsilon$. Therefore for
every $\epsilon > 0$ there exist an $a \in A$  such that $a < s + \epsilon$,
as desired.

In other direction: suppose that $s$ is a lower bound and for every $\epsilon
> 0$ there exist $a \in A$ such that $a < s + \epsilon$. Then any number
$s + \epsilon$ is not a lower bound. Therefore any number, which is greater
than $s$ is not a lower bound. Therefore any lower bound is less or equal to
$s$. Therefore $s$ is a greatest lower bound.

Maybe this proof is a little bit more coplicated, than it should be, but
at least every step is followed properly.

\section*{1.3.3}
\textit{(a) Let $A$ be bounded below, and define $B = \{b \in \textbf{R}:
  b $ is a lower bound for $A\}$. Show that $sup(B) = inf(A)$.}

$B$ is a set, therefore Axiom of Completeness states that there exist real
number $k = sup(B)$. Therefore all lower bounds are less or equal to $k$.

In order to prove that $k$ is infinum, we need to show that it is a lower
bound.

We do it by contradiction: suppose that $k$ is not a lower bound. Therefore
there exists $a \in A$ such that $k > a$. Let $\epsilon = k - a > 0$. Then,
because $k = sup(B)$ there exist $b \in B$ such that $k - \epsilon < b$.
Therefore $a < b$. Therefore there exist an element of $A$, that is less than
lower bound of $A$. Therefore $b$ is not a lower bound. Therefore we have a
contradiction. Thus $k$ is a lower bound.

Because $k$ is a lower bound, $k \in B$ by definition of $B$. Therefore it is
a lower bound, that is greater or equal to any other lower bounds, because it
is a supremum of $B$. Therefore it is an infinum of $A$ by definition of
infinum.

\textit{(b) Use (a) to explain why there is no need to assert that greatest
  upper bound exist as part of the Axiom of Completeness.}

Proof of part (a) does not take into account the fact, that $A$ (that is
bounded below) has an infinum. We prove its existence of the infinum by
the fact, that we have a set of lower bounds (which is in its turn
has a supremum by Axiom of Completeness), and setting the fact, that its
supremum is lower bound itself  and therefore proving that it is in the set of
lower bounds and therefore setting the fact, that it exists.

\textit{(c) Propose another way to use the Axiom of Completeness to prove
  that sets bounded below have greatest lower bounds.}

The only idea, that goes into my mind is to create set $B = \{-a: a \in A\}$.
Then it'll have a supremum, for which the inverse will be the infinum of the
set. We can polish this idea with some theorems and axioms, but I'm satisfied
with the current proof already, and nobody is requiring it.

\section*{1.3.4}
\textit{Assume that $A$ abd $B$ are nonempty, bounded above, and satisfy
  $B \subseteq A$. Show $sup(B) \leq sup(A)$.}

We prove it by contradiction: let $B \subseteq A$ and
$$sup(B) > sup(A)$$

Then let $\epsilon = sup(B) - sup(A) > 0$. Then by lemma we have

$$b \in B: b > sup(B) - \epsilon$$
$$b \in B: b > sup(B) - sup(B) + sup(A)$$
$$b \in B: b > sup(A)$$

Therefore $b > sup(A)$, which is an upper bound for $A$ and by extension
$\forall a \in A: b > a$. Therefore $b \notin A$ and $b \in B$. Therefore
$B \not\subseteq A$, which is a contradiction. Therefore $sup(B) \leq sup(A)$.

\section*{1.3.5}
\textit{Let $A \subseteq \textbf{R}$ be bounded above, and let $c \in
  \textbf{R}$. Define the sets $c + A$ and $cA$ by $c + A = \{c + a: a \in A\}$
  and  $cA = \{ca: a \in A\}$
}

\textit{(a) Show that $sup(c + A) = c + sup(A)$.}

% $$\forall a \in A: a \leq sup(A)$$
% $$\forall a \in A: c + a \leq c +  sup(A)$$
% $$n \in c + A \to \exists a \in A: n = c + a$$
% $$\forall n \in c + A: n \leq c + sup(A)$$

% $c + sup(A)$ is an upper bound of c + A
We gonna prove it by contradiction

Suppose $c + sup(A)$ is not an upper bound of $c + A$. Then
$$\exists n \in c + A: n > c + sup(A)$$
let us call such element $l$;
also, by the definition of $c + A$
$$\forall n \in c + A : \exists a \in A: c + a = n$$
therefore
$$\exists a \in A: c + a = l$$
because $l > c + sup(A)$
$$c + a > c + sup(A)$$
$$a > sup(A)$$
Which is a contradiction. Therefore $c + sup(A)$ is an upper bound for $c + A$.

Suppose $c + sup(A) \neq sup(c + A)$. Then $sup(c + A)$ is less
than $c + sup(A)$. Let $\epsilon = c + sup(A) - sup(c + A)$. Then

$$\exists k \in A: k > sup(A) - \epsilon$$
$$k > sup(A) - c - sup(A) + sup(c + A)$$
$$k > - c  + sup(c + A)$$

Therefore
$$\exists h \in c + A: h = k + c$$
$$h - c = k$$
$$h - c >  -c + sup(c + A)$$
$$h > sup(c + A)$$

therefore

$$\exists h \in c + A: h > sup(c + A)$$

which is a contradiction. Therefore $c + sup(A) = sup(c + A)$, as desired.

\textit{(b) If $c \geq 0$, show that $sup(cA) = c * sup(A)$}

If $c = 0$, then it $cA = \{0\}$, and the case is trivial. Therefore let's
discuss further case when $c > 0$.

Suppose $c * sup(A)$ is not an upper bound for $cA$.

Then
$$\exists q \in cA: q > c * sup(A)$$
by the definition of $cA$
$$\exists j \in A: q = cj$$
therefore
$$q > c * sup(A)$$
$$cj > c * sup(A)$$
$$j > sup(A)$$

Which is a contradiction, because $j \in A$. Therefore $c * sup(A)$ is an
upper bound for $cA$.

Suppose $c *sup(A) \neq sup(cA)$. Then $sup(c A)$ is less
than $c * sup(A)$.

$$c * sup(A) > sup(cA)$$
$$c * sup(A) - sup(cA) > 0$$
$$\frac{c * sup(A) - sup(cA)}{c} > 0$$
% $$\frac{c * sup(A) - sup(cA)}{c} > 0$$

Let $\epsilon = \frac{c * sup(A) - sup(cA)}{c}$. Then


$$\exists k \in A: k > sup(A) - \epsilon$$
$$k > sup(A) - \frac{c * sup(A) - sup(cA)}{c}$$
$$k > sup(A) - sup(A) - sup(cA)/c$$
$$k > sup(cA)/c$$

Therefore
$$\exists h \in cA: h = ck$$
$$h/c = k$$
$$h/c>  sup(cA)/c$$
$$h > sup(cA)$$

therefore

$$\exists h \in cA: h > sup(cA)$$

Which is a contradiction. Therefore $sup(cA) = c * sup(A)$ as desired.

\textit{(c) Postulate a simular type of statement for $sup(cA)$ for the case
  $c < 0$}

Proposition: suppose that $A$ is bounded below;  if $c < 0$ then $sup(cA) = c
* inf(A)$

Suppose $c * inf(A)$ is not an upper bound for $cA$.

Then
$$\exists q \in cA: q > c * inf(A)$$
by the definition of $cA$
$$\exists j \in A: q = cj$$
therefore
$$q > c * inf(A)$$
$$cj > c * inf(A)$$
$$j < inf(A)$$

Which is a contradiction, because $j \in A$. Therefore $c * sup(A)$ is an
upper bound for $cA$.

Suppose $c * inf(A) \neq sup(cA)$. Then $sup(c A)$ is less
than $c * inf(A)$.

$$sup(cA) < c * inf(A)$$
$$sup(cA) - c * inf(A) < 0$$
$$\frac{sup(cA) - c * inf(A)}{c} > 0$$

Let $\epsilon = \frac{sup(cA) - c * inf(A)}{c} > 0$. Then

$$\exists k \in A: k < inf(A) + \epsilon$$
$$ k < inf(A) + \epsilon$$
$$ k < inf(A) + \frac{sup(cA) - c * inf(A)}{c}$$
$$ k < inf(A) + sup(cA)/c - inf(A)$$
$$ k < sup(cA)/c$$

Therefore

$$\exists h \in cA: h = ck$$
$$h/c = k$$
$$h/c = k < sup(cA) / c$$
$$h/c < sup(cA) / c$$
$$h > sup(cA)$$
therefore

$$\exists h \in cA: h > sup(cA)$$

Which is a contradiction. Therefore $sup(cA) = c * inf(A)$ as desired.

\section*{1.3.6}
\textit{Compute, without proofs, the suprema and infima of the following
  sets:}

\textit{(a) $\{n \in \textbf{N}: n^2 < 10\}$}

$sup = 3$, $inf = 1$.

\textit{(b) $\{n/(m + n): m,n\in \textbf{N}\}$}

$sup = 1/2$, $inf = 0$.

\textit{(c) $\{n/(2n + 1): n\in \textbf{N}\}$}

$sup = 1/2$, $inf = 1/3$.

\textit{(d) $\{n/m: m,n\in \textbf{N}$ with $m + n \leq 10\}$}

$sup = 9$, $inf = 1/9$.

\section*{1.3.7}
\textit{Prove that if $a$ is an upper bound for $A$, and if $a$ is also an
  element $A$, then it must be that $a = sup(A)$}

Let's prove this one by contradiction

Suppose that $a \neq sup(A)$. Because $a$ is still an upper bound, $sup(A) <
a$, Therefore $a \in A$, but $sup(A) < a$, which is a contradiction.
Therefore $a = sup(A)$.

\section*{1.3.8}
\textit{If $sup(A) < sup(B)$, then show that there exists an element $b \in B$,
  that is an upper bound for $A$.}

Let $\epsilon = sup(B) - sup(A)$. Then

$$\exists b \in B: b > sup(B) - \epsilon$$
$$ b > sup(B) - \epsilon $$
$$ b > sup(B) - sup(B) + sup(A) $$
$$ b > sup(A) $$

therefore $b$ is an upper bound for $A$.

\section*{1.3.9}
\textit{Without worryong about formal proofs for the moment, decide if the
  following statements about suprema and infima are true or false. For any that
  are false, supply an example where the claim in question does not appear to
  hold.}

\textit{(a) A finite, nonempty set always contains its supremum}

True

\textit{(b) If $a < L$ for every element $a$ in the set $A$, then $sup(A) <
  L$.}
False. $sup((0, 1)) = 1$; $\forall a \in (0, 1): a < 1$, therefore $sup(A) = L$.

\textit{(c) If A and B are sets with the property that $a < b$ for every
  $a \in A$ and $b \in B$, then it follows that $sup(a) < inf(B)$}

False. $sup((0,1)) = inf((1, 2))$

\textit{(d) If $sup(A) = s$ and $sup(B) = t$, then $sup(A + B) = s + t$. The
  set $A + B$ is defined as $A + B = \{a + b: a \in A$ and $b \in B\}$.}

True

\textit{(e) If $sup(A) \leq sup(B)$, then there exists an element $b \in B$
  that is an upper bound for $A$.}

False. $sup([1, 2]) = sup((1, 2))$

\section*{1.4.1}
\textit{Without doing too much work, show how to prove Theorem 1.4.3 in the
  case where $a < 0$ by converting this case into the already proven.}

First of all, let's state the theorem itself.

\textbf{Theorem 1.4.3 (Density of Q in R)}.
\textit{For every two real numbers $a$ and $b$ with $a < b$, there exists a
  rational number $r$ satisfying $a < r < b$.}

Then let's talk about the possible cases for $a < 0$. Then $b > a$ by the
assumtions of the theorem. Therefore $b \geq 0$ or $b < 0$. If $b > 0$ then
there exist 0 between them. If $b = 0$, then there exist a rational number
$b = 0 < 1/n < -a$, and by extention $a < 1/n < b$ as desired. Therefore we
will have some work to do  only with case $b < 0$.

It is  proven, that $a_1 < r < b_1$ if $0 \leq a_1 < b_1$. Therefore for
$a < b < 0$. Therefore $-a > -b > 0$. Therefore if we set $a_1 = -b$ and
$b_1 = -a$ then by previously stated theorem, there exist

$$a_1 < r < b_1$$
$$-b  < r < -a$$
$$b  > r > a$$
$$a < r < b$$
as desired.

\section*{1.4.2}
\textit{Recall that \textbf{I} stands for the set of irrational numbers.}

\textit{(a) Show that if $a,b \in \textbf{Q}$, then $ab$ and $a + b$ are
  elements of $\textbf{Q}$ as well.}

Because $a,b \in Q$ $\exists m_1,m_2 \in \textbf{Z}$, $\exists n_1,n_2 \in
\textbf{N}$ such that
$$a = \frac{m_1}{n_1}$$
$$b = \frac{m_2}{n_2}$$

therefore
$$a + b = \frac{m_1}{n_1} + \frac{m_2}{n_2} =
\frac{m_1 n_2 + m_2 n_1}{n_1 n_2}$$

$\textbf{Z}$ is presumed closed under addition and $\textbf{N}$ is presumed
closed under $\textbf{N}$, therefore 
$$ a + b = \frac{m_1 n_2 + m_2 n_1}{n_1 n_2} \in \textbf{Q}$$
also $\textbf{Z}$ is closed under multiplication, and therefore
$$ a  b = \frac{m_1 m_2}{n_1 n_2} \in \textbf{Q}$$

\textit{(b) Show that if $a \in \textbf{Q}$ and $t \in \textbf{I}$, then
  $a + t \in \textbf{I}$ and $at \in \textbf{I}$ as long as $a \neq 0$.}

We prove both things by contradiction.

Suppose $a + t \in \textbf{Q}$. Then $\exists b \in Q: a + t = b$. Also,
$a \in Q \to -a \in Q$. Therfore

$$a + t = b$$
$$t = b - a$$
Therfore, because $Q$ is closed under addition (as we discussed previously),
$t \in Q$, which is a contradiction. Therefore $a + t \in I$.

Suppose $at \in Q$ for $a \neq 0$. Then
$$\exists b \in Q: at = b $$
therefore if $a = m/n \in Q$, then $1/a = n / m \ in Q$. Therefore
$$t = b/a$$

Therefore $t \in Q$, which is a contradiction. Therefore $at \in I$ for
$a \neq 0$.

\textit{(c) Part (a) can besummarized by saing that $Q$ is closed under
  addition and multiplication. Is $I$ closed under addition and
  multiplication? Given two irrational numbers $s$ and $t$, what can we say
  about $s + t$ and $st$.}

$I$ is not closed under addition, nor under multiplication. Proof is
$\sqrt{2} + 1$ and $0 - \sqrt{2}$ are both irrational, but
$$\sqrt{2} + 1 - \sqrt{2} = 1 \in Q$$
and
$$\sqrt{2} * \sqrt{2} = 2 \in Q$$

\section*{1.4.3}
\textit{Using Exercise 1.4.2, supply a proof for Corollary 1.4.4 by
  applying Theorem 1.4.3 to the real numbers $a - \sqrt{2}$  and
  $b - \sqrt{2}$.}

First, let's state Corollary 1.4.4.

Given any two real numbers $a < b$, there exists an irrational number $t$
satisfying $a < t < b$.

We know, that between two numbers $a_1 < b_1$ there exists a rational number
$r$, for which it is true
$$a_1 < r < b_1$$
Let $a_1 = a + \sqrt{2}$ and $b_1 = b + \sqrt{2}$. Then
$$a_1 < r < b_1$$
$$a + \sqrt{2}  < r < b + \sqrt{2}$$
$$a  < r - \sqrt{2} < b $$

As we know, $r - \sqrt{2}$ is an irrational number, therefore between $a$ and
$b$ there exists an irrational number.

\section*{1.4.4}
\textit{Use the Archimedean Property of $R$ to rigorously prove that
  $inf\{1/n: n \in N\} = 0$}

It is true, that $\forall n \in N: n > 0$. Therefore

$$n > 0$$
$$1/n > 0$$

Therfore 0 is a lower bound for $1/n$. Also, because of Archimededean Property,
if we take any $\forall \epsilon > 0: \exists 1/n: 1/n < \epsilon $
$$\forall \epsilon > 0: \exists 1/n: 1/n < \epsilon $$
$$\forall \epsilon > 0: \exists 1/n: 0 + \epsilon > 1/n $$


Therfore for every $\epsilon$ there exists an element of a set such that
$0 + \epsilon$ is greater than this set. Therefore 0 is an infinum of this set,
as desired.

\section*{1.4.5}
\textit{Prove that $\cap^{\infty}_{n = 1}(0, 1/n) = \emptyset$. Notice that
  this demonstrates that the Nested Interval Property must be closed for the
  conclusion of the theorem to hold.}

First of all, $1/n > 0$ implies, that if $y \leq 0$ then $y \notin (0, 1/n)$
for any $n \in N$.

Because of the Archimedean Property, for every $y \in R > 0$ there exists
$n \in N$ such that $1/n < y$. Therefore there does not exist $y > 0$ such
that $y \geq 1/n$ for every $n \in N$. Therefore for every $y \geq 0$ there
exist $n \in N$ such that $y \notin (0, 1/n)$.

Therefore there are no real
numbers in $\cap^{\infty}_{n = 1}(0, 1/n)$. Therefore 
$$\cap^{\infty}_{n = 1}(0, 1/n) = \emptyset$$
as desired.

This conclusion proves, that if we have use an open interval for nested
interval property, then we'll have a problem.

\section*{1.4.6}
\textit{(a) Finish the proof of Theorem 1.4.5 by showing that the assumption
  $\alpha ^ 2 > 2$ leads to a contradiction of the fact that
  $\alpha = sup(T)$.}

First, Theorem 1.4.5 states that there exists a real number $\alpha \in R$
satisfying $\alpha ^ 2 = 2$.

Our assumpion is that
$$T = \{t \in R: t ^ 2 < 2\}\text{ and }\alpha = sup(T)$$

Our strategy is to state that if $a^2 > 2$, then $a$ is not a least upper
bound.

If $a$ is an least upper bound for $T$, then it is true, that
$$\forall \epsilon > 0: \exists t \in T: t > \alpha - \epsilon $$

Therefore for every $n \in N$
$$\exists t \in T: t > \alpha - 1/n $$

Now let us follow with the proof. For all $n \in N$.
$$t > \alpha - 1/n $$
$$\alpha - 1/n < t $$
$$(\alpha - 1/n)^2 < t^2 $$

$$(\alpha - 1/n)^2 < t^2 $$
$$\alpha^2 - \frac{2 \alpha}{n} + \frac{1}{n^2} < t^2 $$
$$\alpha^2 - \frac{2 \alpha}{n} + \frac{1}{n^2} < t^2 $$

Let's jusfity something now. $1^2 = 1 < 2$, therefore $1 < \alpha$. Therefore
$2 \alpha - 1 > 0$. Also, because $\alpha^2 > 2$, $\alpha^2 - 2 > 0$.
Therefore 
$$\frac{\alpha^2 - 2}{2 \alpha -  1} > 0 \in R$$

Now, let us pick $n \in N$ such that
$$1/n < \frac{\alpha^2 - 2}{2 \alpha -  1}$$
then

$$1/n < \frac{\alpha^2 - 2}{2 \alpha -  1}$$
$$1/n^2 < \frac{\alpha^2 - 2}{2 \alpha -  1}$$
$$n^2 > \frac{2 \alpha -  1}{\alpha^2 - 2}$$
$$\frac{2 \alpha -  1}{n^2} < \alpha^2 - 2$$
$$\frac{2 \alpha}{n^2} -  \frac{1}{n^2} < \alpha^2 - 2$$
$$- \alpha^2 + \frac{2 \alpha}{n^2} -  \frac{1}{n^2} < - 2$$
$$\alpha^2 - \frac{2 \alpha}{n^2} +  \frac{1}{n^2} > 2$$
$$\alpha^2 - \frac{2 \alpha}{n} +  \frac{1}{n^2} > 2$$
but
$$\alpha^2 - \frac{2 \alpha}{n} + \frac{1}{n^2} < t^2 < 2 $$

Therefore we have a contradiction. Therefore $\alpha \leq 2$. Therefore
$\alpha = 2$, as desired. Phew.

\textit{(b) Modify the argument to prove the existence of $\sqrt{B}$ for any
  real number $b \geq 0$.}

Let's discuss the case  $a^2 < b$.

$$(\alpha + 1/n)^2 = \alpha^2 + \frac{2 \alpha}{n} + \frac{1}{n^2} <
\alpha ^2 + \frac{2 \alpha}{n} + \frac{1}{n} = \alpha + \frac{2 \alpha + 1}{n}
$$

Let 
$$\frac{1}{n} < \frac{b - \alpha}{2 \alpha + 1}$$
then 
$$(\alpha + \frac{1}{n})^2 = \alpha^2 + (b - \alpha^2) = b$$

Therefore $\alpha + 1/n \in T$. Therefore $a^2 \geq b$.

Now let $$1/n < \frac{\alpha^2 - b}{2 \alpha -  1}$$. Then by reasoning in
the last part of exercise we can state, that $\alpha^2 \leq b$. Therefore
$\alpha^2 = b$, as desired.

\section*{1.4.7}
\textit{Finish the following proof for Theorem 1.4.12.}

First of all, let us state Theorem 1.4.12

If $A \subseteq B$ and $B$ is countable, then $A$ is either contable, finite
or empty.

Assume B is a countable set. Thus, there exists $f: N \to B$, which 1-1 and
onto. Let $A \subseteq B$ be an infinite subset of $B$. We must show that $A$
is countable.

Let $n_1 = min\{n \in N: f(n) \in A\}$. As a start to a definition of
$g: N \to A$, set $g(1) = f(n_1)$. Show how to inductively continue this
process to produce a 1-1 function $g$ from $N$ onto $A$.

Proposition: $g(n) = f(n_n)$.

Step: let $n_{n + 1} = min\{k > n \in N: f(k) \in A\text\}$. Then $f(n) <
n_{n+ 1} \notin n_{n + 1}$. Therefore $g(n + 1) = f(n_{n + 1})$. Therefore

$$\forall n_1 \neq n_2, k_1, k_2 \in N \exists g(n_1) = f(k_1)  \neq g(n_2) =
f(k_2)$$
and
$$\forall l \in g(N) \exists k \in N: g(k) = f(n_k) = l$$
Therefore there exist a bijective function $g: N \to A$. Therefore A is
countable, as desired.

If $A$ is not infinite, then it's finite (duh).

Same with empty.

\section*{1.4.8}
\textit{Use the following outline to supply for the statements in Theorem
  1.4.13.}

First of all, let's state Theorem 1.4.13

(i) If $A_1, A_2,...A_m$ are each countable sets, then the union
$A_1 \cup A_2 \cup ... \cup A_m$ is countable.

(ii) If $A_n$ is countable set for each $n \in N$, then $\cup^{\infty}_{n = 1}
A_n$ is countable.

\textit{(a) First, prove statement (i) for two countable sets, $A_1$ and $A_2$.
  Example 1.4.8 (ii) may be a useful reference. Some technicalities can be
  avoided by first replacing $A_2$, with the set $B_2 = A_2 \setminus A_1
  = \{x \in A_2: x \notin A_1\}$. The point of this is that the union
  $A_1 \cup B_2$ is equal to $A_1 \cup A_2$ and the sets $A_1$ and $B_2$ are
  disjoint. (What happens if $B_2$ is finite?)}

Let us first set $B_2 = A_2 \setminus A_1$. We will do it in order to have two
useful properties:
$$a \in B_2 \to a \in A_1^c \cup A_2 \to a \notin A_1$$
$$A_1 \cup B_2 = A_1 \cup (A_1^c \cap B_2) = (A_1 \cap A_1^c) \cup (A_1 \cap
B_2) = A_1 \cap B_2$$

Let's finally begin with the proof.  $B_2 \subseteq A_2$. By using
previous theorem we can state, that $B_2$ is either countable, finite, or
empty. If it is empty, then $A_1 \cup A_2 = A_1$, and therefore is countable.

For the finite case we'll need function, that returns the n'th smallest
element of the set. We can argue, that this function does not need the
rigorous definition, but we'll give it anyways:

$$s_F(1) = \{x \in F: \forall y \in F: x \leq y\}$$
$$s_F(n) = \{x \in F \setminus \{s(1),...,s(n - 1)\}\}: \forall y \in F \setminus \{s(1),...,s(n - 1)\}\}: x \leq y\}$$
for the finite set $F \subseteq R$.

$$\forall n_1 > n_2 \in N: s_F(n_2) \notin F \setminus \{s(1),...,s(n_1 - 1)\}
\to s_F(n_1) \neq s_F(n_2)$$
therefore the function is injecttive

We remove one element at every iteration from $F$ every iteration, therefore
$\{s_F(1),...s_F(|F|)\}$ spans the whole set $F$. Therefore the function is
surjective. Therefore the function is bijective.

Now let us define $q: N \to A_1 \cup B_2$. Let $g_1: N \to A_1$ be a bijective
function, that exists, because the set $A_1$ is countable. Then 
\begin{equation}
  q(x)=
  \begin{cases}
    s_{B_2}(x) & \text{if } x \leq |B_2|\\
    g_1(x - |B_2|) & \text{if } x > |B_2|
  \end{cases}
\end{equation}

Let's analyse this function. If $x_1 < x_2 \leq |B_2|$ or $x_1 > x_2 > |B_2|$,
then $q(x_1) \neq q(x_2)$ by injectivity of $s_{B_2}$ or $g_1(x)$ respectively.
If $x_1 \leq |B_2| < x_2$, then $q(x_1) \in B_2$ and $q(x_2) \in A_1$. Those
two sets are disjoint, and therefore $q(x_1) \neq q(x_2)$. Therefore for
all possible $x_1,x_2 \in N \to q(x_1) \neq q(x_2)$. Therefore the function
is injective.

Let $b \in N$ and  $N_b = \{x \in N > b\}$. Also let  $N_a = \{x \in N_b: x -
b\}$. Then
$$\forall c \in N_a: \exists v \in N_b: c = v - b$$
$$\forall v \in N_b: \exists n \in N: v = n + b$$
$$\forall c \in N_a: \exists n \in N: c = n$$
therfore $N \subseteq N_a$.
$$\forall n \in N: \exists v \in N_b: v = n + b$$
$$\forall v \in N_b: \exists c \in N_a: c = v - b$$
$$\forall n \in N: \exists c \in N_a: c = n$$
therfore $N_a \subseteq N$. Thus $N = N_a$.

Therefore
$$\forall x > |B_2|: \exists n \in N: g_1(x - |B_2|) = g_1(n)$$
Therefore $g_1$ spans the whole set $A_1$. $s_{B_2}$ spans the whole $B_2$ by
surjectivity of $s$. Therefore $q$ spans $A_1 \cup B_2$. Therefore it
is surjective.

Therefore $A_1 \cup B_2$ is bijective, therefore $A_1 \cup B_2$ is countable
if $B_2$ is finite. One last thing, that I want to add before finishing this case
is to acknowledge the fact, that this theorem is  true not for
real numbers only. This fact throws our minimal element part out of  the proof.
This misfortune can be avoided through the usage of lists and by converting sets
into them. I did not use this fact here because of the need to axiomaticly
define lists, which I can, but don't want to do. 

Now let us proceed with the case when $B_2$ is countable. Let $g_1: N \to A_1$ be a bijective funciton for $A_1$ and $g_2: N \to B_2$ be a bijective funciton
for $B_2$ (both of those exist because of the fact, that both of the sets are
countable).  Let $q: N \to A_1 \cup B_2$ be defined as
\begin{equation}
  q(x) =
  \begin{cases}
    g_1((x + 1) /  2) \text{ if x is odd} \\
    g_2(x / 2) \text{if x is even}
  \end{cases}
\end{equation}

Then $\forall n_1 \neq n_2 \in N \to g(n_1) \neq g(n_2)$ because of either
injectivity of both functions, or because of the fact, that both sets are
disjoint. Therefore $q$ is injective. Also
$${x \text{ is odd}: (x + 1) / 2} = {x \text{is even}: x/2 } = N$$
Therefore $q$ spans $A_1 \cup B_2$. Therefore the function is surjective.

Thus $q$ is bijective and $A_1 \cup B_2 = A_1 \cup B_2$ is countable if
$B_2$ is countable.

Therefore for any countable sets $A_1$ and $A_2$ it is true, that their
union is countable as well.

\textit{Now, explain how the more general statement in (i) follows}

We prove the first part of the teorem by induction.

Base: $A_1 \cup A_2$ is countable.

Proposition: $\cup^{m}_{n = 1}A_n$ is countable for $m \in N$.

Step:
$$\cup^{m + 1}_{n = 1}A_n = \cup^{m}_{n = 1}A_n \cup A_{m + 1}$$

$\cup^{m}_{n = 1}A_n$ is a countable set because of the proposition.
$A_{m + 1}$ is countable by assumtion of the theorem. Therefore
$\cup^{m + 1}_{n = 1}A_n$ is a union of two countable sets, and therefore is
itself contable, as desired.

\textit{(b) Explain why induction connot be used to prove part (ii) of
  Theorem 1.4.13 from part (i)}

Induction cannot be used to prove part (ii) because of the fact, that induction
relies of finality of the given set.

\textit{(c) Show how arranging N into the two-dimentional array }
$$\text{1 3 6 10 15 ...}$$
$$\text{2 5 9 14 ...   }$$
$$\text{4 8 13 ...}$$
$$\text{7 12 ...}$$
$$\text{11 ...}$$
$$...$$
\textit{leads to a proof of a Theorem 1.4.13 (ii)}

This proof will not be as rigorous as the ones before that, but i'll try them
anyways. Let us convert all of the given sets onto lists. Then let us construct
lists
$$l_1 = {A_{1_1}}$$
$$l_2 = {A_{2_1}, A_{1_2}}$$
$$l_3 = {A_{3_1}, A_{2_2}, A_{3_1}}$$
$$...$$
$$l_n = {A_{n_1}, A_{n - 1_{2}}, A_{n - 2_{3}1} ...}$$

if $A_{n_n}$ already has been included into the lists $l_{1..n - 1}$, then we don't
include it in the list.
Then if we concatenate all of those lists together, then we will have function
$q: N \to \cup^{\infty}_{n = 1} A_n$ which is defined as
$$q(x) = \text{x'th element of the final list}$$
This function will be injective, because we threw out already included elements, and
will be surjective, because all of the elements of $\cup^{\infty}_{n = 1} A_n$ are
eventually in the list. Thus, we have a bijective function between $N$ and
$\cup^{\infty}_{n = 1} A_n$. Therefore $\cup^{\infty}_{n = 1} A_n$ is countable, as
desired.

\section*{1.4.9}
\textit{(a) Given sets $A$ and $B$, explain why $A \sim B$ is equivalent to asserting
  $B \sim A$.}

Short answer: bijectivity implies inversability.

Medius answer: injectivity leads us to the fact, that for every element of codomain there
exists only one element of the domain and by extension lets us
define inverse function in the first place;
surjectiviry guarantees, that the inverse function is defined for every element of the domain.

Long answer:

$A \sim B$ implies, that there exists a bijective function $g: A \to B$. Bijective means
that
$$\forall a_1 \neq a_2 \in A \to g(a) \neq g(b)$$
$$\forall b \in B: \exists a \in A: g(a) = b$$

This implies that for all $b \in B$ there exist only one $a \in A$ such that $g(a) = b$.
This fact lets us define $g^{-1}: B \to A$ as
$$g^{-1}(b) = a \text{ such that } g(a) = b$$

Then
$$\forall b_1 \neq b_2 \in B \to g^{-1}(b_1) \neq g^{-1}(b_2)$$
because $g$ is a function and therefore $g^{-1}(b_1) \neq g^{-1}(b_2) \to
g(a_1) \neq g(a_2) \to a_1 \neq a_2$. Therefore $g^{-1}$ is injective.

$$\forall a \in A: \exists g(a) \to \exists b \in B: g^{-1}(b) = a$$
and therefore
$$\forall a \in A: \exists b \in B: g^{-1}(b) = a$$
therefore function is surjective.

Therefore the function is bijective and therefore $B \sim A$.

Therefore $A \sim B \iff B \sim A $, as desired.

% $$\forall b \in B: \not \exists a_1 \neq a_2 \in A: g(a_1) = g(a_2) = b$$.
% because g is a function, for every $a \in A$ there exists only one $b \in B$. Therefore
% there does not exist $b_1 \neq  b_2 \in B$ such that $g(a) = b_1$ and $g(a) = b_2$.

\textit{(b) For three sets $A, B$ and $C$, show that $A \sim B$ and $B \sim C$
  implies $A \sim C$. These two properties are what is meant by saying that $\sim$
  is an equivalence relation.}

Because $A \sim B$ and $B \sim C$ there exist two bijective functions $g_1: A \to B$ and
$g_2: B \to C$.
Therefore
$$\forall a_1 \neq a_2 \in A \to g_1(a_1) \neq g_1(a_2)$$
$$\forall b_1 \neq b_2 \in B \to g_2(b_1) \neq g_2(b_2)$$
therefore
$$\forall a_1 \neq a_2 \in A \to g_1(a_1) \neq g_1(a_2) \to g_2(g_1(a_1)) \neq g_2(g_1(a_2))$$

Also
$$\forall a \in A: \exists b \in B: g_1(a) = b$$
$$\forall b \in B: \exists c \in C: g_2(b) = c$$
and therefore
$$\forall a \in A: \exists b \in B: g_1(a) = b \to  \exists c \in C: g_2(g_1(a)) \in C $$

Therefore $g_2 \circ g_1: A \to C$ is a bijective function. Therefore $A \sim C$, as desired.

\section*{1.4.10}
\textit{Show that the set of all finite subsets of N is a countable set. (It turns out that
  the set of all subsets of N is not a countable set. This is a title of Section 1.5)}

Our strategy here will be to show that for every $n \in N$ set of sets of
length $n$ is countable, therefore their union is countable.

Maybe I should proceed with induction.
Base: 
First of all, set N in countable.  Therefore the set of sets of length 1 is countable.

Proposition:
Suppose that set of sets of length $n \in N$ is countable.

Step:
Because set of sets of length N is countable, let us take a set $S_m$.
Then, let us add a number into in , which is not already in this set.
To be more precise, let us define
$$N_m = N \setminus S_m$$
$$S_{m_k} = S_m\cup \{\text{k'th number of}N_m\}$$

Then each of $S_{m_k}$ is a union of countable sets, and therefore countable.
Therefore $\cup^{\infty}_{k = 1} S_{m_k} = S_{m + 1}$ is countable.

Therefore each set of sets of length $n \in N$ is countable. Therefore their union is
countable. Therefore set of finite subsets of $N$ is countable, as desired.

\section*{1.4.11}
\textit{Consider the open interval $(0,1)$, and let $S$ be the set of points in the
  open unit square; that is, $S = \{(x, y): 0 \leq x,y \leq 1\}$}

\textit{(a) Find a 1-1 function, that maps (0, 1) into, but not necessarily onto, S.
  (This is easy.)}

Yeah, it is. Let $g(x) = (x, 0.5)$.

\textit{(b) Use the fact that every real number has a decimal expansion to produce a
  1-1 function that maps $S$ into (0, 1). Discuss whether the formulated function is onto.
  (Keep in mind that any terminating decimal expansion such as .235 represents the same
  real number as .234999999999 ).}

Let $g(x, y): S \to (0, 1)$ be such a function, that maps digits of $x$ into the
odd digits of the result, and $y$ into the event digits of the result. As an example
$$g(.235, .746) = .273456$$

This function is into, because we are essentially writing two dirrerent numbers in an
odd way. 
Therefore $\forall s_1, s_2 \in S \to g(s_1) \neq g_(s_2) \ $

The problem arises with surjectivity. Suppose that we have a number $x = 0.2$. If
this function would be the output of our function, then $y$ would need to be 0;
Therefore the function is not surjective.

\textit{The Schroder-Bernstein Theorem discussed in Exercise 1.4.13 to follow
  can now be applised to conclude that $(0,1) \sim S$}

\section*{1.4.12}
\textit{A real number $x \in R$ is called algebraic if there exists integers
  $a_0, a_1, a_2, ..., a_n \in Z$, not all zero, such that }

$$a_n x^n + a_{n - 1} x^{x - 1} + ... + a_1 x + a_0 = 0$$
\textit{Said another way, a real number is algebraic if it is the root of a polynomial
  with integer coefficients. Real numbers that are not algebraic are called transcendental
  numbers. Reread the last paragraph of Section 1.1. The final question posed here is
  closely related to the question of whether or not transcendental numbers exist.}

\textit{(a) Show that $\sqrt{2}$, $\sqrt[3]{2}$ and $\sqrt{3} + \sqrt{2}$ are algebraic.}

Let $a_0 = -2$, $a_1 = 0$ and $a_2 = 1$. Then

$$a_2 x^2 + a_1 x + a_0 = 0$$
$$ x^2 + 0  - 2 = 0$$
$$x^2 = 2$$
$$x = \sqrt{2} \text{ or } x = -\sqrt{2}$$

Therefore $\sqrt{2}$ is algebraic.

Let $a_0 = -2$, $a_1 = 0$, $a_2 = 0$ and $a_3 = 1$. Then

$$a_3 x^3 + a_2 x^2 + a_1 x + a_0 = 0$$
$$ x^3 + 0  - 2 = 0$$
$$x^3 = 2$$
$$x = \sqrt[3]{2}$$

Therefore $\sqrt[3]{2}$ is algebraic.

Let $a_0 = 1$, $a_1 = 0$, $a_2 = -10$, $a_3 = 0$ and $a_4 = 1$. Then

$$x^4 - 10 x^2 + 1 = 0$$
$$x^4 - 10 x^2 + 25 =  24$$
$$(x^2 - 5)^2 =  4 * 6$$
$$x^2 - 5 =  2\sqrt{6}$$
$$x^2 = 5 + 2\sqrt{6}$$
$$x^2 = 2 + 3 + 2\sqrt{2}\sqrt{3}$$
$$x = \sqrt{2} + \sqrt{3}$$

Therefore $\sqrt{2} + \sqrt{3}$ is algebraic.

\textit{(b) Fix $n \in N$, and let $A_n$ be the algebraic numbers obtained as roots of
  polynomials with integer coefficients that have degree $n$. Using the fact that
  every polynomial has a finite number of roots, show that $A_n$ is countable. (For
  each $m \in N$, consider polynomicals $a_n x^n + a_{n - 1} x^{n - 1} + ... +
  a_1 x + a_0$ that satisfy $|a_n| + |a_{n - 1}| + ... + |a_1| + |a_0| \leq m$.)}

Let us fix $m \in M$, then there exist a finite number of $a_n, ..., a_1, a_0 \in Z$ such that
$|a_n| + |a_{n - 1}| + ... + |a_1| + |a_0| \leq 0$. Therefore for each $m \in M$ there
are finitely many combinations of coefficients and thus finitely many numbers of roots.
Therefore let us correspont each root, with a number $n \in N$ is ascending order
(keeping in mind, we should check, that this number is not already in the function). Then
we'll get a bijective  function from $N$ to all the possible roots of polynomials
with integral coefficiens. Therefore $A_n$ is countable.

\textit{(c) Now, argue that the set of all algebraic numbers is countable. What may we
  conclude about the set of transcendental numbers?}

Because $A_n$ is countable for each $n \in N$ we know, that
$\cup^{\infty}_{n = 1}A_n$ is countable as well. By definition, algebraic number
is a root of one of such polynomials, and therefore algebraic numbers are a countable
set.

The fact, that the set of algebraic numbers is countable presents us with a fact, that
the set of transcedental numbers is not countable (because if it was, then the set
of real numbers would be a union of two countable sets and therefore countable as
well).

\section*{1.4.13 (Schroder-Bernstein Theorem)}
\textit{Assume that there exists a 1-1 function $f: X \to Y$ and another 1-1 function
  $g: Y \to X$. Follow the steps to show that there exists a 1-1, onto function $h: X \to Y$
  and hence $X \sim Y$.}

\textit{(a) The range of $f$ is defined by $f(X) = \{y \in Y: y = f(x) \text{ for some }
  x \in X$. Let $y \in f(X)$. (Because $f$ is not neccesarily onto, the range $f(X)$
  may not be all of $Y$.) Explain why there exists a unique $x \in X$ such that $f(x) = y$.
  Now define $f^{-1}(y) = x$, and show that $f^{-1}$ is a 1-1 function from $f(X)$ onto $X$.}

Because $f$ is given to be injective, it is true that
$$\forall x_1 \neq x_2 \in X \to  f(x_1) \neq f(x_2)$$
$f$ is also a function.  By definition of a function we know, that for every element
of domain there exists only one element of codomain. Therefore
$$\forall f(x_1) \neq f(x_2) \in f(X) \to  x_1 \neq x_2$$
By plugging $y$ into the equation we have
$$\forall y_1 \neq y_2  \in f(X) \to  x_1 \neq x_2$$
Thus we can define $f^{-1}: f(X) \to X$:
$$f^{-1}(y) = \{x \in X: f(x) = y\}$$
This function will be injective by the fact that
$$\forall y_1 \neq y_2  \in f(X) \to  x_1 \neq x_2 \to f^{-1}(x_1) \neq f^{-1}(x_2)$$

\textit{In a similat way, we can also defin the 1-1 function $g^{-1}: g(X) \to Y$}

Same logic applies to $g(x)$. Therefore we have
$$g^{-1}(x) = \{y \in Y: g(y) = x\}$$
which by the same logic is injective.

\textit{(b) Let $x \in X$ be arbitrary. Let the chain $C_x$ be the set consisting of all
  elements of the form }
$$\text{(1) }..., f^{-1}(g^{-1}(x)), g^{-1}(x), x, f(x), g(f(x)), f(g(f(x))), ...$$
\textit{Explain why the number of elements to the left of $x$ in the above chain may be zero,
  finite, or infinite}

The number of elements on the left is 0, if there does not exists $y \in Y$ such that
$g(y) = x$.

The number of elements on the left is finite number, if after some element $l$ there
does not exist an element $y \in Y$ or $x \in X$, such that $g(y) = l$ or $f(x) = l$.

The number of elements on the left is infinite, if there exist both of those numbers
for every element (for example, if both functions are onto).

% One idea, that I want to explore, is that if there exists only a finite set of
% elements to the left of $x$, then its number is fixed (namely 1).

% Suppose, that we have our $x$. Then, if the number of elements to the left is not
% 0, then there exist $y: g(y) = x$ by the definition of $g^{-1}(x)$. Therefore
% $x \in g(Y) \subseteq X$. Suppose now that 





\textit{(c) Show that any two chains are either identical, or completely disjoint}

Suppose that $x_1 \neq x_2$. Then $f(x_1) \neq f(x_2)$ by injectivity of $f$.
Also, $f(x) \in Y$, and therefore $g(f(x_1)) \neq g(f(x_2))$ and so on.

$f^{-1}$ and $g^{-1}$ are injective as well. Therefore the same logic applies. Thus
we can state that
$$\forall x_1 \neq x_2 \to C_{x_1} \cap C_{x_2} = \emptyset $$
and
$$\forall x_1 = x_2 \to C_{x_1} \cap C_{x_2} = C_{x_1} = C_{x_2} $$

\textit{(d) Note that the terms of the chain in (1) alternate between elements of $X$
  and elements of $Y$. Given a chain $C_x$, we want to focus on $C_x \cap Y$,
  which is just a part of the chain that sits in $Y$.}

\textit{Define the set $A$ to be the union of all chains $C_x$ satisfying
  $C_x \cap Y \subseteq f(X)$. Let $B$ constst if the union of the remaining chains not
  in $A$. Show that any chain contained in $B$ must be of the form}
$$y, g(y), f(g(y)), g(f(g(y))), ....$$
\textit{where $y$ is an element of $Y$ that is not in $f(X)$}

Let $A$ and $B$ be defined as in the exercise text. Then
$$\forall C_x \in B: \exists c \in C_x: c \in Y \text{ and } c \notin f(X)$$.
Thus
$$\nexists x \in X: f(x) = c$$
at the same time
$$c \in Y \to \exists x \in X: g(y) = x$$.

Becaus $g: Y \to g(Y) \subseteq X$ and $f: X \to f(X) \subseteq Y$ we can write
the chain in the form 
$$c, g(c), f(g(c)), ...$$
Substituting $c$ with $y$ we get 
$$y, g(y), f(g(y)), ...$$
where $y \notin f(X)$, as desired.

\textit{(e) Let $X_1 = A \cap X$, $X_2 = B \cap X$, $Y_1 = A \cap Y$, and $Y_2 = B \cap Y$.
  Show that $f$ maps $X_1$ onto $Y_1$ and that $g$ maps $Y_2$ onto $X_2$. Use this
  infortation to prove $X \sim Y$.}

Let $X_1$, $X_2$, $Y_1$ and $Y_2$ be defined as in exercise text. Then
let us discuss $f(X_1)$
$$A = \{C_x: C_x \cap Y \subseteq f(X)\}$$
$$X_1 = \{C_x \cap X: C_x \cap Y \subseteq f(X)\}$$
$$Y_1 = \{C_x \cap Y: C_x \cap X \subseteq g(Y)\}$$

therefore
$$\forall y \in Y_1: \exists x \in X_1: f(x) = y$$
and
$$\forall x \in X_2: \exists y \in Y_2: g(y) = x$$
Or in other words, $f: X_1 \to Y_1$ is surjective(onto).

By the same logic, $g: Y_2 \to X_2$ is also surjective.

Let us define function $h: X \to Y$
\begin {equation}
h(x) =
  \begin{cases}
    f(x) \text{ if } x \in X_1 \\
    g^{-1}(x) \text{ if } x \in X_2
  \end{cases}
\end {equation}

Functions $f$ and $g^{-1}$ are both injective.  $X_1 \cap X_2 = \emptyset$. Therefore
$h$ is injective as well.
Also, $X_1 \cup X_2 = X$,  and both $f$ and $g$ are surjective. Therefore $h$ is surjective
as well.

Therefore this function is bijective. Therefore we have a bijective function from
$X$ to $Y$. Therefore $X \sim Y$, as desired.
    
\section*{1.5.1}
\textit{Show that $(0, 1) = \{x \in R: 0 < x < 1\}$ is uncountable if and only if R
  is uncountable.}

Let

$$ f(x) = \frac{2x - 2)}{x ^ 2 - 2x}$$

Calsulus shows that this function maps $(0, 1) \to R$. Also, it shows,
that ist is increasing and therefore it is bijective. Thus
there exist a bijective. Thus $(0, 1) \sim R$. Therefore it is uncountable
if and only if $R$ is uncountable.

\section*{1.5.2}
\textit{(a) Explain why the real numnder $x = b_1 b_2 b_3...$ cannot be f(1).}

If $f(1) = b$ then
$$b_1 = 2 \to b_1 \neq 2$$
$$b_1 \neq 2 \to b_1 = 2$$
Therefore we have a contradiction.

\textit{(b) Now, explain why $x \neq f(2)$, and in general why $x \neq f(n)$ for any
  $n \in N$. }

If $f(1) = b$ then
$$b_n = 2 \to b_n \neq 2$$
$$b_n \neq 2 \to b_n = 2$$
Therefore we have a contradiction for $n \in N$.

\textit{(c) Point ount the contradiction that arises from these observations and
  conclude that $(0, 1)$ is uncountable.}

If $(0,1)$ is countable, then $b \in (0, 1)$, but it cannot be correspondent
to $n \in N$ for any $f: N \to (0, 1)$. Therefore either $b \notin (0, 1)$ or
$(0, 1)$ is not countable. Because $b \in (0, 1)$ we conclude that $(0, 1)$ is
uncountable.

\section*{1.5.3}
\textit{Supply rebuttals to the following complaints about the proof of
  Theorem 1.5.1}

\textit{(a) Every rational number has a decimal expansion so we could apply
  this same argument to show that the set of rational numbers between 0 and 1
  is uncountable. However, because we know that any subset of $Q$ must me countable,
  the proof of Theorem 1.5.1 must be flawed}

We can try to apply the same argument to $Q$, but now we have a problem with the
fact, that every rational number in decimal expansion repeats itself after some
point or another. Therefore $b \notin Q$, therefore we cannot conclude anything.

\textit{(b) A few numbers have two different decimal representations. Specifically,
  any decimal expansion that terminates can also be written with repeating 9's.
  For instance, 1/2 can be written as .5 or as .4999.... Doesn't this cause some
  problems?}

No, it doesn't. And the reason on why it doesn't cause any problems is because
our argument stems on the fact, that given $b \in R$ is not in the set,
if the set is countable. Therefore different representations problem is irrelevant.

\section*{1.5.4}
\textit{Let $S$ be the set consisting of all sequences of 0's and 1's. Observee that $S$
  is not a particular sequence, but rather a large set whose elements are sequences;
  namely,}
$$S = \{(a_1, a_2, a_3,...): a_n = 0 or 1\}$$
\textit{As an example, the sequence (1, 0, 1, 0, 1, 0, ...) is an element of $S$, as the
  sequence (1, 1, 1, 1, ...).}

\textit{Give a rigorous argument, that S is uncountable}

I don't know, if it counts, as rigorous, but here we go.

Each $s \in S$ corresponds to a binary reresentation of a number in $(0, 1)$. Therefore
$S \sim (0, 1) \sim R$, therefore it is uncountable.

We can work around sets and NIP to show the same thing if we want to, but I dont want to.

\section*{1.5.5}
\textit{(a) Let $A = \{a, b, c\}$. List the eight elements of $P(A)$. (Do not forget that
  $\emptyset$ is considered to be a subset of every set.)}

$$\emptyset, \{a\}, \{b\}, \{b, a\}, \{c\}, \{c, a\}, \{c, b\},  \{c, b, a\}$$

\textit{(b) If $A$ is finite with $n$ elements, show that $P(A)$ has $2^n$ elements.
  (Constructing a particular subset of $A$ can be interpreted as making a series of
  decisions about whether or not toinclude each element of $A$.}

Repeat of 1.2.11

This proof is dumb, but intuitive:

Every subset is corresponding to a number in binary number: 0 for excluded,
1 for included. Therefore there exist $2^n$ possible combinations.

For a more concrete proof let's resort to induction.

Base case(s): subsets of $\emptyset$ are $\emptyset$ itseft ($2^0 = 1$ in total). Subsets of
set with one element are $\emptyset$ and set itself ($2^1 = 1$ in total).

Proposition is that set with n elements has $2^n$ subsets.

Inductive step is that for set with $n + 1$ elements can either have or hot
have the $n + 1$'th element. Therefore there exist $2^n + 2^n = 2 * 2^n =
2^{n + 1}$ subsets, as desired.

\section*{1.5.6}
\textit{(a) Using the particular set $A = \{a, b, c\}$, exhibit two different
  1-1 mappings from $A$ to $P(A)$}

\begin{equation}
  f_1(x) =
  \begin{cases}
    a \to \{a\} \\
    b \to \{b\} \\
    c \to \{c\} \\
  \end{cases}    
\end{equation}

\begin{equation}
  f_2(x) =
  \begin{cases}
    a \to \emptyset \\
    b \to \{b\} \\
    c \to \{c\} \\
  \end{cases}    
\end{equation}

\textit{(b) Letting $B = \{1, 2, 3, 4\}$, produce an example of a 1-1 map $g: B \to P(B)$.
}

\begin{equation}
  f_3(x) =
  \begin{cases}
    1 \to {1} \\
    2 \to {2} \\
    3 \to {3} \\
    4 \to {4} \\
  \end{cases}    
\end{equation}

\textit{(c) Explain why, in parts (a) and (b), it is impossible to construct mappings,
  that are onto}

$|A| < |P(A)|$

\section*{1.5.7}
\textit{Return to the particular functions constructed in Exercise 1.5.6 and construct
  subset B that results using the preceding rule. In each case, note that B is not in
  the range of the function used. }

For $f_1$:
$$B = \{\emptyset\}$$

For $f_2$:
$$B = \{a\}$$

For $f_3$:
$$B = \{\emptyset\}$$

\section*{1.5.8}
\textit{(a) First, show that the case $a' \in B$ leads to a contradiction.}

Suppose that $a' \in B$. By definition of $B$,  $a \notin f(a')$. Therefore we have a
contradiction.

\textit{(b) Now, finish the argument by showing that the case $a' \notin B$ is
  equally unacceptable.}

Suppose that $a' \notin B$. Therfore, by definition of $B$, $a' \notin B \to a' \in B$.
Therefore we have a contradiction. Thus, we cannot construct a surjective map from $A$ to
$P(A)$. Therefore $A \not\sim P(A)$.

\section*{1.5.9}
\textit{As a final exercise, answer each of the following be establishing a 1-1
  correspondence with a set of known cardinality.}

\textit{(a) Is the set of all function from $\{0, 1\}$ to $N$ countable or
  uncountable?}

Examples of such functions
\begin{equation}
  f_1 =
  \begin{cases}
    \{0\} \to 5 \\
    \{1\} \to 123
  \end{cases}
\end{equation}

\begin{equation}
  f_2 =
  \begin{cases}
    \{0\} \to 7 \\
    \{1\} \to 7
  \end{cases}
\end{equation}

Each of those functions we can correspond to a set of $Q^+ = {q \in Q: q > 0}$,
which is countable (infinite subset of a countable set $\to$ countable) .

To clarify my result, think of $f(0)$ as of numerator, and $f(1)$ as denumenator.

\textit{(b) Is the set of all function from $N$ to $\{0, 1\}$ countable or uncountable.}

Uncountable. Each of those function we can correspond to a sequence of 0's and 1's from
Exercise 1.5.4.

\textit{(c)Given a set $B$, a subset $A$ of $P(B)$ is called an antichain, if no
  element of $A$ is a subset of any other element of $A$. Does $P(N)$ contain an
  uncountable antichain?}

Yes, it does. We can correspond a function from $N$ to $\{0, 1\}$ to an antichain
by adding element $2n$ to the set, if the n'th element of a sequence is 0, and
adding $2n - 1$ to the set, if the n'th position is 1. Therefore, for
2 different chains we will have two different sets, each of which will be
different by al least two numbers, and therefore not a subset of each other.

\chapter{Sequences and Series}
\section*{2.2.1}
\textit{Verify, using the definition of convergence of a sequence, that the following
  sequenceces converge to the proposed limit.}

\textit{(a) $\lim\frac{1}{(6 n ^ 2 + 1} = 0$}

Let $\epsilon$ be arbitrary. Choose
$N \in \textbf{N}: N > \sqrt{\frac{1}{6 \epsilon} - \frac(1)(6)}$. Let $n \in N \geq N$. Then

$$n > \sqrt{\frac{1}{6 \epsilon} - \frac{1}{6}}$$
$$n ^ 2  > \frac{1}{6 \epsilon} - \frac{1}{6}$$
$$6 n ^ 2  > 1/\epsilon - 1$$
$$6 n ^ 2 + 1 > 1/\epsilon$$
$$\frac{1}{(6 n ^ 2 + 1)} < \epsilon$$
$$|\frac{1}{(6 n ^ 2 + 1)}| < \epsilon$$

as desired.

\textit{(b) $\lim\frac{3n + 1}{2n + 5} = \frac{3}{2}$}

Let $\epsilon$ be arbitrary. Choose
$N \in \textbf{N}: N > \frac{13}{4 \epsilon} - 10/4$. Let $n \in N \geq N$. Then

$$ n > \frac{13}{4 \epsilon} - 10/4$$
$$ 4n > \frac{13}{\epsilon} - 10$$
$$ 4n + 10 > \frac{13}{\epsilon} $$
$$ \frac{13}{4n + 10} < \epsilon $$
$$ |\frac{13}{4n + 10}| < \epsilon $$
$$ |\frac{-13}{4n + 10}| < \epsilon $$
$$ |\frac{6n + 2 - 6n - 15}{4n + 10}| < \epsilon $$
$$ |\frac{2(3n + 1) - 3(2n + 5)}{2(2n + 5)}| < \epsilon $$
$$ |\frac{3n + 1}{2n + 5} - \frac{3}{2}| < \epsilon $$

as desired.

\textit{(c) $\lim\frac{2}{\sqrt{n + 3}} = 0$}

Let $\epsilon$ be arbitrary. Choose
$N \in \textbf{N}: N > {\frac{2}{\epsilon}}^2 - 3$. Let $n \in N \geq N$. Then

$$n> {\frac{2}{\epsilon}}^2 - 3$$
$$n + 3 > {\frac{2}{\epsilon}}^2$$
$$\sqrt{n + 3} > \frac{2}{\epsilon}$$
$$\frac{2}{\sqrt{n + 3}} < \epsilon$$
$$|\frac{2}{\sqrt{n + 3}}| < \epsilon$$

as desired.

\section*{2.2.2}
\textit{What happens if we reverse the order of the quantifiers on Definition 2.2.3?}

\textit{Definition: A sequence $(x_n)$ verconges to $x$ if there exists an $\epsilon > 0$
  such that for all $N \in \textbf{N}$ it is true that $n \geq N$ implies $|x_n - x| <
  \epsilon$.}

\textit{Give an example of a vercongent sequence. Can you give an example of a vercongent
  sequence, that is divirgent? What exactly is being described in this strange definition?}

An example of a vercongent sequence:
$$(x_n) = 5$$.

An example of a vercongent sequence, that is divergent:
$$(x_n) = (-1)^n$$.

Here described a bounded sequence (i.e. $|(x_n)| < M$  for some $M > 0\in R$

\section*{2.2.3}
\textit{Describe what we would have to demonstrate in order to disprove each
  of the following statements.}

\textit{(a) At every college in the United States, there is a student who is at least
  seven feet tall}

There exist a college, where every student is shorter than 7 feet.

\textit{(b) For all colleges in the United States, there exists a professor who
  gives every student a grade of either A or B}

There exist a college, where every professor gives C or less every time.

\textit{(c) There exist a college in the United States where every student is at least
  six feet tall}

In all colleges across US there exists a student, who is shorter than 6 feet.

\section*{2.2.4}
\textit{Argue that the sequence }
$$1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, (\text{5 zeroes}), 1...$$
\textit{
  does not converge to zero. For what values of $\epsilon > 0$ does there exist a response
  N? For which values of $\epsilon > 0$ is there no suitable response?}

If we set $\epsilon = 0.5$, then from time to time elements will get out of
the desired range.

For values $\epsilon > 1$ there always exist a suitable N.

For values $ 0 < \epsilon \leq 0$ there exists no responce.

\section*{2.2.5}
\textit{Let $[[x]]$ be the greatest integer less than of equal to $x$. For example,
  $[[\pi]] = 3$ and $[[3]] = 3$. Find $\lim a_n$ and supply proofs for each conclusion
  if}

\textit{(a) $a_n = [[1/n]]$}
$$\forall n \in N \to 0  < 1/n \leq 1$$
$$n \in N \geq 2 \to  0  < 1/n < 1$$
Therefore for $n > 2$
$$\forall \epsilon > 0 \in R: |[[1/n]]| = [[1/n]] = 0 < \epsilon$$
as desired.

\textit{(b) $a_n = [[(10 + n)/2n]]$}

Let n > 10. Then

$$n > 10$$
$$2n > 20$$
$$\frac{10}{2n} < \frac{1}{2}$$
$$0 < \frac{10}{2n} + \frac{1}{2} < 1$$
$$[[\frac{10}{2n} + \frac{1}{2} ]] = 0$$

Thus for every $\epsilon > 0$ we can pick $N = 10$ and it follows that 
$$|[[(10 + n)/2n]]| < 0$$

Therefore $(a_n) \to 0$, as desired.

\textit{Reflecting on these examples, comment on the statement following Definition
  2.2.3 that "the smaller the $\epsilon$-neighborhood, the larger $N$ may
  need to be".}

The key word here is "may". It may have to be larger, it may not. In some cases one
value works for all of the $\epsilon$'s. I bet my money on the fact, that it  can be rigorously proven, that this is the case only for the sequences, that are constant after
some term, but I'll skip that

\section*{2.2.6}
\textit{Suppose that for a particular $\epsilon > 0$ we have found a suitable
  value of $N$ that "works" for a given sequence in the sence of Definition 2.2.3.}

\textit{(a) Then, any larger/smaller (pick one) N will also work for the same $\epsilon > 0$.}

Larger. This fact follows from definition.

\textit{(b) Then, this same N will also work for any larger/smaller value of $\epsilon$.}

Larger. $x \in V_\epsilon \to x \in V_{\epsilon + s}$

\section*{2.2.7}
\textit{Informally speaking, the sequence $\sqrt{n}$ "converges to infinity".}

\textit{(a) Imitate the logical structure of Definition 2.2.3 to create a rigorous definition
  for the mathematical statement $\lim x_n = \infty$. Use this definition to prove
  $\lim \sqrt{n} = \infty$.}

\textbf{Definition of convergence to infinity}
A sequence $(a_n)$ converges for infinity if, for every $\epsilon \in R$, there exists an
$N \in \textbf{N}$ such that whenever $n \geq N$ it follows that $a_n > \epsilon$.

I relaxed a bit statement about the $\epsilon$, namely substituted $\epsilon > 0$ to
$\epsilon \in R$. In case you are wondering, Both cases are the equivalent.

Let $\epsilon \in R$ be arbitrary. Then we can pick $N \in \textbf{N}: N > \epsilon ^ 2$.
Then for $n \in \textbf{N} > N$:
$$n > \epsilon ^ 2$$
$$\sqrt{n} > \epsilon$$

Therefore the sequence converges to infinity

\textit{(b) What does your definition in (a) say about the particular sequence
  $(1, 0, 2, 0, 3, 0, 4, 0, 5, 0, ...)$}

It says, that it doesn't converge to infinity (specifically for the case $\epsilon > 0$)

\section*{2.2.8}
\textit{Here are two useful definitions:}

\textit{(i) A sequence $(a_n)$ is eventually in a set $A \subseteq R$ if there exists
  $N \in \textbf{N}$ such that $a_n \in A$ for all $n \geq N$.}

\textit{(ii) A sequence $(a_n)$ is frequently in a set $A \subseteq R$ if for every
  $N \in \textbf{N}$ there exists $n \geq N$ such that $a_n \in A$.
}

\textit{(a) Is the sequence $(-1)^n$ eventually or frequently in the set $\{1\}$?}

Frequently

\textit{(b) Which definition is stronger? Does frequently imply eventually or does
  eventually imply frequently?}

Eventually is stronger. Eventually implies frequently.

\textit{(c) Give an alternative rephrasing of Definition 2.2.3B using either frequently
  or eventually. Which is the term we want?}

We want to use eventually

A sequence $(a_n)$ converges to $a$ if, given any $\epsilon$-neighborhood $V_\epsilon(a)$ of
$a$, sequence is eventually in $V_\epsilon(a)$.

\textit{(d) Suppose an infinite number of terms of a sequence $(x_n)$ are equal to 2.
  Is $(x_n)$ necessarily eventually in the interval $(1.9, 2.1)$? Is it frequently in
  $(1.9, 2.1)$?}

It is not necessarily eventually  in $(1.9, 2.1)$. Example:
$$(1, 2, 2, 2, 3, 2, 4, 2, 5, 2, 6, 2, ....)$$

It is indeed frequently in $(1.9, 2.1)$. This stems from the fact, that before $a_n$,
there exist only a finite number of elements before it. Therefore 2 is bound to
be met again at some time.

\section*{2.3.1}
\textit{Show that the constant sequence $(a, a, a, ...)$ converges to $a$.}

$$|a_n - a| = |a - a| = 0$$
therefore for every $n \in N$ it is true, that $|a_n - a| < \epsilon$.
Therefore $(a_n) \to a$, as desired.

\section*{2.3.2}
\textit{Let $x_n >\geq 0$ for all $n \in N$.}

\textit{(a) If $(x_n) \to 0$, show that $(\sqrt{x_n}) \to 0$}

Suppose that $(x_n) \to 0$. Then

$$\forall \epsilon \in R > 0: \exists N \in \textbf{N}: \forall n \in  > N:
|x_n - 0| < \epsilon$$.

$$|x_n - 0| < \epsilon$$
$$|x_n| < \epsilon$$
$$x_n < \epsilon$$
$$\sqrt{x_n} < \sqrt{\epsilon}$$
therefore
$$\forall \epsilon \in R > 0: \exists \epsilon = \sqrt{\epsilon} > 0:
\exists N \in \textbf{N}: \forall n \in  > N: |\sqrt{x_n} - 0| < \epsilon$$.
therefore
$$\forall \epsilon \in R > 0: 
\exists N \in \textbf{N}: \forall n \in  > N: |\sqrt{x_n} - 0| < \epsilon$$.
From which it follows that $(\sqrt{x_n}) \to 0$ by definition of a limit.

\textit{(b) If $(x_n) \to x$, show that $(\sqrt{x_n}) \to \sqrt{x}$}

Suppose that $(x_n) \to x$. Then

$$\forall \epsilon \in R > 0: \exists N \in \textbf{N}: \forall n \in  > N:
|x_n - x| < \epsilon$$.
$$|x_n - x|  < \epsilon$$
$$|(\sqrt{x_n} + \sqrt{x})(\sqrt{x_n} - \sqrt{x})|  < \epsilon$$
$$|\sqrt{x_n} - \sqrt{x}|  < \frac{\epsilon}{|\sqrt{x_n} + \sqrt{x}|}$$
$$ |\sqrt{x_n} + \sqrt{x}| \leq |\sqrt{x_n}| + |\sqrt{x}| = \sqrt{x_n} +
\sqrt{x} \to \frac{\epsilon}{|\sqrt{x_n} + \sqrt{x}|} <
\frac{\epsilon}{\sqrt{x_n} + \sqrt{x}}$$
$$|\sqrt{x_n} - \sqrt{x}|  < \frac{\epsilon}{\sqrt{x_n} + \sqrt{x}}$$
$(x_n)$ is convergent and therefore bounded. Therefore there exists
$M \in R > 0: x_n < M$. Therefore $\sqrt{x_n} < \sqrt{M}$. Thus
$\sqrt{x_n} + \sqrt{x} < \sqrt{M} + \sqrt{x}$. Therefore
$$|\sqrt{x_n} - \sqrt{x}|  < \frac{\epsilon}{\sqrt{M} + \sqrt{x}}$$


therefore
$$\forall \epsilon \in R > 0: \exists \epsilon_1 = \frac{\epsilon}{\sqrt{M}
  + \sqrt{x}} > 0:
\exists N \in \textbf{N}: \forall n \in  > N: |\sqrt{x_n} - \sqrt{x}|
< \epsilon$$.
therefore
$$\forall \epsilon \in R > 0: 
\exists N \in \textbf{N}: \forall n \in  > N: |\sqrt{x_n} - \sqrt{x}|
< \epsilon$$.
From which it follows that $(\sqrt{x_n}) \to x$ by definition of a limit.

\section*{2.3.3 (Squeezze Theorem).}
\textit{Show that if $x_n \leq y_n \leq z_n$ for all $n \in N$, and if
  $\lim x_n = \lim z_n = l$, then $\lim y_n = l$ as well.}

$$\forall n \in N: x_n \leq y_n \to \lim x_n \leq \lim y_n$$
$$\forall n \in N: y_n \leq z_n \to \lim y_n \leq \lim z_n$$
therefore
$$\forall n \in N: x_n \leq y_n \leq z_n \to  \lim x_n \leq \lim y_n \leq \lim z_n$$

Therefore
$$\lim x_n = \lim z_n = l \to \lim y_n = l$$
as desired.

\section*{2.3.4}
\textit{Show that limits, if they exist, must be unique. In other words,
  assume $\lim a_n = l_1$ and $\lim a_n = l_2$, and prove that $l_1 = l_2$.}

We will procede with a proof by contradiction.

Suppose $l_1 \neq l_2$. Then

$$\forall \epsilon > 0: \exists N \in N: n \geq N \to |a_n - l_1| \leq \epsilon$$
$$\forall \epsilon > 0: \exists N \in N: n \geq N \to |a_n - l_2| \leq \epsilon$$

Let $\epsilon = |l_1 - l_2|/2$. Then $\exists N_1 \in N$ such that $n_1 \geq N$
implies that 
$$|a_{n_1} - l_1| < |l_1 - l_2|/2$$
also there exists $N_2 \in N$ such that  $n_2 \geq N_2$ implies that 
$$|a_{n_2} - l_2| < |l_1 - l_2|$$
Let $n = max\{n_1, n_2\}$. Then
$$|a_n - l_1| < |l_1 - l_2|/2$$
$$|a_n - l_2| < |l_1 - l_2|/2$$
$$|a_n - l_1| + |a_n - l_2| < |l_1 - l_2|$$
$$|a_n - l_1| + |l_2 - a_n| < |l_1 - l_2|$$
then by triangular inequality
$$|a_n - l_1 + l_2 - a_n| \leq |a_n - l_1| + |l_2 - a_n| < |l_1 - l_2| $$
$$|a_n - l_1 + l_2 - a_n| < |l_1 - l_2| $$
$$| - l_1 + l_2| < |l_1 - l_2| $$
$$| l_1 - l_2| < |l_1 - l_2| $$
which is a contradiction. Therefore
$$\lim a_n = l_1 \text{ and } \lim a_n \to l_2 \to l_1 = l_2$$
as desired.

\section*{2.3.5}
\textit{Let $(x_n)$ and $(y_n)$ be given, and deefine $(z_n)$ to be the
  "shuffled" sequence $(x_1, y_1, x_2, y_2, x_3, y_3, ...)$. Prove that
  $(z_n)$ is convergent it and only if $(x_n)$ and $(y_n)$ are both
  convergent with $\lim x_n = \lim y_n $.}

\textbf{In the forward direction:}

Suppose that $(z_n)$ is convergent to some $l \in R$. Then 
$$\forall \epsilon > 0: \exists N \in \textbf{N}: n \geq N \to |z_n - l| < \epsilon$$
Because $(z_n)$ is "shuffled" we can follow that 
$$\forall \epsilon > 0: \exists N \in \textbf{N}: (n + 1) / 2 \geq N \to
|x_{(n + 1) / 2} - l| < \epsilon$$
$$\forall \epsilon > 0: \exists N \in \textbf{N}: n  / 2 \geq N \to |y_{n/2} - l| < \epsilon$$
Therefore if we let $m_1 = (n + 1) / 2$ and $m_2 = n/2$
$$\forall \epsilon > 0: \exists N \in \textbf{N}: m_1 \geq N \to |x_{m_1} - l| < \epsilon$$
$$\forall \epsilon > 0: \exists N \in \textbf{N}: m_2 \geq N \to |y_{m_2} - l| < \epsilon$$
Therefore $\lim x_n = \lim y_n = l$, as desired.

\textbf{In the backward direction:}

Suppose that both $(x_n)$ and $(y_n)$ are convergent to some $l$. Then

$$\forall \epsilon > 0: \exists N_1 \in \textbf{N}: m_1 \geq N_1 \to |x_{m_1} - l| < \epsilon$$
$$\forall \epsilon > 0: \exists N_2 \in \textbf{N}: m_2 \geq N_2 \to |y_{m_2} - l| < \epsilon$$

If we pick $N = (max\{N_1, N_2\} + 1) * 2$, then
$$\forall \epsilon > 0: \exists N \in \textbf{N}: n \geq N \to |x_{(n - 1) / 2 } - l| < \epsilon$$
$$\forall \epsilon > 0: \exists N \in \textbf{N}: m_2 \geq N \to |y_{n/2} - l| < \epsilon$$

If $n$ is odd then  $z_n = x_{(n - 1) / 2}$, and if $n$ is even
then $z_n = y_{n/2}$. Therefore 

$$\forall \epsilon > 0: \exists N \in \textbf{N}: n \geq N \to |z_n - l| < \epsilon$$

Thus $(z_n) \to l$ as well, as desired.

\section*{2.3.6}
\textit{Show that if $(b_n) \to b$, then the sequence of absolute values
  $|b_n|$ converges to $|b|$.}

As proven in 1.2.5
$$||a| - |b|| \leq |a - b|$$
Therefore  $ ||b_n| - |b|| \leq |b_n - b| $
Thus
$$\forall \epsilon > 0: \exists N \in \textbf{N}: n \geq N \to ||b_n| - |b|| < \epsilon$$
Thus $(|b_n|) \to |b|$, as desired.

\textit{(b) Is the converse of part (a) true? If we know that $|b_n| \to |b|$,
  can we deduce that $(b_n) \to b$?}

No. Glaring example is
$$a_n = (-1)^n$$

\section*{2.3.7}
\textit{(a) Let $(a_n)$ be a boundeed (non necessarily convergent) sequence,
  and assume $\lim b_n = 0$. Show that $\lim (a_n b_n) = 0$. Why are not
  allowed to use Algebraic Limit Theorem to prove that?}

Let $M > |a_n|$ for all $n \in N$ (it exists because of the boundness of $a_n$).
Then
$$ |a_n b_n| = |a_n||b_n| \leq M |b_n|$$
$$ |a_n b_n| \leq  M |b_n|$$
$$ -M|b_n| \leq a_n b_n \leq  M |b_n|$$

Both $(M|b_n|)$ and $(-M|b_n|)$ converge to 0. Therefore $(a_n b_n)$ converges
to 0 as well by Squeeze theorem.

A little sidenote: this statement can be proven with the standart definition
of limit as well, but that proof is longer and I wanted to use brand-new,
self-proven theorem on this one.

We can't use Algebraic Limit Theorem on that one beacause it prerequsites
that both sequences are convergent, and here it is not the case.

\textit{(b) Can we conclude anything about the convergence of $(a_n b_n)$
  if we assume that $(b_n)$ converges to some nonzero limit $b$?}

We can probably conclude that if $(a_n)$ is divergent, then $(a_n b_n)$ is
divergent as well, but we can't conclude nothing definitive about
just a bounded sequence.

\textit{(c) Use (a) for prove Theoremt 2.3.3, part (iii), for the case when
  $a = 0$.}

If $(b_n)$ is convergent (possibly to zero), then it is bounded.
Therefore, by (a), we can state that
$$(a_n b_n) \to a b = 0$$

But wait a second, we used something from the algebraic property in part (a)!
Doesn't in mean, that we proved a theorem by assuming, that it is true? No.
In part (a) we concluded, that $M|b_n|$ is convergent to 0 by using
part (i) of algebraic limit theorem. We didn't even use the case when
$M = 0$, and therefore we can sleep well.

\section*{2.3.8}
\textit{Give an example of each of the following, or state that such a
  request is impossible by referencing the proper theorem(s):}

\textit{(a) sequences $(x_n)$ and $(y_n)$, which both diverge, but
  whose sum $(x_n + y_n)$ converges}
$$(x_n) = n$$
$$(y_n) = -n$$

\textit{(b) sequences $(x_n)$ and $(y_n)$, where $(x_n)$ converges, $(y_n)$
  diverges, and $(x_n + y_n)$ converges}

Impossible.

I want to say, that the algebraic limit theorem prevents 
this statement to be true, but I don't know if we can apply it here.

Suppose that $(x_n + y_n)$ converges to $l$. Let $(ln) = l$ be a constant
sequence. Then

$$(x_n + y_n - l) \to 0$$
Also, because both $(x_n)$ and $(l)$ converge, $(l - x_n)$ converges as well.
therefore
$$(x_n + y_n - l+ l - x_n)$$ converges as well. Therefore $(y)$ converges,
which is a contradiction.

Turns out that yeah, it applies.

\textit{(c) a convergent sequence $(b_n)$ with $b_n \neq 0$ for all $n$ such
  that $(1/b_n)$ diverges}

$$(b_n) = 1/n$$.

\textit{(d) an unbounded sequence $(a_n)$ and a convergent sequence $(b_n)$
  with $(a_n - b_n)$ bounded;}

Impossible.

Same algebraic limit theorem with a bit of convergence to infinity

\textit{(e) Two sequences $(a_n)$ and $(b_n)$, where $(a_n b_n)$ and
  $(a_n)$ converge, but $(b_n)$ diverge.}

$$(a_n) = 1/n$$
$$(b_n) = (-1)^n$$

\section*{2.3.9}
\textit{Does Theorem 2.3.4 remain true, if all of the inequalities are
  assumed to be strict? If we assume, for instance, that a convergent
  sequence $(x_n)$ satisfies $x_n > 0$ for all $n \in N$, what we may conclude
  about the limit?}

If we swap all inequalities to strict ineqaulities, then the theorem is false.
Example: $(b_n) = 1/n \to 0$, but $b_n > 0$ for all n's.

\section*{2.3.10}
\textit{If $(a_n) \to 0$ and $|b_n - b| \leq a_n$, then show that $(b_n) \to b$.}

$(a_n) \to 0$ implies, that for every $\epsilon \in R > 0$ we can get
$|a_n| < \epsilon$. Therefore

$$\forall \epsilon \in R > 0: \exists N \in \textbf{N}: \forall n \geq N \to
|b_n - b| < a_n \leq |a_n| < \epsilon$$
$$\forall \epsilon \in R > 0: \exists N \in \textbf{N}: \forall n \geq N \to
|b_n - b|  < \epsilon$$
Therefore $(b_n) \to b$, as desired.

\section*{2.3.11 (Cesaro Means).}
\textit{Show that if $(x_n)$ is a convergent sequence, then the sequence given by
  the averages }
$$y_n = \frac{x_1 + x_2 + ... + x_n}{n}$$
\textit{also converges to the same limit.}

The strategy for the proof for this theorem  is to firtsly show, that this
statement is true for all of the sequences, that converge to
0. Then we'll show that this case is equivalent to the theorem in general.

First, let's prove that this is true for the case when  $(a_n) \to 0$

Strategy here is to show, that 

Suppose that we have a given $\epsilon \in R > 0$. Then there exists
$q \in N$ such that  $|a_q| < \epsilon$. Also, because of the fact,
that $(a_n) \to 0$ there exist  $ k \in N \geq q$
such that $|a_k| \geq |a_{q_1}|$ (i.e. maximum element of this set)
for all $q_1 \in N \geq q$.

Then, let
$$ N > \frac{|a_1| + |a_2| +  ... + |a_{q - 1}| - q|a_k|}{\epsilon -   |a_k|}$$

If, for some god fosaken reason, $q \geq N$, then we can set $N$ to be more
than $q$, and it wouldn't make the differenct. Same applies to $k$, if it
matters (it doesn't).

Therefore if we let $n > N$, then we can concur that  
$$ n > \frac{|a_1| + |a_2| +  ... + |a_{q - 1}| - q|a_k|}{\epsilon -   |a_k|}$$
$$\frac{|a_1| + |a_2| +  ... + |a_{q - 1}| - q|a_k|}{\epsilon -   |a_k|} <  n $$
$$|a_1| + |a_2| +  ... + |a_{q - 1}| - q|a_k| < \epsilon n -   |a_k| n $$
$$|a_1| + |a_2| +  ... + |a_{q - 1}| + |a_k| n  - q|a_k| < \epsilon n$$
$$\frac{ |a_1| + |a_2| +  ... + |a_{q - 1}|}{n} + |a_k| -\frac{ q|a_k|}{n} < \epsilon$$
$$\frac{ |a_1| + |a_2| +  ... + |a_{q - 1}|}{n} + \frac{n|a_k|}{n} -\frac{ q|a_k|}{n} < \epsilon$$
$$\frac{ |a_1| + |a_2| +  ... + |a_{q - 1}|}{n} + \frac{n|a_k| - q|a_k|}{n} < \epsilon$$
$$\frac{ |a_1| + |a_2| +  ... + |a_{q - 1}|}{n} + \frac{(n - q)|a_k|}{n} < \epsilon$$
$$\frac{ |a_1| + |a_2| +  ... + |a_{q - 1}|}{n} + \frac{|a_k| + ... + |a_k|}{n} < \epsilon$$
because $|a_{q+}| \leq |a_k|$ 
$$\frac{ |a_1| + |a_2| +  ... + |a_{q - 1}|}{n} + \frac{|a_{q}| + ... + |a_n|}{n} \leq \frac{ |a_1| + |a_2| +  ... + |a_{q - 1}|}{n} + \frac{|a_k| + ... + |a_k|}{n} < \epsilon$$
$$\frac{ |a_1| + |a_2| +  ... + |a_{q - 1}| + |a_{q}| + ... + |a_n|}{n} < \epsilon$$
$$\frac{ |a_1| + |a_2| + ... + |a_n|}{n} < \epsilon$$
$$|\frac{a_1 + a_2 + ... + a_n}{n}| = \frac{ |a_1 + a_2 + ... + a_n|}{n} \leq \frac{ |a_1| + |a_2| + ... + |a_n|}{n} < \epsilon$$
$$|\frac{a_1 + a_2 + ... + a_n}{n}| < \epsilon$$
$$|\frac{a_1 + a_2 + ... + a_n}{n} - 0| < \epsilon$$

Therefore
$$\forall \epsilon \in R > 0: \exists N \in \textbf{N}: \forall n > N \to
|\frac{a_1 + a_2 + ... + a_n}{n} - 0| < \epsilon$$

Therefore $(a_n) \to 0$ implies that $(\frac{a_1 + a_2 + ... + a_n}{n}) \to 0$
as well.

Now, proud of our achievement, we can proceed with the initial argument.
Suppose that $(x_n) \to l$. Then if we set $l_n = l$, then $(l_n) \to l$.
Therefore $(x_n - l_n) \to 0$. And because of it, we can use our initial
conclusion.
$$(\frac{x_1 - l_n + x_2 - l_n + ... + x_n - l_n}{n}) \to 0$$
$$(\frac{x_1 + x_2 + ... + x_n - l_n n}{n}) \to 0$$
$$(\frac{x_1 + x_2 + ... + x_n}{n} - \frac{l_n n}{n}) \to 0$$
$$(\frac{x_1 + x_2 + ... + x_n}{n} - l_n) \to 0$$
Therefore 
$$(\frac{x_1 + x_2 + ... + x_n}{n}) \to l$$

When push comes to shove, we can prove it axiomatically throuh
$$|\frac{x_1 - l_n + x_2 - l_n + ... + x_n - l_n}{n}| < \epsilon$$
$$|\frac{x_1 + x_2 + ... + x_n - n l_n}{n}| < \epsilon$$
$$|\frac{x_1 + x_2 + ... + x_n}{n} - \frac{n l_n}{n}| < \epsilon$$
$$|\frac{x_1 + x_2 + ... + x_n}{n} - l_n| < \epsilon$$
$$|\frac{x_1 + x_2 + ... + x_n}{n} - l| < \epsilon$$
Which comes to the same conclusion. Therefore now we can affirmatively
state that

$$(x_n) \to l \text{ implies } \frac{x_1 + x_2 + ... + x_n}{n} \to l$$
as desired.

\textit{Give an example to show that it is possible for the sequence $(y_n)$
  of averages to converge even if $(x_n)$ does not.}

$$x_n = ([[n / 2]])(-1)^n$$
Where $[[]]$ is a floor function.
Therefore
$$x_n = (0, 0, 1, -1, 2, -2, ...)$$


\section*{2.3.12}
\textit{Consider the doubly indexed array $a_{m,n} = m / (m + n))$.}

\textit{(a) Intuitively speaking, what should $\lim_{m,n \to \infty} a_{m,n}$
  represent? Compute the "iterated" limits}
$$ \lim_{n \to \infty} \lim_{m \to \infty} a_{m,n} \text{ and }
\lim_{m \to \infty} \lim_{n \to \infty} a_{m,n} $$

Intuitively speaking $\lim_{m,n \to \infty} a_{m,n}$ should mean that
the larger the $m$ and $n$ get, the closer $a_{m,n}$ becomes.

$$\lim_{n \to \infty} a_{m_n} = \lim_{n \to \infty} m/(m + n) =
\lim_{n \to \infty} m/m + m/n = 1$$  
$$\lim_{m \to \infty} \lim_{n \to \infty} a_{m,n} =
\lim_{m \to \infty}  1 = 1$$

$$\lim_{m \to \infty} a_{m,n} = \lim_{m \to \infty} m/(m + n) =
\lim_{m \to \infty} = m/m + m/n = \infty$$
$$\lim_{n \to \infty} \lim_{m \to \infty} a_{m,n} = \lim_{n \to \infty} \infty = \infty$$

\textit{(b) Formulate a rigorous definition in the style of Definition 2.2.3
  for the statement}
$$\lim_{m,n \to \infty} a_{m,n} = l$$

A list is a set of length n in form
$$\{a_1, \{a_1, a_2\}, \{a_1, a_2, a_3\}, ..., a_{n -1}, a_n\}\}\}\}... $$
It is denoted by
$$(a_1, a_2, a_3, ..., a_n)$$

A double indexed sequence $a_{m,n}$ is a function from the list $(j, k)$,
where $j, k \in N$ to $R$.

A sequence $(a_{m, n})$ converges to a real number $a$ if, for every positive
number $\epsilon$, there exist $N_1, N_2 \in \textbf{N}$ such that
whenever $m > N_1$ and $n > N_2$ it follows that $|a_{m,n} - a| <
\epsilon$.

\section*{2.4.1}
\textit{Complete the proof of Theorem 2.4.6 by showing that if the
  series $\sum^{\infty}_{n = 0} 2^n b_{2^n}$ diverges, then so does
  $\sum^{\infty}_{n = 1} b_n$. Example 2.4.5 may be a useful reference.}

First of all, let's state Theorem 2.4.6

\textbf{Theorem 2.4.6 (Caucht condensation Test)}
\textit{Suppose $(b_n)$ is decreasing and satisfies $b_n \geq 0$ for all
  $n \in \textbf{N}$. Then, the series $\sum_{n = 1}^{\infty}b_n$ converges
  if and only if the series }
$$\sum_{n = 0}^{\infty}2^n b_{2^n} = b_1 + 2 b_2 + 4 b_4 + 8 b_8 + ...$$
\textit{converges.}

Suppose that $\sum^{\infty}_{n = 0} 2^n b_{2^n}$ diverges. Then a partial
sum
$$s_{2^k} = b_1 + 2 b_2 + 4 b_4 + ... =
b_1 + b_2 + b_2 + b_4 + b_4 + b_4 + b_4 ... \leq
b_1 + b_2 + b_3 + b_4 + b_5 + b_6 + b_7 + ... = s_n $$

Therefore, partial sum for $\sum^{\infty}_{n = 0} 2^n b_{2^n}$ is less than
$\sum^{\infty}_{n = 1} b_n$. Therefore $\sum^{\infty}_{n = 1} b_n$ diverges,
as desired.

\section*{2.4.2}
\textit{(a) Prove that the sequence defined by $x_1 = 3$ and}
$$x_{n + 1} = \frac{1}{4 - x_n}$$
\textit{converges.}



Firstly, let us write a couple of elements of this sequence to see what's going on:
$$(x_n) = 3, 1, 1/3, 3/11, ...$$

Those elements of the sequence tell us, that the sequence is probably decreasing
and also bounded below by some constant around 0.25, and therefore bounded below
by 2. Also, the proof of the fact that the sequence is bounded above by 4 might
come in handy.

To prove the proposition, that this sequence  is convergent we will
use property, that if a sequence is bounded and monotone, then it is convergent.
Let's start with proposition that the this sequence is decreasing.

We'll use induction for this one, because it seems right;

Base: $x_1 < 4$.

Proposition: $x_n < 4$

Step:

$$x_{n + 1} = \frac{1}{4 - x_n} < 4$$
$$4 - x_n > \frac{1}{4}$$
$$- x_n > \frac{1}{4} - 4$$
$$x_n < 3\frac{1}{4}$$

Therefore $x_n < 4$. From this we can conclude that 
$$x_n < 4$$
$$4 - x_n > 0$$
$$\frac{1}{4 - x_n} > 0$$
$$x_{n + 1} > 0$$

Now let us prove, that the sequence is decreasing. We'll also do it with induction.

Base: $x_1 - x_2 = 3 - 1 = 2 > 0$

Proposition: $x_{n - 1} - x_n > 0$.

Step:
We had proved already, that $x_n > 0$, and $x_n < 4$ Therefore
$$x_n - x_{n + 1} > 0$$
$$ x_n - \frac{1}{4 - x_n} > 0$$
$$ (4 - x_n)x_n - 1 > 0$$
$$ 4 x_n - x_n^2 - 1 > 0$$
$$x_n^2 - 4 x_n + 1 < 0$$
$$(x_n - (2 - \sqrt{3}))(x_n - (2 + \sqrt{3})) < 0$$
It can be shown, that our sequence bounded by $2 - \sqrt{3}$ and $2 + \sqrt{3}$.

Thus the sequence is decrasing and bounded, and therefore convergent.

\textit{(b) Now thatwe know $\lim x_n$ exists, explain why $\lim x_{n + 1}$
  must also exist and equal the same value.}

By the definition of limit of a sequence, for every $\epsilon$ there exist an
$N$ such that for all $n \geq N$ $|x_n - l| < \epsilon$. Therefore, because
every $(x_{n + 1}) \subset (x_n)$ we can conclude, that $(x_{n + 1})$ converges
to the same value.

\textit{(c) Take the limit of each side of the recursive eqation in part (a) of
  this exercise to explicitly compute $\lim x_n$.}

$$\lim x_{n + 1} = \frac{1}{4 - \lim x_n}$$
$$\lim x_n = \frac{1}{4 - \lim x_n}$$
$$\lim x_n = \frac{1}{4 - \lim x_n}$$
$$(4 - \lim x_n)\lim x_n = 1$$
$$4 \lim x_n - (\lim x_n) ^ 2  - 1= 0$$
$$ (\lim x_n) ^ 2 - 4 \lim x_n  + 1= 0$$

From my 5th grade class I remember that.
$$\lim x_n = 2 -  \sqrt{3} \text{ or } \lim x_n = 2 +  \sqrt{3}$$

Given that $2 + \sqrt{3} > 3$ and that the sequence is decreasing and first
element of it is $3$, we can state that $x_n > 3$ for all n. Therefore
$$\lim x_n = 2 - \sqrt{3}$$

\section*{2.4.3}
\textit{Following the model of Exercise 2.4.2, show that the sequence defined
  by $y_1 = 1$ and $y_{n + 1} = 4 - 1/y_n$.}

Let's write down a few elements of this sequence:
$$y_1 = 1, 3, 11/3, 41/11, ...$$
This one is probably increasing and bounded above by 4 and below by 1

Base: $1 \leq y_1 = 1 \leq 4$

Proposition: $1 \leq y_n \leq 4$

Step:

$$1 \leq y_n \leq 4$$
$$1 \geq 1/y_n \geq 1/4$$
$$-1 \leq -1/y_n \leq -1/4$$
$$3 \leq 4 - 1/y_n \leq 3 \frac{3}{4}$$
$$1 \leq 4 - 1/y_n \leq 4$$
$$1 \leq y_{n + 1} \leq 4$$


$$1 \leq y_n \leq 2 + \sqrt{3}$$
$$1 \geq 1/y_n \geq \frac{1}{2 + \sqrt{3}}$$
$$-1 \leq -1/y_n \leq -\frac{1}{2 + \sqrt{3}}$$
$$3 \leq 4 - 1/y_n \leq 4  -\frac{1}{2 + \sqrt{3}}$$
$$3 \leq 4 - 1/y_n \leq \frac{8 + 4 \sqrt{3} - 1}{2 + \sqrt{3}}$$
$$3 \leq 4 - 1/y_n \leq \frac{7 + 4 \sqrt{3}}{2 + \sqrt{3}}$$
$$3 \leq 4 - 1/y_n \leq \frac{(2 + \sqrt{3})^2}{2 + \sqrt{3}}$$
$$3 \leq 4 - 1/y_n \leq 2 + \sqrt{3}$$

as desired.

Therefore $1 \leq y_n \leq 4$ for all $n \in N$.

Now, let us try to prove that the sequence is increasing

$$y_{n + 1} \geq y_n $$
$$y_{n + 1} - y_n \geq 0 $$
$$4 - 1/y_n - y_n \geq 0 $$
$$4y_n - 1 - y_n^2 \geq 0 $$
$$y_n^2 - 4 y_n + 1 \leq 0 $$
$$(y - (2 + \sqrt{3}))(y - (2 - \sqrt{3})) \leq 0 $$

Therefore  if we prove that the sequence is bounded by $2 + \sqrt{3}$ and
$2 + \sqrt{3}$, then we'll have our proof

Base: $2 - \sqrt{3} \leq y_1 = 1 \leq 2 + \sqrt{3}$

Proposition: $2 - \sqrt{3} \leq y_n \leq 2 + \sqrt{3}$

Step:
$$2 - \sqrt{3} \leq y_n \leq 2 + \sqrt{3}$$
$$\frac{1}{2 - \sqrt{3}} \geq 1/y_n \geq \frac{1}{2 + \sqrt{3}}$$
$$-\frac{1}{2 - \sqrt{3}} \leq -1/y_n \leq - \frac{1}{2 + \sqrt{3}}$$
$$4 -\frac{1}{2 - \sqrt{3}} \leq 4  -1/y_n \leq 4 - \frac{1}{2 + \sqrt{3}}$$
$$\frac{7 - 4 \sqrt{3}}{2 - \sqrt{3}} \leq y_{n + 1} \leq \frac{7 + 4 \sqrt{3}}{2 + \sqrt{3}}$$
Given that $7 + 4 \sqrt{3} = (2 + \sqrt{3})^2$ and $7 - 4 \sqrt{3} = (2 - \sqrt{3})^2$ 
$$2 - \sqrt{3} \leq y_{n + 1} \leq 2 + \sqrt{3}$$
Therefore all of elements of our sequence are bounded by $2 - \sqrt{3}$ and
$2 + \sqrt{3}$. Thus $y_{n + 1} \geq y_n$, and therefore the sequence is increasing.

Now we have the proof that the sequence is both bounded and increasing, and thus
converges to some number. After some calculation (and by using common sense) we can
also conslude, that the sequence converges to $2 + \sqrt{3}$.

\section*{2.4.4}
\textit{Show that }
$$ \sqrt{2}, \sqrt{2 \sqrt{2}}, \sqrt{ 2 \sqrt{ 2 \sqrt{2}}}, ... $$
\textit{converges and find the limit.}

If we write the same expression as in exerice but using powers instead of
radicals we'll get
$$ 2^{\frac{1}{2}}, 2^{\frac{3}{4}}, 2^{\frac{7}{8}}, ...$$ 

Therefore the sequence can be written as
$$x_n =  2 ^{\sum_{m = 1}^n 2^{-m}}$$

$2^m > 0$ for all $m \in N$, therefore the sum is increasing. Thus, the power, in
which we put 2 is also increasing, therefore the whole sequence is increasing.

Because each element of this sequence is a power of a positive number, we can
conclude, that all of the elements are bound below by 0.

Therefore we just need to prove that 
$$\sum_{m = 1}^{\infty} 2^{-m} = 1$$
and we'll have our proof.

We'll do it by using the properties of this sum. We can see that
$$\sum_{m = 1}^{n}2^{-m} + 2^{-n} = 1$$
$2^{-n} > 0$, and therefore $1 - \sum_{m = 1}^{n}2^{-m} = 2^{-n} > 0$ for
all $m \in N$.

Thus, for every $\epsilon > 0$ we can conclude that $\exists M \in \textbf{N}$ such
that for all $m \geq M$.
$$1/m < \epsilon$$
therefore
$$\sum_{m = 1}^{\infty} 2^{-m} = 1$$
and thus
$$(x_n) =  2 ^{\sum_{m = 1}^n 2^{-m}} \to 2^1 = 2$$

\section*{2.4.5 (Calculating Square Roots)}
\textit{Let $x_1 = 2$, and define }
$$x_{n+1} = \frac{1}{2}(x_n + \frac{2}{x_n})$$
\textit{a) Show that $x_n^2$ is always greater than 2, and then use this to prove
  that $x_n - x_{n + 1} \geq 0$. Conclude that $\lim x_n = \sqrt{2}$.  }

We'll use induction to show that $x_n^2$ is always greated than 2.

Base: $2 ^2 = 4 > 2$.

Proposition: $x_n ^ 2 > 2$

Step:
% $$(\frac{1}{2}(x_n + \frac{2}{x_n}))^2 = \frac{1}{4}(x_n ^ 2 + 4 + \frac{4}{x_n^2})$$
$$x_n^2 > 2$$
$$x_n^2 - 2 > 0$$
$$(x^2 - 2)^2 > 0$$
$$x_n ^ 4 - 4 x_n^2 + 4 > 0$$
$$x_n ^ 2 - 4 + \frac{4}{x_n^2} > 0$$
$$(x_n ^ 2 + 4 + \frac{4}{x_n^2}) > 8$$
$$\frac{1}{4}(x_n ^ 2 + 4 + \frac{4}{x_n^2}) > 2$$
$$x_{n + 1}^2 > 2$$
Thus $x_n^2 > 2$ for all $n \in \textbf{N}$.

Thus
$$x_n^2  > 2$$
$$x_n^2 - 2 > 0$$
$$\frac{1}{2}x_n - \frac{2}{2 x_n} > 0$$
$$x_n - x_{n + 1} = x_n - \frac{1}{2}(x_n + \frac{2}{x_n}) = \frac{1}{2}x_n - \frac{2}{2x_n} > 0$$

Therefore the sequence is decreasing. Now let us prove, that every element of the
sequence is positive.

Base: $x_1 > 0$

Step:
$$x_n> 0$$
$$x_n^2 > 0$$
$$x_n^2 > -2$$
$$x_n^2 + 2 > 0$$
$$x_n + \frac{2}{x_n} > 0$$
$$(x_n + \frac{2}{x_n}) > 0$$
$$\frac{1}{2}(x_n + \frac{2}{x_n}) > 0$$
$$x_{n + 1} > 0$$

Thus every element of the sequence is positive and decreasing. Therefore the
sequence is convergent (because it is bounded above by the first element and
below by 0).

Thus this sequence converges to a positive limit. Let $l = \lim x_n$. Then also
$l = \lim x_{n + 1}$. Thus

$$l = \frac{1}{2}(l + 2/l)$$
$$2 l = l + 2/l$$
$$l = 2/l$$
$$l^2 = 2$$
$$l = \sqrt{2}$$
as desired.

\textit{(b) Modify the sequence $(x_n)$ so that it converges to $\sqrt{c}$. }

$$x_1 = c$$.
$$x_n =  \frac{1}{2}(x_n + \frac{c}{x_n})$$

The proof that this sequence works is obtained through the same logic, as in part (a).

\section*{2.4.6 (Limit Superior)}
\textit{Let $(a_n)$ be a bounded sequence.}

\textit{(a) Prove that the sequence defined by $y_n = sup\{a_k: k \geq n\}$
  converges}

In order to prove that we need to show that this seqeunce is both bounded and
monotone (in this case, decreasing).

Suppose that $n_1 > n_2 \in \textbf{N}$. Then it follows that
$$\{a_k: k \geq n_1\} \subseteq \{a_k: k \geq n_2\}$$

Therefore 
$$\forall l \in \{a_k: k \geq n_2\} \to l \leq sup\{a_k: k \geq n_2\}$$
and thus
$$\forall l \in \{a_k: k \geq n_1\} \to l \leq sup\{a_k: k \geq n_2\}$$
(i.e. if a number $o$ is a supremum for $\{a_k: k \geq n_1\}$, then
it is an upper bound for $\{a_k: k \geq n_2\}$).
And thus $y_n \geq y_{n + 1}$. Therefore sequence $(y_n)$ is decreasing.

Also, because the sequence is bounded, $\exists M \in R > 0$ such that
$|a_n| \leq M$ for all $n \in N$. Thus, $y_n$ is bounded below by $-M$.
Therefore the sequence is decreasing and bounded. Therefore it converges.

\textit{(b) The limit superior of $(a_n)$, or $\lim\sup a_n$, is defined by}
$$\lim \sup a_n = \lim y_n$$
\textit{where $y_n$ is the sequence from part (a) of this exercise. Proveide
  a reasonable definition for $\lim\inf a_n$ and briefly explay why it always
  exists for any bounded sequence.}

For a bounded sequence $(a_n)$, $\lim\inf a_n$ is a limit of a sequence,
defined by
$$y_n = \inf\{a_k: k \geq n\}$$.

Because for any $n_1 > n_2 \in \textbf{N}$ it follows that
$\{a_k: k \geq n_1\} \subseteq \{a_k: k \geq n_2\}$, we
will get that the sequence $y_n$ is bounded, and because of the
boundness of $(a_n)$ we will get that the sequence is increasing.

\textit {(c) Prove that $\lim\inf a_n \leq \lim \sup a_n$ for every bounded
  sequence, and give an example of a sequence, for which this ineqality is
  strict.}

We'll use a proof by contradiction on this one.

Suppose that $\lim\inf a_n > \lim \sup a_n$. Then let $l = \lim\inf a_n -
\lim \sup a_n$.

Thus there exists $n \in N$ for which
$$|\sup\{a_k: k \geq n\} - \lim\sup a_n| < l/2$$
$$-l/2 < \sup\{a_k: k \geq n\} - \lim\sup a_n < l/2$$
$$ \lim\inf a_n - l/2 < \sup\{a_k: k \geq n\} + \lim\inf a_n - \lim\sup a_n <\lim\inf a_n +  l/2$$
$$ \lim\sup a_n + l/2 < \sup\{a_k: k \geq n\} + l < \lim\inf a_n + l/2$$
$$  \sup\{a_k: k \geq n\} + l/2 < \lim\inf a_n $$
$$  \sup\{a_k: k \geq n\} < \lim\inf a_n - l/2$$
Therefore $a_n $

At the same time there exists $m \in M$ for which
$$|\inf\{a_k: k \geq m\} - \lim\inf a_n| < l/2$$
$$-l/2 < \inf\{a_k: k \geq m\} - \lim\inf a_n < l/2$$
$$\lim\inf a_n -l/2 < \inf\{a_k: k \geq m\}  < l/2 + \lim\inf a_n$$
$$\lim\inf a_n - l/2 < \inf\{a_k: k \geq m\}  <  l/2 + \lim\inf a_n$$
Thus
$$  \sup\{a_k: k \geq n\} < \inf\{a_k: k \geq m\}$$

Pick $j = \max\{m, n\}$.  Thus 
$$\{a_k: k \geq n\} \subseteq \{a_k: k \geq j\}$$
$$\{a_k: k \geq m\} \subseteq \{a_k: k \geq j\}$$
Therefore
$$ \sup\{a_k: k \geq n\} \geq \sup \{a_k: k \geq j\}$$
$$ \inf\{a_k: k \geq n\} \leq \inf \{a_k: k \geq j\}$$
Thus 
$$ \sup \{a_k: k \geq j\} \leq \sup\{a_k: k \geq n\} < \inf\{a_k: k \geq m\}
\leq \inf \{a_k: k \geq j\} $$
$$ \sup \{a_k: k \geq j\} <  \inf \{a_k: k \geq j\} $$

Therefore supremum of the set is less than an infinum of a set, which is
a contradiction. Therefore $\lim\inf a_n \leq \lim \sup a_n$.

Example of a sequence, for which this inequality is strict is
$$a_n = -1^n$$

\textit{(d) Show that $\lim \inf a_n = \lim \sup a_n$ if and only if
  $lim a_n$ exists. In this case, all three share the same value.}

\textbf{In forward direction:}

Suppose that $\lim \inf a_n = \lim \sup a_n = l$. Then for every $\epsilon > 0$
there exists $n \in N$ such that
$$|\sup \{a_k: k \geq n\} - l| < \epsilon$$
Also there exists $m \in N$ such that 
$$|\inf \{a_k: k \geq m\} - l| < \epsilon$$
for the same $\epsilon$.

Let us set $j = \max\{n, m\}$. Then
$$-\epsilon < \sup \{a_k: k \geq j\} - l < \epsilon$$
$$ l - \epsilon < \sup \{a_k: k \geq j\} < \epsilon + l$$

Also
$$-\epsilon < \inf \{a_k: k \geq j\} - l < \epsilon$$
$$l - \epsilon < \inf \{a_k: k \geq j\} < l + \epsilon$$

For any $j \in N \geq k$
$$a_j \leq \sup \{a_k: k \geq j\} < \epsilon + l$$
$$l - \epsilon < \inf \{a_k: k \geq j\} \leq a_j$$
therefore
$$l - \epsilon < a_j < l + \epsilon$$
$$ - \epsilon < a_j - l <  \epsilon$$
$$|a_j - l| <  \epsilon$$

Therefore 
$$\forall \epsilon > 0: \exists N \in \textbf{N}: \forall n \geq N \to
|a_n - l| < \epsilon$$
or in other words, $\lim a_n = l = \lim \inf a_n = \lim \sup a_n$, as desired.

\textbf{In other direction: }

Suppose that $\lim a_n = l$. From this we can state that
$$|a_n - l| < \epsilon$$
$$ \epsilon < a_n - l < \epsilon$$
$$l - \epsilon < a_n  < \epsilon + l$$

Thus both $\sup \{a_k: k \geq n\}$ and $\inf \{a_k: k \geq n\}$
are bounded by $l - \epsilon$ and $l + \epsilon$. In other words
$$l - \epsilon \leq  \inf \{a_k: k \geq n\} \leq \sup \{a_k: k \geq n\} \leq \epsilon + l$$

This is true for all $\epsilon > 0$. Thus if we set $\epsilon = 1/j$
where $j \in N$, there exists appropriate $i \in N$ for which
$$l - 1/j \leq  \inf \{a_k: k \geq i\} \leq  \sup \{a_k: k \geq i\} \leq  l + 1/j$$

Therefore if we take a limit of all sides of this inequality  we can state that$$\lim (l - 1/j) \leq  \lim \inf \{a_k: k \geq i\} \leq \lim \sup \{a_k: k \geq i\} \leq \lim( l + 1/j)$$
$$l \leq  \lim \inf a_n \leq \lim \sup a_n \leq l$$
Here, the ability to take limits of all sides is justified by the fact, that
$\forall n \in N > k: a_n \geq b_n \to \lim a_n \geq \lim b_n$.

Thus $\lim \inf a_n = \lim \sup a_n = l$, as desired.

\section*{2.5.1}
\textit{Prove Theorem 2.5.2}

First of all, let us state the theorem itself.

\textbf{Theorem 2.5.2}
Subsequences of a convergent sequence converge to the same limit.

Suppose that $a_n$ is a convergent sequence,  $b_n$ is  a subsequence of
$a_n$, and $l$ is a limit of $a_n$.  Because $a_n$ is a convergent sequence
we can state that 
$$\forall \epsilon \in R > 0: \exists N \in \textbf{N}: \forall n \geq N \to
|a_n - l| < \epsilon$$

For each $N \in \textbf{N}$ there exists $M \in \textbf{N}$, such that
$M \geq N$ and  $b_M = a_N$. Also, for each $m > M \in \textbf{N}$ there exists
$n > N \in \textbf{N}$ such that $b_m = a_n$
Thus we can state that 
$$\forall \epsilon \in R > 0: \exists N \in \textbf{N}: \exists M \in \textbf{N} > N:  \forall m \geq M \to |b_m - l| < \epsilon$$
$$\forall \epsilon \in R > 0:  \exists M \in \textbf{N} :  \forall m \geq M \to |b_m - l| < \epsilon$$
or in other words, $(b_n) \to l$, as desired.

\section*{2.5.2}
\textit{(a) Prove that if an infinite series converges, then the associative
  property holds. Assume $a_1 + a_2 + a_3 + a_4 + a_5 + ...$ converges
  to a limit $L$ (i.e. the sequence of partial sums $(s_n) \to L$). Show that
  any regrouping of the terms}
$$(a_1 + a_2 + ... + a_{n_1}) + (a_{n_1 + 1} + a_{n_1 + 2} + ... + a_{n_2}) +
(a_{n_2 + 1} + a_{n_2 + 2} + ... + a_{n_3}) + ...$$
\textit{leads to a series that also converges to $L$.}

Suppose that we regrouped terms of the above-given sequence. Then each
of the terms will be defined by sequence of partial sums
$$s_1 = (a_1 + a_2 + ... + a_{n_1})$$
$$s_2 = (a_1 + a_2 + ... + a_{n_1}) + (a_{n_1 + 1} + a_{n_1 + 2 } + ... a_{n_2})$$
$$s_n = a_1 + ... + a_{n_n}$$

Therefore, $s_n$ is a subsequence of an original sequence. 
Therefore, because the original sequence converges to the same limit,
$s_n$ will be convergent to the same limit, as desired.

\textit{(b) Compare this result to the example discussed at the end of Section
  2.1 where infinite addition was shown not to be associative. Why doesn't
  our proof in (a) apply to this example?}

Our proof doesn't apply to any of the sequences, discussed in the 2.1
because each one  of them was not convergent , which is a
prerequisite for part (a).

\section*{2.5.3}
\textit{Give an example of each of the following, or argue that such a request
  is impossible.}

\textit{(a) A sequence that does not contain 0 or 1 as a term but contains
  subsequences , converging to each of those values}
\begin{equation}
  a_n =
  \begin{cases}
    1/n \text{ if n is even} \\
    1 - 1/n \text{ if n is odd}
  \end{cases}
\end{equation}

\textit{(b) A monotone sequence that diverges but has a convergent subseqence}

Suppose that we have such a sequqnce. Then, because  subseqeuence is
convergent, it is bounded. Thus, there exist $M > 0$ for which
$ |a_{n_j}| \geq M \to -M \leq a_{n_j} \leq M$. For each of elements
of the original sequemce, there exists an element of subsequence,
that is "further down the line" (i.e.
$\forall n \in N: \exists m, j \in N: n < m \text{ and } a_m =
a_{n_j}$). Then
$$a_n \leq a_{m_j} \leq M$$
in case of the increasing sequence
$$a_n \geq a_{m_j} \geq -M$$
in case of the decreasing sequence.

Thus, the original sequence is bounded. Thus the original sequence is bounded
and monotone. Therefore the original sequence is convergent. Therefore
we have a contradiction.

\textit{(c) A sequence that contains subsequences converging to every point
  in the infinite set $\{1, 1/2, 1/3, 1/4, 1/5, ....\}$}

Let
$$a_n = [1], [1, 1/2], [1, 1/2, 1/3], [1, 1/2, 1/3, 1/4], [1, 1/2, 1/3, 1/4, 1/5], ...$$
where $[$ and $]$ are added just for the visual clue.

Therefore we have in it a subsequence, that converges to an arbitrary number
$1/n$ for $n \in N$.

\textit{(d) An unbounded sequence with a convergent subsequence.}

\begin{equation}
  a_n =
  \begin{cases}
    n \text{ if n is odd } \\
    0 \text{ if n is even }
  \end{cases}
\end{equation}

\textit{(e) A sequence that has a subsequence that is bounded but contains no
  subsequence that converges}

Impossible. Because there exists bounded subsequence, which is in and of itself
a sequence, there exists a convergent subsequence of a subsequence ( by
Bolzano-Weierstrass Theorem). Subsequence of a subsequence is a subsequence of
original sequence. Thus an original sequence contains a convergent
subsequence.

\section*{2.5.4}
\textit{Assume $(a_n)$ is a bounded sequence with the property that every
  convergent subsequence of $(a_n)$ converges to the same limit $a \in R$.
  Show that $(a_n)$ must converge to $a$.}


Because $(a_n)$ is bounded, there exists $M > 0$ such that
$|a_n| < M$.

Suppose that we have an $\epsilon$-neighborhood around $a$. Then
$[-M, M] \setminus V_{\epsilon}(a)$ could contait elements of $(a_n)$.
Suppose that $[-M, M] \ V_{\epsilon}(a)$ contains infinite amount of elements
of $(a_n)$. Then there exists a subsequence of $(a_n)$, that
it is outside of  $V_{\epsilon}$. Thus, there exists a convergent subsequence
of sequence, that converges to some number, other, than $a$, which is a
contradiction. Therefore there exists only finite amount of elements in
$[-M, M] \setminus V_{\epsilon}(a)$.

Therefore, for any $\epsilon$, there exists an $N$ (which is
a maximum index of a finite elements of elements outside the neighborhood, or
1 in case that there are no elements outside of the neighborhood),
for which it is true, that $n > N \in \textbf{N} \to a_n \in V_{\epsilon}(a)$.
Therefore, $(a_n) \to a$ by topological version of definition of convergence.

\section*{2.5.5}
\textit{Extend the result proved in Example 2.5.3 to the case $|b| < 1$.
  Show that $\lim (b^n) = 0$ whenever $-1 < b < 1$.}

Let $-1 < b < 1$. Then we need to prove that
$$\forall \epsilon: \exists N \in \textbf{N}: \forall n \geq N \to
|b^n - 0| < \epsilon$$
$$\forall \epsilon: \exists N \in \textbf{N}: \forall n \geq N \to
|b^n| < \epsilon$$
We already know, that for $0 < b_1 < 1$ it is true, that $\lim (b_1^n) = 0$.
Thus, for $b = 0$, the case is triival, and for $b < 0$ it defaults to
$|b| = b_1$. Therefore $\lim (b^n) = 0$ for $-1 < b < 1$.

\section*{2.5.6}
\textit{Let $(a_n)$ be a bounded sequence, and define the set }
$$S = \{x \in R: x < a_n \text{ for indefinetly many terms }a_n \}$$
\textit{Show that there exists a subsequence $(a_{n_k})$ converging to
  $s = \sup S$. (This is a direct proof of the Bolzano-Weierstrass Theorem
  using Axiom of Completeness.) }

The wording of this exercise doesn't make it easy to understand what exactly
are we trying to prove. After some pondering, I concluded, that what this
particular exercise is trying to say, is that there exists a subsequence,
that is convergent to a limit supremum of this sequence. If it does, then
some rewording of 2.4.6 part(a) will give you the desired result. If it
doesn't, then I don't know what do I need to prove, and therefore
declare this particular exerice as unfinished.

This exercise already took too much time which was spent on anything but
math, therefore instead of pondering further, I will rather spend time on
other exerices from this book.

After some research on this topic, I found out that there exist another
proof of Bolzano-Weierstrass Theorem, that doesn't use NIP, and my idea
now is that this particular exercise tried to ask us to give that proof.
That proof can be (kinda) derived from the limit superior theorem, by
the fact, that if a limit superior exists, then there exists 
a subsequence in the original sequence, that converges to limit
superior.

\section*{2.6.1}
\textit{Give an example of each of the following, or argue that such a request
  is impossible }

\textit{(a) A Cauchy sequence that is not monotone}
$$a_n = (-0.5)^n$$
It convergent (as proven in previous exercises), and is a Cauchy sequence
(because it is convergent)

\textit{(b) A monotone sequence that is not Cauchy}

$$a_n = n$$
It is not convergent, therefore not a Caucy sequence.

\textit{(c) A Cauchy sequence with a divergent subsequence.}

Impossible, because a Cauchy sequence is a convergent sequence, and
all of the subsequences of convergent sequence are convergent to the
same number.

\textit{(d) An unbounded sequence containing a subsequence that is Cauchy}

\begin{equation}
  a_n =
  \begin{cases}
    n \text{ if n is odd } \\
    0 \text{ if n is even}
  \end{cases}
\end{equation}

\section*{2.6.2}
\textit{Supply a proof for Theorem 2.6.2}

Firstly, let us state the theorem itself

\textbf{Theorem 2.6.2}
\textit{Every convergent sequence is a Cauchy sequence}

Suppose that $(x_n)$ converges to $x$. Thus,
$$\forall \epsilon/2 > 0: \exists N \in \textbf{N}: \forall n \geq N \to
|x_n - x| < \epsilon/2$$

Let $m > N$. Then
$$|x_n - x| < \epsilon/2$$
$$|x_m - x| < \epsilon/2$$
thus
$$|x_n - x| + |x_m - x| < \epsilon$$
$$|x_n - x| + |x - x_m| < \epsilon$$
$$|x_n - x + x - x_m| \leq |x_n - x| + |x - x_m| < \epsilon$$
$$|x_n - x + x - x_m| < \epsilon$$
$$|x_n - x_m| < \epsilon$$

Thus any convergent sequence is a Cauchy sequence, as desired.

\section*{2.6.3}
\textit{(a) Explain how the following pseudo-Cauchy property differs
  from the proper definition of a Cauchy sequence: A sequence $(s_n)$ is
  pseude-Cauchy if, for all $\epsilon > 0$, there exists an $N$ such that
  if $n \geq N$, then $|s_{n + 1} - s_n| < \epsilon$.}

It obviously differs in the fact, that in given definition we are only
onsidering the element, that goes after one element, instead of
all of elements, that are after.

\textit{(b) If possible, give an example of a divirgent sequence $(s_n)$ that
  is pseudo-Cauchy}

Harmonic series.

\section*{2.6.4}
\textit{Assume $(a_n)$ and $(b_n)$ are Cauchy sequences. Use a triangle
  inequality argument to prove $c_n = |a_n - b_n|$ is Cauchy.}

Therefore for each $\epsilon$ there exists
$$\forall \epsilon/2 > 0: \exists N_1 \in \textbf{N}: \forall n,m \geq N_1 \to
|a_n - a_m| < \epsilon/2$$
$$\forall \epsilon/2 > 0: \exists N_2 \in \textbf{N}: \forall n,m \geq N_2 \to
|b_n - b_m| < \epsilon/2$$

Let us pick $N = \max\{N_1, N_2\}$. Then
$$|a_n - a_m| + |b_n - b_m| < \epsilon$$
$$|c_n - c_m| = ||a_n - b_n| - |a_m - b_m|| \leq |a_n - b_n - a_m + b_m|
\leq |a_n - a_m| + |b_n - b_m| < \epsilon$$
$$|c_n - c_m|  < \epsilon$$
Thus $(c_n)$ is also Cauchy, as desired.

\section*{2.6.5}
\textit{If $(x_n)$ and $(y_n)$ are Cauchy sequences, then one easy way to
  prove that $(x_n + y_n)$ is Cauchy is to use the Cauchy Criterion. By
  Theorem 2.6.4, $(x_n)$ and $(y_n)$ must be convergent, and the Algebraic
  Limit Theorem then implies $(x_n + y_n)$ is convergent and hence Cauchy}

\textit{(a) Give a direct argument that $(x_n + y_n)$ is a Cauchy
  sequence that does not use the Cauchy Criterion or the Algebraic Limit
  Theorem.}

Suppose that $(x_n)$ and $(y_n)$ are Cauchy sequences. Therefore
$$\forall \epsilon/2 > 0: \exists N_1 \in \textbf{N}: \forall m,n \geq N_1 \to
|x_n - x_m| < \epsilon/2$$
and
$$\forall \epsilon > 0: \exists N_2 \in \textbf{N}: \forall m,n \geq N_2 \to
|y_n - y_m| < \epsilon/2$$

Therefore let $N = \max\{N_1, N_2\}$. Then
$$\forall \epsilon/2 > 0: \exists N \in \textbf{N}: \forall m,n \geq N \to
|x_n - x_m| < \epsilon/2$$
and
$$\forall \epsilon > 0: \exists N \in \textbf{N}: \forall m,n \geq N \to
|y_n - y_m| < \epsilon/2$$

Thus
$$|x_n - x_m| + |y_n + y_m| < \epsilon$$
$$|x_n - x_m + y_n - y_m | = |x_n - x_m + y_n - y_m| \leq |x_n - x_m| + |y_n + y_m| < \epsilon$$
therefore $(x_n + y_n)$ is a Cauchy sequence as well, as desired.

\textit{(b) Do the same for the product $(x_n y_n)$.}

Suppose that $(x_n)$ and $(y_n)$ are Cauchy sequences. Therefore
they are bounded by $|x_n| < M_1$ and $|y_n| < M_2$. Let us pick the
greatest $M = \max\{M_1, M_2\}$. Then
$$\forall \epsilon/2 > 0: \exists N_1 \in \textbf{N}: \forall m,n \geq N_1 \to
|x_n - x_m| < \frac{\epsilon}{2M}$$
and
$$\forall \epsilon > 0: \exists N_2 \in \textbf{N}: \forall m,n \geq N_2 \to
|y_n - y_m| < \frac{\epsilon}{2M}$$

Thus 
$$|x_n y_n - x_m y_m| = |x_n y_n - x_n y_m + x_n y_m - x_m y_m | =
|x_n (y_n - y_m) +  y_m( x_n - x_m) | \leq$$
$$ \leq |x_n||y_n - y_m| + |y_m| |x_n - x_m| \leq M \frac{\epsilon}{2M} +
M \frac{\epsilon}{2M} = \frac{\epsilon}{2} + \frac{\epsilon}{2} = \epsilon$$

Therefore $(x_n y_n)$ is a Cauchy sequence as well.

\section*{2.6.6}

\textit{(a) Assume the Nested Interval Property (Theorem 1.4.1) is true and
  use a technique similar to the one employed in the proof of the
  Bolzano-Weierstrass Theorem to give a proof for the Axiom of Completeness
  (The reverse implication was given in Chapter 1. This shows, that AoC is
  equivalent to NIP}

Suppose that we have a bounded set $A$. Then, because it is bounded, there
exists an interval $I_1 = [a, b]$, where $a \in A$, and
$b \in \textbf{R} \setminus A$ - upper bound of $A$. Then, let us
divide this set (and in general set $I_n$) into two sets of equal
length $[a, l]$ and $[l, b]$.
Then set $I_{n + 1}$ to the set, $[l, b]$, if it has any elements of $A$;
otherwise set it to $[a, l]$. Because of the NIP
$$\cap_{n = 1}^{\infty} I_n  = B\neq \emptyset$$

Suppose now, that there exist two elements $c < d \in B$. Then let us
look at the interval $[c, d]$ with length $h$. Because of the Archimedian
property, there exists a $n \in N$, such that $1/n < |d - c|$, and thus
there must exists $1/2^n < |d - c|$. Therefore there exists a set $I_j$, which
length is less than distance between $c$ and $d$. Therefore we
have a contradiction. (The absolute value here is redundant, but it doesn't
hurt). Therefore there exists only one number in $B$. Let us henceforth call
it $l$.

Suppose, that there exists  $q \in A$, such that $q > l$. Then it follows,
that there exists an interval $[q, l]$. By the same logic, as in the previous
paragraph, we can conclude, that such element does not exist. Therefore,
for all $q \in A$ it is true, that $q \leq l$. Thus, $l$ is an upper bound.

Suppose now, that $r$ is an upper bound of $A$ with the property, that
$r < l$. Then $[r, l]$ contains no elements of $A$. Therefore there exists
$I_g \subseteq [r, l]$ such that it contains no elements of $A$,
which is a contradiction. Therefore, for any upper bound $r$ it is true, that
$r \geq l$.

Therefore $l$ is an upper bound with the property, that any  upper bound
is either greater or equal to $l$. Thus, $l$ is a lowest upper bound
(or supremum) of $A$. Therefore, by existence of such a bound,
any bounded set of $R$ has a least  upper bound. In other words,
$$NIP\iff AoC$$
, as desired.

\textit{(b) Use the Monotone Convergence Theorem to give a proof of the
  Nested Interval Property. (This extablishes the equivalence of AoC,
  NIP, and MCT}

Suppose, that we have nested intervals
$$I_1 \subseteq I_2 \subseteq I_3 \subseteq I_4 ... \subseteq I_n \subseteq ...$$

Then, let $(a_n) = \text{lower bound of }I_n$. Then it follows, that
$$a_1 \leq a_2 \leq a_3 ... \leq a_n \leq \text{ upper bound of any of } I_n$$
Therefore, this sequence is increasing  and bounded above (and below by $a_1$).
Thus, it is convergent to some number $l_1 \in R$.

Let us also define $(b_n) = \text{upper bound of }I_n$
$$b_1 \geq b_2 \geq b_3 ... \geq b_n \geq \text{ lower bound of any of } I_n$$
Therefore, this sequence is decreasing  and bounded below (and above by $b_1$).
Thus, it is convergent to some number $l_2 \in R$.

Now look at $l_1$. Let us pick $I_j$ with lower  bound $a_1$ and upper bound
$b_1$. Suppose, that $l_1 < a_1$. Then let $\epsilon = a_1 - l_1$. Then it
follows, that all of the elements of the sequence after $g$ (i.e. $a_n$ for $n \geq g$) are not in $V_\epsilon(l_1)$. Thus, we have a contradition. Thus,
$l \geq a_1$. Wit hthe same logic and opposite sign we can prove that
$l \geq b_1$. Thus, $l \in I_g$ for any $g \in N$. Thus,
$$l \in \cap_{n = 1}^{\infty}I_n$$
Thus, the intersection of any number of nested sets is non-empty. Thus,

$$MCT \iff NIP$$
, as desired.

\textit{(c) This time, start with the Bolzano-Weierstrass Theorem and
  use it to construct a proof of the Nested Interval Property. (Thus, BW
  is equivalent to NIP, and hence to AoC and MCT as well.)}

Suppose that we have nested intervals
$$I_1 \subseteq I_2 \subseteq I_3 \subseteq I_4 ... \subseteq I_n \subseteq ...$$

Now let us define a sequence, such that every member of a sequence is
a number is a corresponding interval $I_n$.
$$(a_n) = \{x: x \in I_n\}$$

First of all, this sequence is going to be bounded by $I_1$. Therfore, by
BW, it has a convergent subsequence. Let us call this subsequence $(a_{n_j})$,
and call a limit of such a sequence $l$. Then by the same logic as in part (b),
$l \in I_g$ for any $g \in \textbf{N}$, and therefore
$$l \in \cap_{n = 1}^{\infty}I_n$$
Thus,
$$BW \iff NIP$$

\textit{(d) Finally, use the Cauchy Criterion to prove the Bolzano-Weierstrass
  Theorem. This is a final link in the equivlence of the five characterizations
  of completeness discussed at the end of Section 2.6}

Suppose that we have a bounded sequence $(a_n)$. Then, it follows, that
there exists $M > 0$ such that $a_n \in [-M, M]$ for any $n \in N$.
We are going to decribe how to get from $I_j$ to $I_{j + 1}$, and
by extension from $a_{n_j}$ to $a_{n_{j + 1}}$. For the complete costruction
let $I_1 = [-M, M]$ and proceed from there.
Now let's start constructing sequence.

Pick $a_{n_{j}} $ from interval $I_j = [a, b]$. Then divide $I_j$ into two
equal-length intervals $[a, l]$ and $[b, l]$. As discussed in previous
exercises, one of them will hav infinite amount of elements. Let $I_{j + 1}$
be interval with infinite amount of elements of $(a_n)$.

Therefore for each of $\epsilon > 0$ there will exists $j \in N$ such that
it will be true, that $I_j \subseteq V_\epsilon(a_j)$. And thus will contain
every  element of subsequence $\{a_{n_k}: k \geq j\}$ (the proof for validity
of this statement was already discussed in original proof of BW). Thus,
$(a_{n_j})$ will be a Cauchy sequence, and therefore convergent to some
value. Therefore
$$CC \iff BW$$

\section*{2.7.1}
\textit{Proving the Alternating Series Test (Theorem 2.7.7) amounts to
  showing that the sequence of partial sums }
$$s_n  = a_1 - a_2 + a_3 - ... \pm a_n$$
\textit{converges. (The opening example in Section 2.1 includes a
  typical illustration of $(s_n)$.) Different characterizations of
  completeness lead to different proofs.}

Let us state the theorem itself first:

\textbf{Theorem 2.7.7 (Alternating Series Test)}
\textit{Let $(a_n)$ be a sequence satisfying}
$$\text{(i) } a_1 \geq a_2 \geq a_3 \geq ... \geq a_n \geq ... \text{ and }$$
$$\text{(ii) } (a_n) \to 0$$
\textit{Then, the alternating series $\sum_{n = 1}^{\infty}(-1)^{n + 1} a_n$
converges}

Firstly, let us draw some conclusions from the theorem. If $a_n < 0$ for
some $n \in N$, then all of the $a_j$ for $j > n$ are $a_j \leq a_n < 0$, and
thus it does not converge to zero, which is a contradiction. Thus we
can state that $a_n \geq 0$ for all $n \in N$.

\textit{(a) Prove the Alternating Series Test by showing that $(s_n)$ is a
  Cauchy sequence.}


Let $\epsilon > 0$. Then, because $(a_n)$ converges, there exist
$N \in \textbf{N}$ such that for all $n > N$ it follows that
$|a_n| < \epsilon$.

Let $s_n = a_1 - a_2 + a_3 - ... \pm a_n$. Now let
$s_m = a_1 - a_2 + a_3 - ... \pm a_m$ for $m \geq n$. Then
$$|s_m - s_n| = |a_{n + 1} - a_{n + 2} + ... \pm a_m|$$

Let us examine isnterval $[0, a_n]$ with regards to elemets, that follow $a_n$:
$$a_n \geq a_{n + 1} \to a_n - a_{n + 1} \in [0, a_n]$$
Also, because $a_n \geq a_{n + 1} \geq a_{n + 2}$ we can state that
$a_n - a_{n + 1} + a_{n + 2} \in [0, a_n]$.  Thus
$$[a_n - a_{n + 1} + a_{n + 2}, a_{n + 2}] \subseteq
[a_n - a_{n + 1}, a_n] \subseteq [0, a_n]$$
Thus
$$ a_n \geq a_{n + 1} - a_{n + 2} + a_{n + 3} - ... \pm a_m$$
for any $m \geq n$.
Therefore
$$|s_m - s_n| = |a_{n + 1} - a_{n + 2} + ... \pm a_m| \leq a_n < \epsilon$$


Therefore we can state that for any $\epsilon > 0$ there exist $N$, such that for all
$n, m \geq N$ it follows that
$$|s_n - s_m| \leq a_n < \epsilon$$
Therefore $(s_n)$ is a Cauchy sequence. Thus it converges, as desired.

\textit{(b) Supply another proof for this result using the NIP (Theorem 1.4.1).
}

Let us define  interval $[a_n - a_{n + 1}; a_n]$ for some $n \in N$.
We can see that
$a_n$ and $a_n - a_{n - 1}$ are in this interval. Because
$a_{n + 2} \geq 0$ and  $a_{n + 2} \leq a_{n + 1}$ we can state that
$$a_n - a_{n + 1} \leq a_{n + 2} + a_n - a_{n + 1} \leq a_n$$
It also doesn't hurt to state here that
$$a_{n + 2} + a_n - a_{n + 1} \leq a_n \to [a_n - a_{n + 1}, a_{n + 2} + a_n - a_{n + 1}] \subseteq [a_n - a_{n + 1}; a_n]$$

Now let us define
$$I_n = [a_{2n} - a_{2n + 1}, a_{2n}]$$
from our discussion in previous paragraph we can state that
$$a_{2_n + m} \in I_n$$
where $m \in N \cup \{0\}$. Also
$$I_n \subseteq I_{n+1}$$
For the future use let us also note the fact, that for every $j \in N$ there
exists an interval $I_n$, whose length is less or equal to $a_j$.

Thus we have a collection of nested intervals. NIP states that
$$\cap_{n = 1}^{\infty} I_n = B \neq \emptyset$$
Suppose that we have $a \neq b \in B$. Then there exist $\epsilon = |a - b|$,
for which by convergence of $(a_n)$ there exists $N$ for which it is true that
$$\forall n \geq N \to |a_n| \leq \epsilon$$
Therefore there exists  an interval, whose length is less than the distance
betweeen $a$ and $b$. Therefore both $a$ and $b$  cannot be at some $I_n$ at
the same time. Therefore  we have a contradiction, and we can state that we
cannot have two distinct elements in $B$. Thus,  there exists
only one element in $B$.

Note that for every partial sum $s_n$ we can find corresponding $I_m$ such that
$$s_n \in I_m$$
Thus we can state that
$$(s_n)\to l$$
or in other words, 
$$\sum_{n = 1}^{\infty} (-1)^{n + 1}a_n = \lim s_n = l$$
where $l$ is the sole element of $B$.

\textit{(c) Consider the subsequences $(s_{2n})$ and $(s_{2n + 1})$, and show
  how the Monotone Convergence Theorem leads to a third proof for the
  Alternating Series Test}

Let us note that
$$s_{2n} = a_1 - a_2 + ... + a_{2n - 1} - a_{2n}$$
Thus
$$s_{2n} - s_{2(n + 1)} = a_{2n + 1} - a_{2_n + 2} \geq 0$$
thus series $(s_{2n})$ is increasing. Also
$$s_{2n + 1 } - s_{2(n + 1) + 1} = - a_{2n + 2} + a_{2_n + 3} \leq 0$$
thus series $(s_{2n + 1})$ is decreasing.

Because $(s_{2n + 1})$ is decreasing, there exists $a = \sup(s_{2n + 1})$.
Because $(s_{2n})$ is increasing, there exists $b = \inf(s_{2n})$.
Every $s_j$ is either equal to some $s_{2n}$ if $j$ is even or to some
$s_{2_n + 1}$ if j is odd. Thus $(s_n)$ is bounded above by $a$ and bounded
below by $b$.

By MCT we can state that 
$$\lim s_{2n} = l_1$$
$$\lim s_{2n + 1} = l_2 $$
Now let note that for all $n \in N$
$$s_{2n} - s_{2n + 1} = a_{2n}$$
Because all subsequences of convergent sequence converge to the same number
we can state that 
$$\lim(s_{2_n} - s_{2n + 1}) = \lim(a_{2n}) = 0$$
thus
$$\lim s_{2n} = \lim s_{2n + 1} = l_1 = l_2 = l$$

Because for every $n$ there exists $s_{2n} \leq s_j$ and
$s_{2n + 1} \leq s_j$
we can conclude that by Squeeze theorem
$$\sum_{n = 1}^{\infty} (-1)^{n + 1}a_n = \lim{s_n} = l$$
as desired.

\section*{2.7.2}
\textit{(a) Provide the details for the proof of the Comparison Test
  (Theorem 2.7.4) using the Cauchy Criterion for Series.}

Let us firstly state the theorem itself

\textbf{Theorem 2.7.4 (Comparison Test)}
\textit{Assume $(a_k)$ and $(b_k)$ are sequences satisfying
  $0 \leq a_k \leq b_k$ for all $k \in N$.}

\textit{(i) If $\sum^{\infty}_{k = 1}b_k$ converges,
  then $\sum^{\infty}_{k = 1}b_k$ converges}

\textit{(ii) If $\sum^{\infty}_{k = 1}a_k$ diverges,
  then $\sum^{\infty}_{k = 1}b_k$ diverges}

\textbf{ (i)}

Let us pick some $\epsilon > 0$. Then, by Cauchy Criterion for sequences,
there exists $m > n \geq N \in textbf{N}$.
such that 
$$|s_n - s_m| = |b_{m + 1} + ... + b_n| = b_{m + 1} + ... + b_n < \epsilon$$
for $s_n$ - partial sum for $\sum^{\infty}_{k = 1}b_k$.

Let $l_n$ be a partial sum for $\sum^{\infty}_{k = 1}a_k$. Then
$$|l_n - l_m| = |a_{m + 1} + ... + a_n| = a_{m + 1} + ... + a_n \leq
b_{m + 1} + ... + b_n = |s_n - s_m| \leq \epsilon$$

Thus, by Cauchy Criterion for Series,  $\sum^{\infty}_{k = 1}a_k$ converges.

\textbf{(ii)}

By Cauchy Criterion for sequences (or rather by negation of it),
there exists $\epsilon > 0$ such that 
for all  $m > n \in N$ it is true that 
$$|l_n - l_m| = |a_{m + 1} + ... + a_n| = a_{m + 1} + ... + a_n > \epsilon$$
for $l_n$ - partial sum for $\sum^{\infty}_{k = 1}a_k$.

Let $s_n$ be a partial sum for $\sum^{\infty}_{k = 1}b_k$. Then
$$\epsilon < |l_n - l_m| = |a_{m + 1} + ... + a_n| = a_{m + 1} + ... + a_n \leq
b_{m + 1} + ... + b_n = |s_n - s_m|$$

For all $m > n \geq N$ 

Thus, by Cauchy Criterion for Series,  $\sum^{\infty}_{k = 1}b_k$ diverges.

\textit{(b) Give another proof for the Comparison Test, this time using the
  Monotone Convergence Theorem}

Firstly, let us look closer at $\sum^{\infty}_{k = 1}(b_k)$: first
thing that we could
notice, is that every term of the sum is non-negative, and therefore
we can state that the sequence of partial sums for it  is increasing.
Same can be said about $\sum^{\infty}_{k = 1}(a_k)$.

\textbf{(i)}

Let us define  $s_k$ to be  a partial sum for
$\sum^{\infty}_{k = 1}(b_k)$ and $l_k$ to be a partial sum for
$\sum^{\infty}_{k = 1}(a_k)$. Because
$s_k$ is increasing and convergent, it is true that
$$s_k \leq l$$
for any $k \in N$ where $l = \lim s_k$. Thus
$$l_k \leq s_k \leq l$$
for any $k \in N$. Therefore $l_k$ is bounded above by $l$ as well.
Bringing up to our attention again that $a_k$ is non-negative, we can
state that
$$0 \leq l_k \leq l$$
thus $\sum^{\infty}_{k = 1}(a_k)$ is bounded and monotone and therefore by MCT,
convergent.

\textbf{(ii)}

Here we will not neccesarily use MCT itself, but the fact, that any convergent
function is bounded.

Partial sums for $\sum^{\infty}_{k = 1}(a_k)$ are not convergent and
increasing, therefore we can state that they are unbounded, precisely
that it is convergent to infinity. Because $l_k \leq s_k$, the same can be said
about partial sums for  $\sum^{\infty}_{k = 1}(b_k)$.

\section*{2.7.3}
\textit{Let $\sum a_n$ be given. For each $n \in N$, let $p_n = a_n$ if $a_n$
  os positive and assign $p_n = 0$ if $a_n$ is negative. In a similar manner,
  let $q_n = a_n$ if $a_n$ is negative and $q_n = 0$ if $a_n$ is positive. }

\textit{(a) Argue that if $\sum a_n$ diverges, then at least one of
  $\sum p_n$ or $\sum q_n$ diverges.}

We are going to proceed with a proof by contradiction.

Suppose that both $\sum p_n$ and  $\sum q_n$ converge. Let $s_n$ be
a parital sum for $\sum a_n$, $s1_n$ a parial sum for $p_n$ and $s2_n$ a
partial sum for $q_n$. Then
$$s1_n + s2_n = p_1 + p_2 + ... + p_n + q_1 + q_2 + ... + q_n =
a_1 + a_2 + a_3 + ... + a_n = s_n$$

Thus $\sum a_n$ converges, which is a contradiction. Therefore both
sums $\sum q_n$ and $\sum p_n$ cannot be convergent. Therefore at least one of
$\sum q_n$ or $\sum p_n$ must diverge.

\textit{(b) Show that if $\sum a_n$ converges conditionaly, then both
  $\sum p_n$ and $\sum q_n$ diverge.}

We are going to proceed with a proof by contradiction as well. Suppose that
$p_n$ converges (we are only looking at the case with $p_n$ here, but the
same logic can be applied for $q_n$ as well).

Then, we can state that $\sum -p_n$ converges as well. Thus $\sum a_n - p_n$
converges. Therefore $\sum q_n$ also converges. Therefore $\sum p_n - q_n$
converges as well. Therefore $\sum a_n$ converges absolutely, which is a
contradiction.

Therefore we can state, that in orger to $\sum a_n$ to converge absolutely,
both $\sum p_n$ and $\sum q_n$ must diverge, as desired.

\section*{2.7.4}
\textit{Give an example to show that it is possible for both $\sum x_n$ and
  $\sum y_n$ diverge, but for $\sum x_n y_n$ converge.}

$$x_n = y_n = 1/n \to x_n y_n = 1/n^2$$

\section*{2.7.5}
\textit{(a) Show that if $\sum a_n$ converges absolutely, then $\sum a_n^2$
  also converges absolutely. Does this proposition hold without absolute
  convergence?}

Let $s_n$ be a partial sum for $\sum |a_n|$. Then, because $\sum |a_n|$
converges, there exists $M > 0$ such that $s_n \leq M$. Therefore
each $|a_n| \leq M$. Therefore
$$s_n^2 = |a_1^2| + |a_2^2| + ... + |a_n^2| \leq M|a_1| + M|a_2| + ... +
M|a_n| = M(|a_1| + |a_2| + ... + |a_n|) = Ms_n$$

Threefore, because of the convergence $\sum |a_n|$ we can state, that
$M\sum |a_n| = \sum M |a_n|$ converges as well. Thus, $\sum |a_n|^2$ also
converges. Thus $\sum a_n^2$ converges absolutely, as desired.

If we substitute $a_n$ with some conditionally convergent series, then
this proof collapses on the moment, when we state that $|a_n| \leq M$. That
doesn't neccesarily disprove the proposition, only shows where does
absolute convergence plays the role  in our proof. In order to disprove
this proposition we need an example of a convergent sequence, that contradicts
our proposition. One such sequence is
$$\sum a_n = \sum \frac{(-1)^{n + 1}}{\sqrt{n}}$$
$$\sum a_n^2 = \sum \frac{1}{n}$$
where the first sum is convergent by Alternating Series Test, and the last
is divergent, because is is a harmonic series.

\textit{(b) If $\sum a_n$ converges and $a_n \geq 0$, can we conclude anything
  about $\sum \sqrt{a_n}$?}

Not really.

$a_n = \frac{1}{n^2}$ implies that  $\sum a_n $ conveges, but $\sum \sqrt{a_n}$
is a Harmonic Series. For $a_n = \frac{1}{n^2}$ both $\sum a_n$ and
$\sum \sqrt{a_n}$ converge (both conclusions are justified by the fact, that
we had drawn  some time ago that the series
$\sum 1/n^p$ converges if and only if $p > 1$ ).

\section*{2.7.6}
\textit{(a) Show that if $\sum x_n$ converges absolutely, and the sequence
$(y_n)$ is bounded. then the sum $\sum x_n y_n$ converges.}


Let $s_n$ be a partial sum for $\sum |x_n|$ and $l_n$ be a partial sum
for $\sum |y_n x_n|$. Then, because $(y_n)$
is bounded, there exists $M > 0$ such that $|y_n| \leq M$.  Therefore
$$l_n = |x_1 y_1| + |x_2 y_2| + ... + |x_n y_n| \leq M|x_1| + M|x_2| + ... +
M|x_n| = M(|x_1| + |x_2| + ... + |x_n|) = Ms_n$$
Therefore $\sum x_n$  being absolutely  convergent implies that
$\sum M x_n$ is also absolutely convergent, which implies that $\sum x_n y_n$
is also absolutely convergent.

\textit{(b) Find a counterexample that demonstrates that part (a) does not
  always hold if the convergence of $\sum x_n$ is conditional}

Same as in exercise 2.7.5
$$\sum x_n = \sum \frac{(-1)^{n + 1}}{\sqrt{n}}$$
$$(y_n) = \frac{(-1)^{n + 1}}{\sqrt{n}}$$
$$\sum y_n x_n = \sum \frac{1}{n}$$

\section*{2.7.7}
\textit{Now that we have proved the basic facts about geometric series,
  supply a proof for Corollary 2.4.7}

Corollary 2.4.7 states that the series
$$\sum^{\infty}_{n = 1} 1/n^p$$
converges if and only if $p > 1$

% Partial sums for this sequences are defined as
% $$(s_n) = \frac{1}{1^p} + \frac{1}{2^p} + \frac{1}{3^p} + ... + \frac{1}{n^p}$$

% If $p < 1$ then $1/n^p \geq 1/n$ and therefore the series diverges.

Let us employ Cauchy Condensation Test here.
Let us first play with the term of the sequence for it.
$$2^n a_{2^n} = 2^n \frac{1}{(2^n)^p} = \frac{2^n}{2^{np}} =
2^{n - np} = 2^{n(1 - p)} = (2^{1 - p})^n$$
We know, that geometric series converges if and only if
for its term $ar^k$,  $|r| < 1$. Thus, we
can state, that by Cause Condensation Test, the original series converges
if and only if 
$$|2^{1 - p}| < 1$$
$$2^{1 - p} < 1$$
$$1 - p < \log_2 (1)$$
$$1 - p < 0$$
$$p > 1$$
as desired.

\section*{2.7.8}
\textit{Prove Theorem 2.7.1 part (ii)}

Theorem 2.7.1 part (ii) states that

\textit{If $\sum_{k = 1}^{\infty} a_k = A$ and $\sum_{k = 1}^{\infty} b_k = B$
then $\sum_{k = 1}^{\infty} (a_k + b_k) = A + B$}

Let $s_n$ be a partial sum for the series $\sum_{k = 1}^{\infty} a_k = A$,
$t_n$ be a partial sum for $\sum_{k = 1}^{\infty} b_k = B$ and
$p_n$ be a parial sum for $\sum_{k = 1}^{\infty} (a_k + b_k)$. Then
$$s_n + t_n = a_1 + a_2 + ... + a_n + b_1 + b_2 + ... + b_n =
a_1 + b_1 + a_2 + b_2 + ... + a_n + b_n = p_n $$
for every $n \in N$. 

Thus we can state that $\lim (a_n + b_n) = A + B$ by Algebraic Limit Theorem.
Thus 
$$\lim (a_n + b_n) = \lim p_n = \sum (a_n + b_n) = A + B$$
as desired.

\section*{2.7.9 (Ratio Test)}
\textit{Given a series $\sum_{n = 1}^{\infty} a_n$ with $a_n \neq 0$, the
  Ratio Test states that if $(a_n)$ satisfies}
$$\lim|\frac{a_{n + 1}}{a_n}| = r < 1$$
\textit{then the series converges absolutely.}

\textit{(a) Let $r'$ satisfy $r < r' < 1$. (Why must such an $r'$ exist?)
  Explain why there exists an $N$ such that $n \geq N$ implies
  $|a_{n + 1}| \leq |a_n|r'$}

First of all,
$$|\frac{a_{n + 1}}{a_n}| = |a_{n + 1} \frac{1}{a_n}| =  |a_{n + 1}| |\frac{1}{a_n}|$$.

If  $a_n \geq 0$, then $|\frac{1}{a_n}| = \frac{1}{a_n} = \frac{1}{|a_n|}$.
If $a_n < 0$ then $|\frac{1}{a_n}| = - \frac{1}{a_n} = \frac{1}{-a_n} =
\frac{1}{|a_n|}$. Therefore $|\frac{1}{a_n}| = \frac{1}{|a_n|}$. Thus 
$$|\frac{a_{n + 1}}{a_n}| = |a_{n + 1}| |\frac{1}{a_n}| =
\frac{|a_{n + 1}|}{|a_n|}$$

Thus
$$\lim|\frac{a_{n + 1}}{a_n}| = \lim\frac{|a_{n + 1}|}{|a_n|} = r < 1$$

$r'$ exists, beacause for any $\epsilon > 0$ there exists $n \in N$ such that
$1/n < \epsilon$. Therefore if we set $\epsilon = 1 - r$, then there exists
$1/n_1 < \epsilon$. Therefore
$$0 < 1/n_1 < \epsilon$$
$$0 < 1/n_1 < 1 - r$$
$$r < r + 1/n_1 < 1$$

Suppose that we set $\epsilon = r' - r$. Then, by definition of limit, there
exists $N \in \textbf{N}$, such that for any $n \geq N$ it follows that 
$$|\frac{|a_{n + 1}|}{|a_n|} - r| < \epsilon$$
$$|\frac{|a_{n + 1}|}{|a_n|} - r| < r' - r$$
$$- (r' - r) < \frac{|a_{n + 1}|}{|a_n|} - r < r' - r$$
$$- r' +  r < \frac{|a_{n + 1}|}{|a_n|} - r < r' - r$$
$$- r' + 2r < \frac{|a_{n + 1}|}{|a_n|}  < r'$$
$$\frac{|a_{n + 1}|}{|a_n|}  < r'$$
$$|a_{n + 1}|  < r'|a_n|$$
as desired.

\textit{(b) Why does $|a_N|\sum(r')^n$ necessarily converge? }

Because we have $r$ is a limit of non-negative terms, we can conclude,
that $r \geq 0$. Thus, $0 \leq r'  < 1$. Thus, $|r'| < 1$. Therefore,
$\sum(r')^n$ is a geometric series with $|r| < 1$. Therefore it converges.
Therefore $|a_N| \sum(r')^n$ is convergent. Also, $\sum |a_N| (r')^n$ is
convergent as well.

\textit{(c) Now, show that $\sum |a_n|$ converges.}


Let $s_n$ be a partial sum for $\sum a_n$ and let $q_n$ be a partial sum
for $\sum |a_N| r'^n$.

Let $\epsilon$ be arbitrary. Then, there
exists $N_1 \in \textbf{N}$ such that
$n \geq N_1 \to |a_{n + 1}| \leq |a_n|r'$.
$\sum a_n$. There also exists $N_2 \in \textbf{N}$ such that $m > n \geq N_2$
implies than $|q_m - q_n| < \epsilon$ (this comes from Cauchy Criterion for
Series). Then let  $N = \max\{N_1, N_2\}$.

Then for $m > n \geq N$.
$$|s_m - s_n| = ||a_{n + 1}| + |a_{n + 2}| + ... |a_m|| =
|a_{n + 1}| + |a_{n + 2}| + ... |a_m| \leq$$
$$\leq|a_n|r' + |a_{n + 1}|r' +
... + |a_{m - 1}|r' \leq |a_{n - 1}|r'^2 + |a_{n}|r'^2 + ... +
|a_{m - 2}|r'^2 \leq$$
$$\leq |a_N|r'^{N - n} + |a_N|r'^{N - n + 1} + ... + |a_N|r'^{N - m} =
|q_m - q_n| < \epsilon$$

Therefore there exists $N \in \textbf{N}$ such that for every $m > n \geq N$
$$|s_m - s_n| < \epsilon$$

Thus, by Cauchy criterion for series, the  series $\sum |a_n|$ is convergent.
Thus $\sum a_n$ is absolutely convergent.

\section*{2.7.10}
\textit{(a) Show that if $a_n > 0$ and $\lim(na_n) = l$ with $l \neq 0$,
  then the series $\sum a_n$ diverges. }

I think that for this one the strategy is to somehow compare element
of a given  sequence, and then conclude, that the partial sum of a desired
series is more than a partial sum for harmonic series, and therefore is
divergent.

First of all, $n \in N \to n \geq 0$, therfore $a_n n \geq 0$. Therefore
we can state that $l \geq 0$.

Let $(s_n)$ be a sequence of  partial sums  for $\sum a_n$. Then
let $\epsilon = l$. Then there exists $N \in \textbf{N}$ such that for all
$n \in \textbf{N} \geq N$ it follows that
$$ |n a_n - l| > \epsilon$$
$$ n a_n - l > \epsilon$$
$$ n a_n - l > l $$
$$ n a_n  > 2l $$
$$ a_n  > \frac{2l}{n} $$

Let us now look at the Harmonic Series. Define $q_n$ to be a partial sum for
the Harmonic Series. We know, that it is divergent. Thus,
there exists an $\epsilon$, such that
$$|q_m - q_n| \geq \epsilon$$
for any $m > n \in \textbf{N}$. Thus
$$|\frac{1}{m} + \frac{1}{m - 1} + ... + \frac{1}{n + 1} | \geq \epsilon$$
$$\frac{1}{m} + \frac{1}{m - 1} + ... + \frac{1}{n + 1} \geq \epsilon$$
$$\frac{2l}{m} + \frac{2l}{m - 1} + ... + \frac{2l}{n + 1} \geq 2l \epsilon$$
$$a_m + a_{m - 1} + ... + a_{n + 1} \geq 2l \epsilon$$
$$|s_m - s_n| \geq 2l \epsilon$$

Thus, there exists $2l\epsilon \in R$ such that for every
$m > n \in \textbf{N}$ it follows that 
$$|s_m - s_n| \geq 2l \epsilon$$

Thus, by Cauchy Criterion for series, the series $\sum a_n$ is divergent.

\textit{(b) Assume $a_n > 0$ and $\lim(n^2 a_n)$ exists. Show that
  $\sum a_n$ converges.}

Suppose that all assumption in the exercise are true. Then, by defintion
of limit, for every $\epsilon > 0$
there exists $N \in \textbf{N}$ such that if $n \geq N$, then it
follows that
$$|n^2 a_n - l| < \epsilon$$
$$- \epsilon < n^2 a_n - l < \epsilon$$
$$- \epsilon + l < n^2 a_n  < l + \epsilon$$
$$\frac{l - \epsilon}{n^2} < a_n  < \frac{l + \epsilon}{n^2}$$
$$0< a_n  < \frac{l + \epsilon}{n^2}$$

We know, that the sum $\sum 1/n^2$ is convergent. Therefore
$\sum \frac{ l + \epsilon}{n^2}$ is convegent as well for any
$l + \epsilon \in R$. Thus, by Comparison Test, $\sum a_n$ is convergent as
well, as desired.

\section*{2.7.11}
\textit{Find examples of two series $\sum a_n$ and $\sum b_n$ both of which
  diverge, but for which $\sum \min\{a_n, b_n\}$ converges. To make it more
  challanging, produce exampless where $(a_n)$ and $(b_n)$ are positive and
  decreasing.}

If the relax the exercise and try to make just normal versions of given things, then we'll get

\begin{equation}
  a_x =
  \begin{cases}
    1 \text{ if n is even} \\
    1/n^2 \text{ if n is odd}
  \end{cases}
\end{equation}

and

\begin{equation}
  b_x =
  \begin{cases}
    1 \text{ if n is odd} \\
    1/n^2 \text{ if n is even}
  \end{cases}
\end{equation}

Then the obvious minimum will converge, as desired. In this case we even have
every element of every sequence to be positive.

The idea behind this one is probably to construct the desired case from
"barely" divergent sequences and series (In this case we probably need
something along the lines of Harmonic Series )

It (probably) can be shown, that the desired sequences are convergent to zero
(or at the very least one of them).

On this particular exercise we are going to assume some things without any
hope for proof or concrete base, and we are going to pull'em out of thin air.
First of all: the required seiries are most likely to be "barely" divergent.
Secondly, required sequences (not series) are probably convergent to 0. It's
actually hard to belive, that such sequences exist.

I looked this exercise up on the internet for some hints, and I got some.
Suppose that:

$a_n$ - equal to $1/n^2$ for "odd" turns (i.e. for [1], [3, 4], [7, 8, 9], ...),
and maximum of itself for other terms
$b_n$ - equal to $1/n^2$ for "even" turns (i.e. for [2], [5, 6], [10, 11, 12], ...), and maximum of itself for other terms

Then I think, that both of them diverge, but cannot prove it. There exists a
more robust example, but I think, that it is pretty cursed anyways. I'm done
with this exercise anyways, not gonna spend more time on it.


\section*{2.7.12 (Summation by Parts)}
\textit{Let $(x_n)$ and $(y_n)$ be sequences, and let
  $s_n = x_1 + x_2 + ... + x_n$. Use the observation that
  $x_j = s_j - s_{j - 1}$ to verify formula }
$$\sum_{j = m + 1}^{n} x_j y_j = s_n y_{n + 1} - s_m y_{m + 1} +
\sum_{j = m + 1}^{n} s_j (y_j - y_{j + 1})$$
$$s_n y_{n + 1} - s_m y_{m + 1} +
\sum_{j = m + 1}^{n} s_j (y_j - y_{j + 1}) = $$
$$ = s_n y_{n + 1} - s_m y_{m + 1} +
s_{m + 1} (y_{m + 1} - y_{m + 2}) +
s_{m + 2} (y_{m + 2} - y_{m + 3}) +
... +
s_{n} (y_{n} - y_{n + 1}) = $$
$$ = s_n y_{n + 1} - s_m y_{m + 1} +
s_{m + 1} y_{m + 1} - s_{m + 1} y_{m + 2} +
s_{m + 2} y_{m + 2} - s_{m + 2} y_{m + 3} +
... +
s_{n} y_{n} - s_{n} y_{n + 1} = $$
$$ =  - s_m y_{m + 1} +
s_{m + 1} y_{m + 1} - s_{m + 1} y_{m + 2} +
s_{m + 2} y_{m + 2} - s_{m + 2} y_{m + 3} +
... + s_{n - 1} y_{n - 1} - s_{n - 1} y_{n}
+ s_{n} y_{n}  = $$
$$ =  y_{m + 1}(s_{m + 1} - s_m ) +
y_{m + 2}(s_{m + 2} - s_{m + 1} ) +
...   y_{n}( s_{n} - s_{n - 1})  = $$
$$ = y_{m + 1} x_{m + 1} +
y_{m + 2} x_{m + 2} + ... +
y_{n}x_n = \sum_{j = m + 1}^{n} x_j y_j
$$
as desired.

\section*{2.7.13 (Dirichlet's Test)}
\textit{Dirichlet's Test for convergence states that if the partial sums of
  $\sum_{n = 1}^{\infty} x_n$ are bounded (but not necessarily convergent), and
  if $(y_n)$ is a sequence satisfyong $y_1 \geq y_2 \geq y_3 \geq ... \geq 0 $
  with $\lim y_n = 0$, then the series $\sum_{n=1}^{\infty} x_n y_n$ converges.
}

\textit{(a) Let $M > 0$ be an upper bound for the partial sums of
  $\sum_{n = 1}^{\infty} x_n$. Use Exercise 2.7.12 to show that }
$$\left|\sum_{j = m + 1}^{n} x_j y_j\right| \leq 2M|y_{m + 1}|$$

Suppose that everything in exercise is true. Then it follows that
$|s_j| < M$. Then
$$\left|\sum_{j = m + 1}^{n} x_j y_j\right| =
\left|s_n y_{n + 1} - s_m y_{m + 1} + \sum_{j = m + 1}^{n}{s_j (y_j - y_{j + 1})} \right| \leq$$
$$\leq \left|s_n y_{n + 1}\right| + \left|s_m y_{m + 1}\right| + \left|\sum_{j = m + 1}^{n}{s_j (y_j - y_{j + 1})} \right| = $$
$$\leq |s_n| |y_{n + 1}| + |s_m|| y_{m + 1}| + |M|\left|\sum_{j = m + 1}^{n}{ (y_j - y_{j + 1})} \right| \leq $$
$$\leq M |y_{n + 1}| + M| y_{m + 1}| + M\left|\sum_{j = m + 1}^{n}{ (y_j - y_{j + 1})} \right| =$$
$$=  M \left(|y_{n + 1}| + | y_{m + 1}| + \left|\sum_{j = m + 1}^{n}{ (y_j - y_{j + 1})} \right|\right) \leq
$$
$$\leq  M \left(|y_{n + 1}| + | y_{m + 1}| + \left| y_{m + 1} - y_{m + 2} + y_{m + 2} - y_{m + 3} +  ... + y_n - y_{n + 1}\right|\right) =$$
$$=  M \left(|y_{n + 1}| + | y_{m + 1}| + \left| y_{m + 1} - y_{n + 1}\right|\right) = $$
$$=  M \left(y_{n + 1} +  y_{m + 1} + y_{m + 1} - y_{n + 1}\right) = 2M|y_{m + 1}|$$

Therefore
$$\left|\sum_{j = m + 1}^{n} x_j y_j\right| \leq 2M|y_{m + 1}|$$
as desired

\textit{(b) Prove Dirichlet's Test just stated.}

Suppose that we are given some $\epsilon$ > 0. By the convergence of $(y_n)$,
there exists $N$, such that for  $\epsilon_1 = \epsilon / 2M$ and $m + 1 \geq N$ it
follows that
$$|y_{m + 1}| < \epsilon_1$$
$$|y_{m + 1}| < \epsilon / 2M$$
$$2M|y_{m + 1}| < \epsilon$$
$$\left|\sum_{j = m + 1}^{n} x_j y_j\right| \leq 2M|y_{m + 1}| < \epsilon$$
$$\left|\sum_{j = m + 1}^{n} x_j y_j\right|  < \epsilon$$
Thus, by Cauchy Criterion for Series, it follows that
$\sum_{n = 1}^{\infty} x_j y_j$ converges, as desired.

\textit{(c) Show how the Alternating Series Test (Theorem 2.7.7) can be
  derived as a special case of Dirichlet's Test.}

Firstly, 2.7.7 states that

\textbf{Theorem 2.7.7 (Alternating Series Test)}
\textit{Let $(a_n)$ be a sequence satisfying}
$$\text{(i) } a_1 \geq a_2 \geq a_3 \geq ... \geq a_n \geq ... \text{ and }$$
$$\text{(ii) } (a_n) \to 0$$
\textit{Then, the alternating series $\sum_{n = 1}^{\infty}(-1)^{n + 1} a_n$
  converges}

We cas set $x_n = (-1)^{n + 1}$, which is gonna be bounded by $[-1, 1]$, and
$y_n = a_n$, which converges to 0 and satisfies all of the necessary
prerequisites for Dirichlet's Test to be applied.

\section*{2.7.14 (Abel's Test).}
\textit{Abel's Test for convergence states that if the series
  $\sum_{n = 1}^{\infty} x_n$ converges, and if $y_n$ is a sequence
  satisfying  }
$$y_1 \geq y_2 \geq y_3 \geq ... \geq 0$$
then the series $\sum_{n = 1}^{\infty} x_n y_n$ converges.

\textit{(a) Carefully point out how the hypothesis of Abel's Test differs from
  that of Dirichlet's Test in Exercise 2.7.13.}

First of all, Abel's Test requires $\sum x_n$ to be not only bounded, but
convergent as well. Secondly, $(y_n)$ is not required to be convergent.

\textit{(b) Assume that $\sum a_n$ has partials sums, that are bounded by a
  constant $A > 0$, and assume $b_1 \geq b_2 \geq b_3 \geq ... \geq 0$. Use
  Exercise 2.7.12 to show that }
$$\left|\sum_{j = 1}^{n} a_j b_j \right| \leq 2 A b_1$$
let $s_n$ be a sequence of partial sums for $\sum a_n$. Then
$$\left|\sum_{j = 1}^{n} a_j b_j \right| = \left|s_n b_{n + 1} - s_1 b_2 + \sum_{j = 1}^{n} s_j (b_j - b_{j + 1})\right| \leq $$
$$\leq |s_n b_{n + 1}| + |s_0 b_1| + \left|\sum_{j = 1}^{n} s_j (b_j - b_{j + 1})\right| $$
$$\leq |s_n b_{n + 1}| + |s_0 b_1| + \sum_{j = 1}^{n} \left|s_j (b_j - b_{j + 1})\right| $$
$$\leq |s_n b_{n + 1}| + |s_0 b_1| + \sum_{j = 1}^{n} \left|s_j (b_j - b_{j + 1})\right| $$
$$\leq |s_n ||b_{n + 1}| + |s_0 ||b_1| + \sum_{j = 1}^{n} \left|s_j|| (b_j - b_{j + 1})\right| $$
$$\leq A|b_{n + 1}| + A|b_1| + \sum_{j = 1}^{n} A \left| (b_j - b_{j + 1})\right| $$
$$\leq A(|b_{n + 1}| + |b_1| + \sum_{j = 1}^{n}  \left| (b_j - b_{j + 1})\right| )$$
$$\leq A(b_{n + 1} + b_1 + \sum_{j = 1}^{n}   (b_j - b_{j + 1}) )$$
$$\leq A(b_{n + 1} + b_1 + b_1 - b_2 + b_2 - b_3 + ... + b_n - b_{n + 1}) $$
$$\leq A(b_1 + b_1) = 2 A b_1 $$
as desired.

\textit{(c) Prove Abel's Test via the following strategy. For a fixed
  $m \in N$, apply part (b) to $\sum_{j = m + 1}^{n} x_j y_j $ by setting
  $a_n = x_{m + n}$ and $b_n = y_{m + n}$. (Argue that an upper bound on the
  partial sums of $\sum_{n = 1}^{\infty} a_n$ can be made arbitrarily small by
  taking $m$ to be large.)}

Let us fix $m \in N$. Then set $a_n = x_{m + n}$ and $b_n = y_{m + n}$. Then
$\sum_{j = 1}^{n} a_j$ is equal to $\sum_{m + 1}^{m + n} x_n$. Thus, for every
$\epsilon > 0$ we can find $m \in N$, such that
$\sum_{j = 1}^{n}a_j < \epsilon$ by convergence of $\sum x_j$ and by using
Cauchy Criterion. Therefore we always have $A = \epsilon$. Thus
$$|\sum_{j = m + 1}^{n} x_j y_j|= |\sum_{j = 1}^{n} a_n b_n| <2 A b_1$$

Therefore by Cauchy Criterion for Series, $\sum x_j y_j$ is convergent.

\section*{2.8.1}
\textit{Using the particular array $(a_{ij})$ from  Section 2.1, compute
  $\lim_{n \to \infty} s_{nn}$. How does this value compare to the two
  iterated values for the sum already computed? }

Firstly, let us state the requested array:  $\{a_{ij}: i,j \in \textbf{N}\}$,
where $a_{ij} = 1/2^{j - i}$ if $j > i$, $a_{ij} = -1$ if $j = i$, and
$a_{ij} = 0$ if $j < i$ (I'd gladly portrait the grid here if I knew how to
do it).

I don't think, that we need concrete proofs and/or some other rigorous stuff,
so it'll suffice to show that
$$s_{1,1} = -1$$
$$s_{2,2} = -3/2$$
$$s_{3,3} = -7/4$$

Or in general, $(s_{n,n}) \to  -2$, or in other words,
$$\lim_{n \to \infty} s_{n,n} = -2$$
as desired.

The resulting value is equal to one of the earlier computed results.

\section*{2.8.2}

\textit{Show that if the iterated series }
$$\sum_{i = 1}^{\infty}\sum_{j = 1}^{\infty}|a_{ij}|$$
\textit{converges (meaning that for each fixed $i \in \textbf{N}$ the series
  $\sum_{j = 1}^{\infty}|a_{ij}|$ converges to some real number $b_i$, and
  the series $\sum_{i = 1}^{\infty}b_i$ converges as well), then the iterated
  series}
$$\sum_{i = 1}^{\infty}\sum_{j = 1}^{\infty}a_{ij}$$
\textit{converges.}

Suppose that
$$\sum_{i = 1}^{\infty}\sum_{j = 1}^{\infty}|a_{ij}|$$
converges. This implies that for every $i \in N$ it is true that
$\sum_{j = 1}^{\infty}|a_{ij}|$  converges. This implies that
$\sum_{j = 1}^{\infty}a_{ij}$ converges. It is also true, tha
$\sum_{i = 1}^{\infty}b_{i}$ converges. Triangular inequality implies that
$$ b_i =  \sum_{j = 1}^{\infty}|a_{ij}| \geq \left|\sum_{j = 1}^{\infty}a_{ij}\right|$$
$$  0 \leq \left|\sum_{j = 1}^{\infty}a_{ij}\right| \leq b_i$$
Thus, by Comparison Test
$$\sum_{i = 1}^{\infty}\left|\sum_{j = 1}^{\infty}a_{ij}\right|$$
converges. Thus, by Absolute Convergence Test
$$\sum_{i = 1}^{\infty}\sum_{j = 1}^{\infty}a_{ij}$$
converges too, as desired.



\section*{2.8.3}

\textbf{Theprem 2.8.1.}
\textit{Let $\{a_{ij}: i, j \in N\}$ be a doubly indexed array of real numbers.
If}
$$\sum_{i = 1}^{\infty}\sum_{j = 1}^{\infty}|a_{ij}|$$
\textit{converges, then both
  $\sum_{i = 1}^{\infty}\sum_{j = 1}^{\infty} a_{ij}$ and
  $\sum_{j = 1}^{\infty}\sum_{i = 1}^{\infty} a_{ij}$ converge to the same value. Moreover, }
$$\lim_{n \to \infty} s_{nn} = \sum_{i = 1}^{\infty}\sum_{j = 1}^{\infty}a_{ij} = \sum_{j = 1}^{\infty}\sum_{i = 1}^{\infty}a_{ij}$$
\textit{where $s_{nn} = \sum_{i = 1}^{n}\sum_{j = 1}^{n}a_{ij}$}

\textit{(a) Prove that the set $\{t_{mn}: m, n \in \textbf{N}\}$ is bounded
  above, and use this fact to conclude that the sequence $(t_{nn})$ converges.}

Because
$$\sum_{i = 1}^{\infty}\sum_{j = 1}^{\infty}|a_{ij}| = l$$
is convergent, we can state that $t_{mn}$ is less than $l$. Also,
$t_{m, m} - t_{n,n} \geq 0$ for $m \geq n$. Thus, the sequence is bounded
and monotone, and therefore convergent.

\textit{(b) Now, use the fact that $(t_{nn})$ is a Cauchy sequence to argue
  that $(s_{nn})$ is a Cauchy sequence and hence converges.}

Let $\epsilon > 0$. Then there exists $N \in \textbf{N}$ such  that for
$m > n \in N$ it follows that
$$|t_{mm} - t_{nn}| < \epsilon$$
$$||a_{m,m}| + |a_{m, m - 1}| + ... + |a_{n + 1, n + 1}|| < \epsilon$$
$$|a_{m,m}| + |a_{m, m - 1}| + ... + |a_{n + 1, n + 1}| < \epsilon$$
$$|a_{m,m} + a_{m, m - 1} + ... + a_{n + 1, n + 1}| \leq |a_{m,m}| + |a_{m, m - 1}| + ... + |a_{n + 1, n + 1}| < \epsilon$$
$$|a_{m,m} + a_{m, m - 1} + ... + a_{n + 1, n + 1}|  < \epsilon$$
$$|s_{mm} - s_{nn}|  < \epsilon$$

Therefore $(s_{nn})$ is a Cauchy sequence, and therefore it is convergent, as
desired.

\section*{2.8.4}
\textit{(a) Argue that there exists an $N_1 \in \textbf{N}$ such that
  $m, n \geq N_1$ implies $B - \frac{\epsilon}{2} < t_{mn} \leq B$.}

There exist $m_0, n_0 \in N$ such that
$$B - \frac{\epsilon}{2} < t_{m_0, n_0} \leq B$$

We know, that $t_{m_1, n_0} - t_{m_0, n_0} \geq 0$ for $m_1 \geq m_0$. Thus,
if we pick $N_1 = \max\{m_0, n_0\}$, then $m, n \geq N_1$ implies that 
$$B - \frac{\epsilon}{2} < t_{m, n} \leq B$$
as desired

\textit{(b) Now, show that there exists an $N$ such that }
$$|s_{mn} - S| < \epsilon$$
\textit{for all $m, n \geq N$.}

We know, that $(t_{n,n})$ is a Cauchy sequence, and thus, for the given
$\epsilon$ there exists $N$, such that $m > n \geq N$ implies that 
$$|t_{m,m} - t_{n,n}| < \epsilon/2$$
$$||a_{m,m}| + |a_{m - 1, m}| + ... + |a_{n + 1,n + 1}|| < \epsilon/2$$
$$|a_{m,m}| + |a_{m - 1, m}| + ... + |a_{n + 1,n + 1}| < \epsilon/2$$
$$|a_{m,m}| + |a_{m - 1, m}| + ... + |a_{n + 1,m}| < \epsilon/2$$
$$|a_{m,m} + a_{m - 1, m} + ... + a_{n + 1,m}| < \epsilon/2$$
$$|a_{m,m} + a_{m - 1, m} + ... + a_{n + 1,m}| < \epsilon/2$$

$$|s_{nn} - S| < \epsilon/2$$
$$|s_{nn} - S| + |a_{m,m} + a_{m - 1, m} + ... + a_{n + 1,m}| < \epsilon$$
$$|s_{nn} + a_{m, n} + .... + a_{n + 1, n} - S| \leq |s_{nn} - S| + |a_{m,m} + a_{m - 1, m} + ... + a_{n + 1,m}| <  \epsilon$$
$$|s_{nn} + a_{m, n} + .... + a_{n + 1, n} - S| < \epsilon$$
$$|s_{mn}  - S| < \epsilon$$

Thus, there exists $N \in \textbf{N}$ such that $m, n \geq N$ implies
$$|s_{mn} - S| < \epsilon$$
as desired

\section*{2.8.5}
\textit{Use the Algebraic Limit Theorem (Theorem 2.3.3) and the Order Limit
  (Theorem 2.3.4) to show that for all $m \geq N$}
$$|(r_1 + r_2 + ... + r_m) - S| \leq  \epsilon$$
\textit{Conclude that the iterated sum
  $\sum_{i=1}^{\infty}\sum_{j=1}^{\infty} a_{ij}$ converges to S.}

We know, that for every $\epsilon > 0$  there exists $N \in \textbf{N}$ such
that for $m,n \geq N$ it follows that
$$|s_{mn} - S| < \epsilon$$

We also should state, that because the sum of absolute values converge
to some number, then it follows, that $\sum r_m$ also converges to some number.
Therefore we can use both Order Limit Theorem and Algebraic Limit Theorem.

$$\left|\left(\sum_{j = 1}^{\infty}a_{1,j} + \sum_{j = 1}^{\infty}a_{2,j} + ... + \sum_{j = 1}^{\infty}a_{m,j}\right) - S\right| \leq  \epsilon$$

% Going bottom-up
% $$(p_m) \to S$$
% $$\forall \epsilon : \exists N \in \textbf{N}: \forall m \geq N \to  |p_m - S| < \epsilon$$
% $$p_n = \sum a_{1,n} + \sum a_{2,n} + ... + \sum a_{m,n}$$

% Maybe the  idea is to show that every element is bound below by some $s_{mn}$ and bound above by $S$.

% $$\sum r_n - \sum s_{mn} = $$
% $$ = (\sum_{j=1}^{\infty} a_{1,j} + \sum_{j=1}^{\infty} a_{2,j} + \sum_{j=1}^{\infty} a_{m,j})
% - (\sum_{j=1}^{n} a_{1,j} + \sum_{j=1}^{n} a_{2,j} + ... + \sum_{j=1}^{n} a_{m,j})$$
% $$\left|\sum r_n - \sum s_{mn}\right| = $$
% $$ = \left|(\sum_{j=1}^{\infty} a_{1,j} + \sum_{j=1}^{\infty} a_{2,j} + \sum_{j=1}^{\infty} a_{m,j})
%   - (\sum_{j=1}^{n} a_{1,j} + \sum_{j=1}^{n} a_{2,j} + ... + \sum_{j=1}^{n} a_{m,j})\right|$$
$$\left|\sum r_n - \sum s_{mn}\right| = $$
$$ = \left|(\sum_{j=1}^{\infty} a_{1,j} + \sum_{j=1}^{\infty} a_{2,j} + \sum_{j=1}^{\infty} a_{m,j})
  - (\sum_{j=1}^{n} a_{1,j} + \sum_{j=1}^{n} a_{2,j} + ... + \sum_{j=1}^{n} a_{m,j})\right| = $$
$$\left|\sum_{j = n + 1}^{\infty} a_{1,j} + \sum_{j = n + 1}^{\infty} a_{2,j} + ...
  + \sum_{j = n + 1}^{\infty} a_{m,j} \right| \leq $$
$$
\leq \sum_{j = n + 1}^{\infty} |a_{1,j}| + \sum_{j = n + 1}^{\infty} |a_{2,j}| + ...
+ \sum_{j = n + 1}^{\infty} |a_{m,j}|  \to 0
$$


By convergence of $\sum \sum |a_{m,n}|$ we know, that every
$\sum_{j = 1}^{\infty} |a_{m,n}|$
converges to some number. Suppose that it converges to $r_m$. 
Thus we can state that
$$\sum_{j = n + 1}^{\infty} = \lim(\sum_{j = 1}^{\infty} |a_{m,j}| - \sum_{j = 1}^{n} |a_{m,j}|) = \lim(r_m - r_m) = 0$$

Thus, $q_n = \left|\sum r_n - \sum s_{mn}\right|$ is a sequence,
each element of which is less than a sum of elements of sequences,
convergent to 0, and thus, by Algebraic Limit Theorem, it itself
is also convergent to 0.

Thus,   $\sum_{i=1}^{\infty}\sum_{j=1}^{\infty} a_{ij} = S$, as desired.

\section*{2.8.6}
\textit{Finish the proof by showing that the other iterated sum, $\sum_{j=1}^{\infty}\sum_{i=1}^{\infty} a_{ij}$, converges to S as well. Notice that the
  same argument can be used once it is established that. for each fixed column
  $j$, the sum $\sum_{i = 1}^{\infty} a_{ij}$ converges to some real number
  $c_i$.}

We can make an argument here, that we don't really need two different proofs
on the account that it doesn't matter in which particular direction (rows or
columns) we construct $r$'s.

To make the argument more concrete, substitute $i$ for $j$  in the
previous section of the proof and you'll get the desired result.

Therefore if $\sum_{j = 1}^{\infty}\sum_{i = 1}^{\infty}|a_{ij}|$ exists,
then it follows that 
$$\lim_{n \to \infty} s_{nn} = \sum_{i = 1}^{\infty}\sum_{j = 1}^{\infty}a_{ij} = \sum_{j = 1}^{\infty}\sum_{i = 1}^{\infty}a_{ij}$$

\section*{2.8.7}
\textit{(a) Assuming the hypothesis -- and hence the conclusion of
  Theorem 2.8.1, show that $\sum_{k = 2}^{\infty}d_k$ converges absolutely.}

This follows from the fact that $\sum_{k = 2}^{\infty}|d_k|$ is increasing, and bounded
above by $\sum \sum |a_{ij}|$, and therefore convergent absolutely.

\textit{(b) Imitate the strategy in the proof of Theorem 2.8.1 to show
  $\sum_{k=2}^{\infty} d_k$ converges to $S = \lim_{n \to \infty}s_{nn}$}

Let $n \in \textbf{N}$ and let $m = 2n$. Then it follows, that
every $a_{ij}$, that comprises $s_{nn}$ is in $d_{m}$. Also, every $a_{ij}$
in $d_{m}$ is contained in $s_{mm}$. Thus, let $q_k$ be a sequence, such that
$q_k = d_m - s_{nn}$. We know that $t_{nn}$ (which is sequence of  paritial
sums for the absolute sum) converges. Thus, if we define $l_k$ to be an
absolute sum of evements of $q_k$, then it follows, that 
$$ |t_mm - t_nn| < \epsilon $$
$$ t_mm - t_nn < \epsilon $$
$$ t_mm - t_nn < \epsilon $$
$$  l_k < \epsilon $$
$$|q_k| < \epsilon$$
Thus, $(q_k) \to 0$. Therefore we can state that
$$\lim d_m = \lim q_m + \lim s_{nn}$$
Thus, $\lim d_m = S$, as desired.

\section*{2.8.8}
\textit{Assume that $\sum_{i = 1}^{\infty} a_i$  converges to $A$, and
  $\sum_{i = 1}^{\infty} b_i$ converges absolutely to $B$.}

\textit{(a) Show that the set}
$$\left\{\sum_{i = 1}^{m}\sum_{j = 1}^{n}|a_i b_j|: m,n \in \textbf{N}\right\}$$
\textit{is bounded. Use this to show that the iterated sum
  $\sum_{i = 1}^{\infty}\sum_{j = 1}^{\infty}|a_i b_j|$ converges so that we
  may apply Theorem 2.8.1}


First of all, because sum of both sequences converge, we can state that
$(a_n) \to 0$ and $(b_n) \to 0$. Thus, both of them are bounded. Thus
let us take a look at a partial sum of $\sum_{i = 1}^{m}\sum_{j = 1}^{n}|a_i b_j|$

$$s_n = |a_1 b_1| + |a_2 b_2| + ... + |a_n b_n| =
|a_1||b_1| + |a_2 || b_2| + ... + |a_n ||b_n| \leq$$
$$\leq |M||b_1| + |M||b_2| + ... + |M||b_n| = |M|(b_1 + b_2 + ... + b_n)$$
where $M$ is a number such that $|a_n| < M$.

Thus we can state that every element of a sequence of partial sums for the
original sequence is less or equal to an element of a sequence of partial sums
for $\sum |Mb_n|$. Thus, the set is bounded above by |MB| (by Algebraic Limit
Theorem for Series). Because the sum it is a sum of non-negative terms we
can conclude, that is is also bounded below by 0 (to be more rigorous, every
element in sequence of partial sums is greater, than an element of a sequence
of 0's, and therefore is non-negative).

By the fact that this sum is a sum of non-negative terms, we can state that
the sequence of partial sums is increasing, and therefore, by MCT, the sum
is convergent.


\textit{(b) Let $s_{nn} = \sum_{i = 1}^n\sum_{j = 1}^n a_i b_j$, and use the
  Algebraic Limit Theorem to show that $\lim_{n \to \infty} s_{nn} = AB$.
  Conclude that }
$$\sum_{i = 1}^{\infty} \sum_{j = 1}^{\infty} a_j b_i =
\sum_{j = 1}^{\infty} \sum_{i = 1}^{\infty} a_j b_i =
\sum_{k = 2}^{\infty}  d_k = AB$$

$$s_n = a_1 b_1 + a_1 b_2 + a_1 b_3 + .. + a_n b_n = $$
$$= a_1 (b_1 + b_2 + ... + b_n) + a_2 (b_1 + b_2 + ... + b_n) + ...
+ a_n(b_1 + b_2 + ... + b_n) = $$
$$= (a_1 + a_2 + ... + a_n)(b_1 + b_2 + ... + b_n)) = p_n q_n$$
where $p_n$ and $q_n$ are sequences of partial sums for $\sum a_n$ and
$b_n$ respectively. Thus we can state that
$$\lim_{n \to \infty} s_{nn} = \lim_{n \to \infty} (\sum_{i = 1}^{n} a_i 
\sum_{i = 1}^{n} b_i) = AB$$

Therefore, by Theorem 2.8.1 and its consequences
$$\sum_{i = 1}^{\infty} \sum_{j = 1}^{\infty} a_j b_i =
\sum_{j = 1}^{\infty} \sum_{i = 1}^{\infty} a_j b_i =
\sum_{k = 2}^{\infty}  d_k = AB$$
as desired.



\chapter{Basic Topology of R}

\section*{3.2.1}
\textit{(a) Where in the proof of Theorem 3.2.3 part (ii) does the assumption
  that the collection of open sets be finite get used}

Theorem 3.2.3 states that

(ii) The intersection of a finite collection of open sets is open.

The assumption of the finality of the set is used in the fact, that we need
the minimum of the epsilons.

\textit{(b) Give an example of an infinite collection of nested open sets}
$$ O_1 \supseteq O_2 \supseteq O_3 \supseteq O_4 \supseteq ... $$
\textit{whose intersection $\cap_{n = 1}^{\infty} O_n$ is closed and nonempty.}

First of all, we should state that open is not an opposite of closed in this
context. We can get $O_n = (-\infty; \infty)$. Then this definition
(technically) fits into the requrement of qs

Let $O_n = (1 - 1/n, 2 + 1/n)$. Let us then define 
$$A = \cap_{n = 1}^{\infty} O_n$$

Then it follows that $x \in A \to 1 \leq x \leq 2$. Thus, $A = [1,2]$, which is
closed, as desired.


\section*{3.2.2 }
\textit{Let }
$$B = \left\{\frac{(-1)^nn}{n + 1}: n = 1,2,3,... \right\}$$

\textit{(a) Find the limit points of B}

1 and -1. This can be seen from the fact that $\lim \frac{n}{n + 1} = 1$ and
$\lim \frac{-n}{n + 1} = 1$, where both of the expressions in the limit
are subsequences of the original sequence (numbers were not derived by using
some process, just pure intuition).

$x \in B$ implies that $x = \frac{n}{n + 1}$  or $x = \frac{-n}{n + 1}$ for
some $n \in N$. Thus it can be shown, that -1 and 1 are the only limit points
of B.

\textit{(b) Is $B$ a closed set?}

No, it isn't, because it doesn't contain its limit points. This stems from
the fact that
$$\frac{(-1)^nn}{n + 1} = 1$$
$$(-1)^nn = n + 1$$
and no such number exists. The same can be shown for $-1$.

\textit{(c) Is $B$ an open set?}

No, because any neighborhood around any element will contain some  numbers,
that are not in $B$ (e.g. irrational numbers).

\textit{(d) Does $B$ contain any isolated points}

Given that none of the limit points are in the $B$, we can conclude that
the set $B$ consists exclusively of isolated points.

\textit{(e) Find $\bar{B}$}
$$\bar{B} = B \cup \{-1, 1\}$$

\section*{3.2.3}
\textit{Decide whether the following sets are open, closed, or neither. If a
  set is not open, find a point in the set for which there is no
  $\epsilon$-neighborhood contained in the set. If a set is not closed, find
  a limit point that is not containted in the set.  }

\textit{(a) $\textbf{Q}$}

Neither closed, nor open. Any number will suffice for both tests.
(proof - look at the irrational numbers).

\textit{(b) \textbf{N}}

Not open - any number will suffice. Closed - does not have limit points, and
thus contains the set of limit points (i.e. empty set).

\textit{(c) $\{x \in R: x > 0\}$}

Open. Is not closed, does not contain 0.

\textit{(d) $(0,1]$}

Not open, 1. Not closed, 0.

\textit{(e) $\{1 + 1/4 + 1/9 + ... + 1/n^2: n \in N\}$}

Not open, number 1 is an example. Not closed, does not contain its supremum.


\section*{3.2.4}
\textit{Prove the converse of Theorem 3.2.5 by showing that if
  $x = \lim a_n$ for some sequence $(a_n)$ contained in $A$ satisfying
  $a_n \neq x$, then x is a limit point of $A$.}

Firstly, let us state the theorem itself

\textbf{Theorem 3.2.5}
\textit{A point x is a limit point of a set $A$ if and only if $x = \lim a_n$
  for some sequence $(a_n)$ contained in $A$ satisfying $a_n \neq x$ for all
  $n \in \textbf{N}$.}

Let  $\epsilon > 0$. Then, by definition of convergence, there
exists $N \in \textbf{N}$, such that $n \geq N \to a_n \in V_\epsilon(x)$
(don't forget, that $x = \lim a_n$). Therefore there exists
$a_n \in V_\epsilon(x) \neq x $. Also, because the sequence is contained in
$A$, $a_n \in V_\epsilon(x) \cap A$.  Therefore every $\epsilon$-neighborhood
of $x$ intersects  $A$ in some point other that $x$. Therefore $x$ is a limit
point, as desired.

\section*{3.2.5}
\textit{Let $a \in A$. Prove that $a$ is an isolated point of $A$ if and only
  if there exists an $\epsilon$-neighborhood $V_\epsilon(a)$ such that
  $V_\epsilon(a) \cap A = \{a\}$ }

\textbf{In one direction:}

Suppose that $a$ is an isolated point of $A$. It follows, that it is not a
limit point. Thus, by definition of a limit point, there does not exist an
$V_\epsilon(a)$ such that it intersects $A$ at some other point than $a$.
In other words, $V_\epsilon(a) \cap A = \{a\}$, as desired.

\textbf{In other direction:}

We'll prove this one by contradiction. Suppose that
$V_\epsilon(a) \cap A = \{a\}$ and $a$ is a limit point. Then, there exists
a sequence $(a_n)$ fully contained in $A$, that converges to $a$. Therefore,
there exists $N \in \textbf{N}$ such that $n \geq N \to a_n \in V_\epsilon(a)$.
Therefore, $\{a, a_n\} \subseteq V_\epsilon(a) \cup A$, which is a
contradiction. Thus, $a$ is not a limit point and therefore is an isolated
point, as desired.

\section*{3.2.6}

\textit{Prove Theorem 3.2.8}

Firstly, let us state the theorem

\textbf{Theorem 3.2.8}
\textit{A set $F \subseteq R$ is closed if and only if every Cauchy sequence
  contained in $F$ has a limit that is also an element of $F$. }

\textbf{In one direction:}

Let  $F \subseteq R$ be closed. We are going to proceed with proof by
contradiction.

Suppose that there exists a Cauchy sequence $(s_n)$, that is contained in $F$,
that has a limit outside of $F$. Then $x$ is a limit point of $F$. Then it is
contained in $F$, because $F$ is closed, which is a contradiction.

\textbf{In other direction:}

Assume that every Cauchy sequence  contained in $F$ has a limit
that is also an element of $F$.

We are going to use a proof by contradiction on this one as well. Suppose that
$F$ is not closed. Then, there exists $x$, that is a limit point of $F$, that
is not in $F$. Therefore, there exists a sequence, that is contained in $F$,
that converges to $x$. Then, this sequence is Cauchy. Thus, we have a Cauchy
sequence, that is contained in $F$, which converges to a point outside of $F$,
which is a contradiction.

\section*{3.2.7}
\textit{Let $x \in O$ where $O$ is an open set. If $(x_n)$ is a sequence
  converging to $x$, prove that all but a finite number of the terms of
  $(x_n)$ must be contained in $O$. }

Becaus $x$ is in open set, there exists an $\epsilon$ such that the neighborhood $V_\epsilon (x) \subseteq O$. Thus, by the convergenve of $(x_n)$ to $x$,
there
exists $N \in \textbf{N}$ such that $n \geq N \to x_n \in V_\epsilon$.
Therefore for this sequence only $N$ or less elements are outside of $O$.
Therefore for any convergent sequence $(x_n)$ it is true, that all but a finite
number of terms of $(x_n)$ are outside of $O$, as desired.

\section*{3.2.8}
\textit{Given $A \subseteq R$, let $L$ be the set of all limit points of
  $A$.}

\textit{(a) Show that the set $L$ is closed.}

Suppose that $L$ is not closed. Thus, there exists $x \notin L$, that is a
limit point of $L$. Therefore there exists a sequence $(s_n)$, that is
convergent to $x$, that is fully contained in $L$. Therefore for
every $\epsilon$ there exists $N \in \textbf{N}$ such that $n \geq N$ implies
that $s_n \in  V_\epsilon(x)$. Because $V_\epsilon(x)$ is an open set, there
exists a neighborhood $V_{\epsilon_1}(s_n) \subseteq V_\epsilon(x)$.
Because $(s_n)$ is contained in $L$, $s_n$ is a limit point of $a$. Therefore,
$V_{\epsilon_1}(s_n)$ contains some element of $A$ other that $s_n$.
Therefore every neighborhood of $V_\epsilon(x)$ contains an element of $A$.
Therefore $x$ is a limit point of $A$. Therefore $x \in L$, which is a
contradiction. Therefore $L$ is closed, as desired.

\textit{(b) Argue that if $x$ is a limit point of $A \cup L$, thex $x$ is a
  limit point of $A$. Use this observation to furnish a proof for
  Theorem 3.2.12}

Suppose that $x$ is a limit point of $A \cup L$. Then it follows that every
$\epsilon$-neighborhood of $x$ intersects $A \cup L$ at some point other than $x$. Let us look at particular  $V_\epsilon(x)$.
We can follow that there exists a point in the neighborhood,
that it is in $A \cup L$ and  is distinct from $x$. Let us call this point  $a$.
From the fact, that $a \in A \cup L$ we can state that $a$ is either in $A$,
$L$, or both.
If the point is in $L$, then it follows, that it is a limit point of $A$.
Because $a \in V_\epsilon(x)$ and the fact, that $V_\epsilon(x)$ is an open set,
we can state that there exists a neighborhood
$V_{\epsilon_1}(a) \subseteq V_\epsilon(x)$. Therefore, because $a$ is a point
in $L$, is is a limit point of $A$. Thus $V_{\epsilon_1}(a)$ intersects $A$ at
some point. Thus, there exists a point $q \in A$ and $q \in V_{\epsilon_1}(a)$.
Therefore $q \in V_\epsilon(x)$. Therefore any neighborhood of $x$ contains a
point in $A$. Therefore it is a limit point of $A$, as desired.

Now let us state Theorem 3.2.12

\textbf{Theorem 3.2.12} \textit{For any $A \subseteq \textbf{R}$, the closure
  $\bar{A}$ is a closed set and is the smallest set containing $A$.}

From what we've argued we can follow that any limit point of $\bar A = A \cup L$
is a limit point of $A$, and therefore is an element of $L$ and therefore an
element of $\bar{A}$.  Thus all limit points of $\bar A$ is in $\bar A$. Therefore
it is a closed set.

Any closed set containing $A$ must contain $L$ as well. This shows, that $\bar A$
is the smallest closed set containing $A$.

\section*{3.2.9}

\textit{(a) If $y$ is a limit point of $A \cup B$, show that $y$ is either a limit
  point of $A$ of a limit point of $B$ (or both).}


We are going to proceed with a proof by contradiction. Suppose that there exists a
limit point $x$ of $A \cup B$, that is not a limit point of $A$ or $B$. Then it
follows, that there exists a sequence $(s_n)$, that is contained in $A \cup B$,
converges to $x$ and such that $s_n \neq x$ for every $n \in N$. Then this
sequence contains infinite anount of elements in either $A$, $B$ or both
(rigorous conclusion can be drawn from the argument, that there could not
be finite amount of elements of the sequence in both $A$ and $B$).
Therefore there exists a subsequence in either $A$ or $B$, that is convergent to
$x$. Therefore $x$ is a limit point for either $A$ or $B$.

\textit{(b) Prove that $\overline{A \cup B} = \overline A \cup \overline B$}

We already concluded, that if $y$ is a limit point of $A \cup B$, then $y$ is a limit point
of $A$, or a limit point of $B$. In other words,
$$y \in L_{A\cup B} \to y \in L_A \text{ or } y \in L_B$$
Then it follows that
$$y \in \overline{A\cup B} \to y \in L_A \text{ or } y \in L_B \text{ or } y \in A \cup B$$
$$y \in \overline{A\cup B} \to y \in L_A \text{ or } y \in L_B \text{ or } y \in A
\text{ or } y \in B$$
$$y \in \overline{A\cup B} \to   y \in A \text{ or }  y \in L_A 
\text{ or } y \in B \text{ or }  y \in L_B$$
$$y \in \overline{A\cup B} \to   y \in \overline A
\text{ or } y \in \overline B $$
$$y \in \overline{A\cup B} \to   y \in \overline A
\cup y \in \overline B $$

The usage of the word "or" is a little liberal here, but the idea is the same - both
$\overline{A \cup B}$ and $\overline A \cup \overline B$ exhaustively decompose to the same
4 sets, and therefore are equal.

\textit{(c) Does the result about closures in (b) extend to infinite }

The current argument extends at least to the finite amount of element by the virtue of
induction. Something tells me, that this particular thing does not extend though to the
infinite amount of element.

One possible example on why this proposition might be false can be drawn
from the fact, that closure of any
ratonal number is the number itself, therefore the union of closures of all rational numbers is
the set of rational numbers, but the closure of the set of all rationall numbers is the
set of real numbers.

I'm gonna roll with this particular counter-example as the proof of the fact, that given
proposition is false, because I don't see anything that is wrong with it.

Another example  is the union of sets, where each set contains one element - $1/n$ for
some $n \in N$. Same logic applies, closure of any given set is the set itself, but
the closure of union of sets includes zero.


\section*{3.2.10 (De Morgan's Laws)}
\textit{A proof for the De Morgan's Laws in the case of two sets is outlined in Exercise 1.2.3.
  The general argument is simular.}

\textit{(a) Given a collection of sets $\{E_\lambda: \lambda \in  \Lambda\}$, show that }
$$\left(\cup_{\lambda \in \Lambda} E_\lambda \right)^c = \cap_{\lambda \in \Lambda} E_\lambda^c$$
and
$$\left(\cap_{\lambda \in \Lambda} E_\lambda \right)^c = \cup_{\lambda \in \Lambda} E_\lambda^c$$

Suppose that $x \in \left(\cup_{\lambda \in \Lambda} E_\lambda \right)^c$. It
follows, that $x$ is not in the union of given sets. Therefore there is no
set $E_n$ such that $x \in E_n$ (because if there would be such a set, then $x$
wouldn't be in $\left(\cup_{\lambda \in \Lambda} E_\lambda \right)^c$).
Therefore $x \in \cap_{\lambda \in \Lambda} E_\lambda^c$. Therefore 
$$\left(\cup_{\lambda \in \Lambda} E_\lambda \right)^c \subseteq \cap_{\lambda \in \Lambda} E_\lambda^c$$

The proof of reverse inclusion is the same as with the forward, but in reverse
order.

$x \in \left(\cap_{\lambda \in \Lambda} E_\lambda \right)^c$ implies that
$x$ is not in every $E_n$. Therefore there exists $x \in E_n^c$ for some $E_n$.
therefore it is in $\cup_{\lambda \in \Lambda} E_\lambda^c$. The proof of
reverse inclusion uses the same argument, but in other direction.

\textit{(b) Now, provide the details for the proof of Theorem 3.2.14}

Let us state the theorem first.

\textbf{Theorem 3.2.14}
\textit{(i) The union of a finite collection of closed sets is closed.}

\textit{(ii) The intersection of an arbitrary collection of closed sets is closed}

Let $\{E_\lambda: \lambda \in \Lambda\}$ be a finite collection of closed sets.
Therefore $\{E_\lambda^c: \lambda \in \Lambda\}$ is a finite collection of open
sets. Therefore intersection
$$\cap_{\lambda \in \Lambda} E_\lambda^c = (\cup_{\lambda \in \Lambda} E_\lambda)^c$$
is open. Therefore $\cup_{\lambda \in \Lambda} E_\lambda$ is closed, as desired.

Let $\{E_\lambda: \lambda \in \Lambda\}$ be a arbitrary collection of closed sets.
Therefore $\{E_\lambda^c: \lambda \in \Lambda\}$ is a arbitrary collection of open
sets. Therefore union
$$\cup_{\lambda \in \Lambda} E_\lambda^c = (\cap_{\lambda \in \Lambda} E_\lambda)^c$$
is open. Therefore $\cap_{\lambda \in \Lambda} E_\lambda$ is closed, as desired.

\section*{3.2.11}
\textit{Let $A$ be bounded above so that $s = \sup A$ exists. Show that
  $s \in \overline A$}

For eny $\epsilon > 0$ there exists an element $a \in A$ such that
$$a > s - \epsilon$$
$$a - s > - \epsilon$$
$$s - a < \epsilon$$
s is a supremum of $A$, therefore $s \geq a$, therefore $s - a \geq 0$. Thus
$s - a = |s - a|$. Therefore 
$$|s - a| < \epsilon$$
Therefore $a \in V_\epsilon(s)$ for any $\epsilon > 0$. Thus, s is a limit point of
$A$. Threfore $s \in \overline A$, as desired.

\section*{3.2.12}
\textit{Decide whether the following statements are true or false. Provide counterexamples for those that are false, and supply proofs for those that are true.}

\textit{(a) For any set $A \subseteq \textbf{R}$, $\overline A^c$ is open.}

True. Closure of any set is closed, therefore complement of closure is open.

\textit{(b) If a set $A$ has an isolated point, it cannot be an open set}

True. We're gonna proceed with a proof by contradiction.
Suppose that $A$ is an open set and $x \in A$ is an isolated point. Then there
exists a neighborhood $V_\epsilon(x) \subseteq A$. Therefore any neighborhood
of $a$ intersects $A$ as some point (this can be proven rigorously, but it's
tedious and redundant). Therefore $x$ is a limit point of $A$, which is a
contradiction. Therefore it is not isolated, therefore we have a contradiction.

\textit{(c) A set $A$ is closed if and only if $\overline A = A$.}

True

In one direction: if $A$ is closed, then $L \subseteq A$, therefore
$\overline A = A \cup L  = A$.

In other direction: $\overline A = A \cup L  = A$ implies that $L \subseteq A$.
Therefore $A$ is closed. (By the way, $L$ denotes a set of limit points of $A$ )

\textit{(d) If $A$ is a bounded set, then $s = \sup A$ is a limit point of A}

False. Any finite set is counterexample.

\textit{(e) Every finite set is closed.}

True. Finite sets consit of isolated points, therefore they don't have limit
points, therefore they contain their limit points.

\textit{(f) An open set that containt every rational number must necessarily be
  all of $R$.}

False. $\textbf{R} \setminus \{\sqrt{2}\}$.

\section*{3.2.13}
\textit{Prove that the only sets that are both open and closed are $R$ and
  the empty set $\emptyset$}

Suppose that there exists a set $B \neq R \neq \emptyset$, that is both
closed and open. Then let $x \in B^c$. It follows, that $x$ cannot be a limit
point of $B$ (because if it was, then it would be in $B$, because $B$ is closed).
Therefore we have an isolated point in an open set $B^c$, which is imposible
(as proven in the previous exercise). Therefore we have a point which is
neither a limit point, nor a isolated point, which is a contradicion.

Same argument doesn't work on $R$ and $\emptyset$, because of the fact,
that $\emptyset$ doesn't contain any points.

\section*{3.2.14}
\textit{A set $A$ is called $F_\sigma$ set if it can be written as the
  countable union of closed sets. A set $B$ is called $G_\delta$ set if it
  can be written as the countable intersection of open sets}

\textit{(a) Show that a closed interval $[a,b]$ is a $G_\delta$ set.}

Let there be a collection of open sets, defined by
$$\{(-1/n + a, b + 1/n): n \in N\}$$

Then its intersection is equal to $[a,b]$ (This can be shown by using the fact, that
$[a,b]$ is in every set of the  collection, but any number outside of $[a,b]$
isn't). Therefore it is a $G_\delta$ set.

\textit{(b) Show that the half-open interval $(a,b]$ is both a $G_\delta$ and
  an $F_\sigma$ set.}

The intersection of
$$\{(a, b + 1/n): n \in N\}$$
is equal to $(a, b]$, therefore it is a $G_\delta$ set.

The union of
$$\{[a + 1/n, b]: n \in N\}$$
is equal to $(a, b]$ (if  $a + 1/n < b$, then let the set corresponding be an
empty set), therefore it is a $F_\sigma$ set.

\textit{(c) Show that $\textbf{Q}$ is an $F_\sigma$, and set of irrationals
  $\textbf{I}$ forms a $G_\sigma$ set.}

We know, that $Q$ is a countable set. Also, singleton $\{m\}$ is a closed set.
Therefore union of 
$\{m: m \in Q\}$
is a countable union of closed sets, that is equal to $Q$. Therefore $Q$ is a
$F_\sigma$ set.

Also, let
$\{R \setminus m: m \in Q\}$
be a collection of open sets. Then its intersection is equal to $I$.
Therefore $I$ is a $G_\delta$ set.

\section*{3.3.1}
\textit{Show that if $K$ is compact, then $\sup K$ and $\inf K$ both exist
  and are elements of $K$}

If $K$ is compact, then it is bounded (i.e. bounded above and below), and
thus have supremum and infinum.

Now let us take into account the fact, that for every $\epsilon > 0$ it is
true, that there exist elements $k_1$ and $k_2$ (with a possibility
that $k_1 = k_2$) such that

$$k_1 > \sup K - \epsilon$$
$$k_2 < \inf K + \epsilon$$
thus
$$\sup K - k_1 <  \epsilon$$
$$k_2 - \inf K < \epsilon$$
because of the properties of supremum and infinum $\sup K - k_1 \geq  0$ and
$k_2 - \inf K \geq 0$. Thus
$$|\sup K - k_1| < \epsilon$$
$$|k_2 - \inf K| < \epsilon$$

Therefore we can state, that every neighborhood around supremum and infinum
has a member of $K$ in it. Thus $\sup K$ and $\inf  K$ are limit points of $K$.
Thus, because $K$ is closed, we can conclude that $\sup K \in K$ and
$\inf K \in K$, as desired.

\section*{3.3.2}
\textit{Prove the converse of Theorem 3.3.4 by showing that if a set
  $K \subseteq R$ is closed and bounded, then it is compact.}

Firstly, let us state the theorem itself:

\textbf{Theorem 3.3.4}
\textit{A set $K \subseteq R$ is compact if and only if it is closed and
  bounded.}

Let $(s_n)$ be a sequence, that is contained in $K$. Thus, because $K$ is
bounded, $(s_n)$ is bounded as well. Thus, by BW, it has a convergent
subsequence $(s_{n_k})$. If this subsequence has its limit as an
element of subsequence(i.e. $s_{n_k} = c$
for some $c \in R$ and some of $n_k \in  \textbf{N}$),
then its limit is in $K$ by the virtue of the fact, that $s_n \in K$ for
all $n \in \textbf{N}$.
If subsequence doesn't have its limit as its element, then we can state,
that its limit is a limit point of $K$, and therefore in $K$ by the fact, that
$K$ is closed.

\section*{3.3.3}
\textit{Show that the cantor set defined in Section 3.1 is compact set. }

I'll not provide a definition here, because it is too lengthy, but you can
look it up online. The Cantor set is bounded by definition, therefore we don't
need to worry about that. Now by the same definition, 
the compliment of Cantor set is a infinite union of open sets, therefore
the compliment of Cantor set is an open set itself. Thus, we can state
that a compliment of a compliment (i.e. the set itself) is closed. Thus,
Cantor set is closed and bounded, and therefore compact, as desired.

\section*{3.3.4}
\textit{Show that if $K$ is compact and $F$ is closed, then $K \cap F$ is
  compact.}

The intersection of arbitrary  collection of closed sets is closed,
therefore $K \cap F$ is closed as well.
$x \in K \cap F \to x \in K \to |x| \leq M$, where $M$ is a bound of $K$.
Therefore $K \cap  F $ is bounded. Therefore $K \cap F$ is compact, as desired.

\section*{3.3.5}
\textit{Decide which of the following sets are compact. For those that are not compact, show how Definition 3.3.1 breaks down. In other words, give an example of a sequence contained in the given set that does not possess a subsequence converging to a limit in the set.}

\textit{(a) $\textbf{Q}$}

Not compact. $(s_n) = \{1, 1.4, 1.41, 1.414, 1.4142, ... \}$ - sequence, that
converges to $\sqrt{2}$.

\textit{(b) $\textbf{Q} \cap [0, 1]$}

Not compact. $(s_n) = \{0.7, 0.7, 0.707, 0.7071, ... \}$ - sequence, that
converges to $\sqrt{2} / 2$.

\textit{(c) \textbf{R}}

Not compact $(x_n) = n$ - doesn't converge, and none of its subsequences
converge as well.

\textit{(d) $\textbf{R} \cap [0,1]$}

Compact.

\textit{(e) $\{1, 1/2, 1/3, 1/4,...\}$}

Not compact. $(s_n) = 1/n$ converges to 0, but 0 is not in the set.

\textit{(d)  $\{1, 1/2, 2/3, 3/4, 4/5,...\}$}

Compact.

\section*{3.3.6}
\textit{As some more evidence of the surprising nature of the Cantor set,
  follow these steps  to show that the sum $C + C = \{x + y: x,y \in C\}$ is
  equal to the closed interval $[0,2]$. (Keep in mind that $C$ has zero length
  and contains no intervals. )}

\textit{The observation that $\{x + y: x,y \in C\} \subseteq [0,2]$ passes
  for obvious, so we only need to prove the reverse inclusion
  $[0, 2] \subseteq \{x + y: x,y \in C\}$. Thus, given $s \in [0,2]$, we
  must find two elements $x,y \in C$ satisfying $x + y = s$.}

\textit{(a) Show that there exist $x_1,y_1 \in C_1$ for which
  $x_1 + y_1 = s$. Show in general that, for an arbitrary $n \in \textbf{N}$,
  we  can always find $x_n,y_n \in C_n$ fow which $x_n + y_n = s$.
}

If $s \in C_1$, then we can set $x_n = s$.

If $s \in (1/3, 2/3)$ pick $x_n = 1/3$.

If $s \in (1, 4/3]$ pick $x_n = 1$

If $s \in (4/3,2]$ pick $x_n = 2/3$.

We are going to proceed with induction on this one. The base case is taken
care of ($C_1$).

Now proposition is than $x_{n - 1} + y_{n - 1} \in C_{n - 1}$,
$x_{n - 1} \in C_{n - 1}$ and $y_{n - 1} \in C_{n - 1}$.

Let the length of closed interval, that composes $C_n$ be equal to $l$
(i.e. 1/3 for $C_1$, 1/9 for $C_2$ and so on; rigorously it'll be equal
to $3^{-n}$). 
Then pick interval  $I_x \subseteq \in C_n$ 
such  $I_x$ is the "largest"
interval, that composed $C_n$ with the property that
$\exists i \in I_x: i \leq x_{n - 1}$ (largest
in this context rigorously means that
$\forall n \in N: u \in I_n, o \in I_x \to u < o$; in a more lay terms
pick the one, that is furthest to the right, but which either contains or
less than $x_{n - 1}$). Define in  the same style $I_y$ for $y_{n - 1}$.
Then pick $q_x$ and $q_y$ be the lowest numbers in corresponding intervals.

Let us now look closer at $q_x$ and $q_y$. Because $q_x$ is a lower bound
for interval in $C_n$,
we can state, that $[q_x,q_x + l]$ is in $C_n$. If $x_{n - 1} \in C_n$, then it
is the end of the story and the only can set new $x_n$ to the $[q_x + l]$.
Otherwise, we also get interval $[q_x + 2l, q_x + 3l]$ in our disposal.


$x_{n - 1} \in C_{n - 1}$  implies that $x_{n - 1} - q_x \leq 2l$.
Same applies to $y_{n - 1}$. 
Thus it follows, that $x_{n - 1} + y_{n - 1} - (q_x + q_y) \leq 4l$,
where $l$ is the length of an interval in $C_n$.

Now let $k = x_{n - 1} + y_{n - 1} - (q_x + q_y) \leq l$.

If $k \leq l$ then set $x_n = q_x$ and $y_n = q_y + k$.

If $l < k \leq 2l$ then set $x_n = q_x + l$ and $y_n + (k - l)$.

If $2l < k \leq 3l$ then set $x_n = q_x + 2l$ and $y_n + (k - 2l)$ (given that
$x_{n - 1} \notin C_n$; If $x_{n - 1} \in C_n$, set $x_n = q_x (k - 2l)$
and $y = q_y + 2l$)

If $k \leq 4l$ then set $x_n = q_x + 3l$ and $y_n + (k - 3l)$.

Thus we can state, that $\exists x_n, y_n \in C_n$ for all $n \in N$.

\textit{(b) Keeping in mind that the sequences $(x_n)$ and $(y_n)$ do not
  necessarily converge, showhow they can nevertheless be used to produce the
  desired $x$ and $y$ in $C$ satisfying $x + y = s$}

Because sequences $(x_n)$ and $(y_n)$ are contained in $C$, we can conclude,
that they are bounded. Therefore they have a convergent subsequence
$(x_{n_k}) \to x$ and $(y_{n_k}) \to y$.
Therefore $(x_{n_k} + y_{n_k}) \to x + y$.
But we know that  $x_n + y_n = s$ for every
$n \in N$ (by the definition of the sequence). Thus, sequence
$(x_n + y_n) \to s$. We know, that subsequences of convergent sequence converge
to the same limit. Thus, $(x_{n_k} + y_{n_k}) \to s$. Because same sequence
cannot converge to two different limits we can conclude that $s = x + y$,
as desired.

\section*{3.3.7}
\textit{Decide whether the following propositions are true or false. If the claim is valid, supply a short proof, and if the claim is false, provide a counterexample.}

\textit{(a) An arbitrary intersection of compact sets is compact.}

I want to note here firstly, that by our definitions, $\emptyset$ is a
compact set

Every element of out arbitrary intersection is contained in the first set.
Thus, the intersection is bounded. We know, tha arbitrary intersection of
closed sets is closed. Thus, the intersection is closed as well. Thus it
is closed and bounded. Thus it it compact.

\textit{(b) Let $A \subseteq \textbf{R}$ be arbitrary, and let
  $K \subseteq \textbf{R}$ be compact. Then, the intersection $A \cap K$ is
  compact}

False. $[0, 1] \cap (0, 1) = (0, 1)$.

\textit{(c) If $F_1 \supseteq F_2 \supseteq F_3 ... $ is a nested sequence
  of nonempty closed sets, then the intersection
  $\cap_{n = 1}^{\infty} F_n \neq \emptyset$}

False. Let $F_n = [n, \infty)$.

\textit{(d) A finite set is always compact.}

True. It contains only isolated points and therefore is closed. Also,
we can find the maximum and minimum of it, therefore it is bounded.

\textit{(e) A countable set is always compact}

False. $N$ is not bounded, $Q$ is not closed.

\section*{3.3.8}
\textit{Follow these steps to prove the final implication in Theorem 3.3.8.}

\textit{Assume $K$ satisfies (i) and (ii), and let
  $\{O_\lambda: \lambda \in \Lambda \}$ be an open cover for $K$. For contradiction, let's assume that no finite subcover exists. Let $I_0$ be a closed
  inverval containing $K$, and bisect $I_0$ into two closed intervals $A_1$
  and $B_1$.}

\textit{(a) Why must either $A_1 \cap K$ or $B_1 \cap K$ (or both) have no
  finite subcover consisting of sets from
  $\{O_\lambda: \lambda \in \Lambda \}$ }

Suppose that both of $A_1 \cap K$ and $B_1 \cap K$ have finite subcover.
Then if we take a union of those subsovers it will cover whole $K$ and
we'll have a finite subcover for $K$, which is a contradiction.

\textit{(b) Show that there exists a nested sequence of closed intervals
  $I_0 \supseteq I_1 \supseteq I_2 ... $ with the property that, for each $n$,
  $I_n \cup K$ cannot be finitely covered and $\lim |I_n| = 0$.}

Let us have $I_n$ ($I_0$ for the base case) such that $I_n \cap K$ does not
contain finite subcovers. Divide this set into two equaly sized closed
intervals $A_n$ and $B_n$. Then at least one of $A_n \cap K$ and $B_n \cap K$
will not be finitely covered (because if both of them were finitely covered,
then $I_n$ would be finitely covered). Therefore set $I_{n + 1}$ to either
$A_n$ or $B_n$, whichever is not finitely covered by original set.
Because they are equally sized, the sizes of intervals are convergent to 0
(size of each of them is $2^-n$ times the size of $I_0$, and $I_0$ is a
closed interval (not closed set) and therefore is bounded, therefore has a
finite length).

\textit{(c) Show that there exists an $x \in K$ such that $x \in I_n$ for all
  $n$.}

Each $I_n \cap K$ is an intersection of two compact sets (closed interval and
a compact set), and therefore compact. Therefore we have a series of nested compacted sets, therefore its intersection is non-empty. Therefore there exists
$x \in K$ such that  $x \in I_n$  for all $n \in N$.

\textit{(d) Because $x \in K$, there must exist an open set $O_{\lambda_0}$
  from th original collection that contains $x$ as an element. Argue that
  there must be an $n_0$ large enough to guarantee  that
  $I_{n_0} \subseteq O_{\lambda_0}$. Explain why this furnished us with the
  desired contradiction.}

Because $O_{\lambda_0}$ is an open set and $x \in O_{\lambda_0}$, there exists
neighborhood $V_\epsilon(x)$ such that $V_\epsilon(x) \subseteq O_{\lambda_0}$.
Also, there exists $I_{n_0}$ with length $\epsilon / 2$ (because lengths of
$I_n$'s are convergent to 0). Thus,
$$ \forall q \in R:  -\epsilon / 2 \leq q - x \leq \epsilon \to
-\epsilon < q - x < \epsilon \to |q - x| < \epsilon$$
Therefore
$$I_{n_0} \subseteq V_\epsilon(x) \to I_{n_0} \subseteq O_{\lambda_0}$$
Therefore $I_{n_0}$ is finitely covered, which is a contradiction. Therefore
there does not exists an open cover for compact set, for which there is no
finite subcover, as desired.

\section*{3.3.9}
\textit{Consider each of the sets listed in Exercise 3.3.5. For each one that
  is not compact, find an open cover for which there is no finite subcover.}

For 
$$Q$$
it is $(-n, n)$

For
$$Q \cup [0,1]$$
it is $\{(-1, 0.7) \cup (0.8, 2), (-1, 0.707) \cup (0.708, 2), (-1, 0.7071) \cup (0.7072, 2), ...\}$ where boundaries of corresponding intervals converge to
$\sqrt{2}/2$.

For
$$R$$
it is $(-n, n)$

For
$$\{1, 1/2, 1/3, 1/4, 1/5\}$$
it is $(1/n, 2)$.

\section*{3.3.10}
\textit{Let's call a set clompact if it has the property that every closed
  cover (i.e., a cover consisting of closed sets) admits a finite subcover.
  Describe all of the clompact subsets of $R$.}

First of all, no open set can be a clompact set, because for any number
in the open set there
exists a neighborhood, and we add to a collection of sets a collection
$[\epsilon - 1/n + x, x + \epsilon + 1/n]$, that will prevent us from
creating a finite subcover.

Same applies to any set, that has an interval in it.

Finite sets are clompact (empty set included).

Cantor set is not clompact
($\{[0, 0], [0, 1 - 1/n], 1 \}$ will prevent us from constructing a finite
subcover.)

Any compact set cannot be a clompact.

Basically, any isolated point is a closed set in and of itself, therefore
infinite sets cannot be clompact.

Therefore the only clompact sets are the finite ones.

If a set is finite, then it'll certainly have in its clompact cover a set, where
each one of the points are located, therefore finite sets are clompact.

Therefore set is clompact if and only if it is finite.


\section*{3.4.1}
\textit{If $P$ is a perfect set and $K$ is compact, is the intersection
  $P \cap K$ always compact? Always perfect?}

$x \in P \cap K \to x \in K$, thus $P \cap K$ is bounded. Intersection of
arbitrary collection of closed sets are closed, therefore $P \cap K$ is
closed as well. Therefore $P \cap K$ is always compact.

The intersection is not always perfect - $[1, 2] \cap [2, 3] = \{2\}$.

\section*{3.4.2}
\textit{Does there exist a perfect set consisting of only rational numbers?}

If we count empty set as a set, that consists of only rational numbers
(i.e. if we translate the text of this exercise to $x \in B \to x \in Q$),
then yes.

If not, then no. This stems from the fact, that the set of rationals is
countable, and non-empty perfect sets are uncountable.

\section*{3.4.3}
\textit{Review the portion of the proof given for Theorem 3.4.2 and follow
  these steps to complete the argument}

First of all, Theorem 3.4.2 states that Cantor set is perfect.

\textit{(a) Because $x \in C_1$ argue that there exists an $x_1 \in C \cap C_1$
  with $x_1 \neq x$ satisfying $|x - x_1| \leq 1/3$}

Here we can even go as far as to say, that there exists $x \in C$ such that
$|x - x_1| \leq 1/6$. The idea here is that the $x_1 \in C_1$ implies,
that there exists a closed interval $I_1 \subseteq C_1$, which length is $1/3$
such that $x \in I_1$. Now the fact, that $x \in I_1$ and $|I_1| = 1/3$ implies
that the minimal distance between the boundary of $I_1$ and $x$ is the half the length  of the interval. Boundaries of
closed intervals, that constitute $C_1$ (or
$C_n$ in general) are inside of $C$. Therefore there exists $x \in C$ (and
$C \cap C_1$, because $C \cap C_n = C$ for every $n \in N$) such
that  $|x - x_1| \leq 1/6 \leq 1/3$.

\textit{(b) Finish the proof by showing that for each $n \in N$ there exists
  $x_n \in C \cap C_n$, different from $x$, satisfying
  $|x - x_n| \leq  1/3^n$}

Length of the interval, that constitutes $C_n$  is $3^{-n}$, therefore
$x \in C \to x \in C_n \to \exists x_n: x_n \in C, |x_n - x| < 1/3^n$.
Therefore for  any $\epsilon$-neighborhood of $x$ we will have $n \in N$ such
that $1/3^n < \epsilon$ (by convergence of $1/3^n$ to 0) and therefore
there will exist $x_n \in C$, that is also in desired neighborhood.
Therefore any $x \in C$ is a limit point of $C$. Therefore $C$ is a closed
set, that does not contain any isolated points. Therefore it is perfect
(how nice).

\section*{3.4.4}
\textit{Repeact the Cantor construction from Section 3.1 starting with the
  open interval $[0,1]$. This time, however, remove the open middle fourth
  from each component.}

\textit{Is the resulting set compact? Perfect?}

The set will surely be bounded. It will also be equal to the compliment
of union of arbitrary collection of open sets, and therefore will be closed.

By the same logic, as in previous exercise (but perhaps this time with a
slightly lower length of an interval) it will be perfect.

\textit{(b) Using the algorithms from Section 3.1, compute the length and
  dimention of this Cantor-like set.}

The provess will be the same as in 3.1, because I didn't get which
exect formula they used in order to get to the desired conclusion.

Let us called the desired set $E$. Then it follows, that after each iteration
we remove $1/4$ of $E_n$ to get to $E_{n + 1}$.


Therefore, the length of a $E$ will be defined as a limit of
a sequence
$$a_1 = 1$$
$$a_{n + 1} = a_n - a_n/4 = 3/4 a_n$$

Python shows, that it'll be equal to 0, but we'll need to use
a more rigorous approach.

First of all, let us show by induction that $a_n$ is more or equal to $0$
for any $n \in N$. Base case is already taken care of ($a_1  = 1\geq 0$).
Proposition is that $a_n \geq 0$. Therefore the step is trivial:
$$a_n \geq 0 \to 3/4 a_n \geq 0 \to a_{n + 1} \geq 0$$
as desired.

$$a_{n + 1} = a_n * 3/4 \to a_{n + 1} - a_n = 1/4 a_n \geq 0$$
therefore the series is decreasing. Therefore it is bounded below and
decreasing, therefore it is convergent. Also it shows, that it'll be bounded
above by 1.

Thus, we can conclude that
$$a_n = a_{n - 1} * 3/4 = a_{n - 2} * 9/16 = ... = a_1 * (3/4^n) = 3/4^n$$

We know, that $|b| < 1 $ implies that $ (b^n) \to 0$. $|E_n|$ =
$3/4^{n - 1}$. Therefore the length of $E$ is equal to $\lim 3/4^n = 0$.

Probably the half of this proof is redundant, but I'll roll with it anyways.

When we remove the first quarter from $[0, 1]$ we'll get
$[0, 3/8] \cup [5/8, 1]$. Therefore if we magnify the set by $8/3$, we'll get
$[0, 1] \cup [5/3, 8/3]$, or two copies of $E$. Thus, the dimention of $E$
(with processes and definitions from 3.1) will be
$$2 = (8/3)^x$$
$$\ln{(2)} / \ln{(8/3)} = x$$
$$x \approx 0.70669505...$$
Idk why do I need to know this, but here's the answer.

\section*{3.4.5}
\textit{Let $A$ and $B$ be subsets of $\textbf{R}$. Show that if there exist
  disjoint open sets $U$ and $V$ with $A \subseteq U$ and $B \subseteq V$,
  then $A$ and $B$ are separated.}

Suppose that $A$ and $B$ are not separated and let $x \in \overline A \cap B$.
(If $\overline A \cap B = \emptyset$, then swap names for  $A$ with $B$ and
$U$ with $V$).

It can be shown, that if $l$ is a limit point of $A$, then it is also a limit
point of $U \supseteq A$ (I won't rigorously prove it here because it was
earlier proven rigorously somewhere in this chapter's exercises; it is
also somewhat obvious and redundant). Thus,
$\overline A \subseteq \overline U$. Therefore,
$$l \in \overline A \cap B \to l \in \overline U \cap V \to l \in V$$

Because $l$ is in $V$ and $V$ is open, it implies
that there exists neighborhood
$V_\epsilon(l) \subseteq V$. Also, because $l$ is a limit point of $U$ we
can state, that every neighborhood of $l$ has a member of $U$ in it.
Thus, $\exists x \in U: x \in V_\epsilon(l) \to
\exists x \in U: x \in V$. Therefore $U$ and $V$ are not disjoint, which is
a contradiction.

\section*{3.4.6}
\textit{Prove Theorem 3.4.6}

Firstly, let us state the theorem itself:

\textbf{Theorem 3.4.6}
\textit{A set $E \subseteq \textbf{R}$ is connected if and only if, for all
  nonempty disjoint sets $A$ and $B$ satisfying $E = A \cup B$, there always
  exists a convergent sequence $(x_n) \to x$ with $(x_n)$ contained in one of
  $A$ or $B$, and $x$ an element of the other.}

Suppose that we divided $E$ into disjoint $A$ and $B$.

\textbf{In one direction:}

$E$ being connected implies that $\overline A \cap B \neq \emptyset$ (If
it is not the case, then swap names for $A$ and $B$). Let
$l \in \overline A \cap B$. Then it follows, that $l$ is a limit point for
$\overline A$. Thus, there exists a sequence, that is contained in $A$,
that is convergent to $l \in \overline A \cap B \to l \in B$, as desired.

\textbf{In other direction:}

Let us divide $E$ into disjoint $A$ and $B$. Then it follows, that there
exists a sequence $(x_n) \to x$ that is contained in $A$, that converges to a
limit in $B$ (once again, if it is not the case, then swap names).
Also, because $A$ and $B$ are disjoint, it follows that $x_n \neq x$ for
all $n \in N$.
Then it follows, that $x$ is a limit point of $A$. Thus
$x \in \overline A \cap B \neq \emptyset$. Therefore $A$ and $B$ are not
separated. Therefore $E$ is connected, as desired.


\section*{3.4.7}
\textit{(a) Find an example of disconnected set whose closure is connected}

$Q$

\textit{(b) If $A$ is connected, is $\overline A$ necessarily connected? If
  $A$ is perfect, s $\overline A$ necessarily perfect?}

If $A$ is empty or a singleton, then $A = \overline A$, therefore $\overline
A$ is connected as well. Therefore we need only to handle the case, when
$A$ is nonempty and not a singleton.

Suppose that $A$ is connected and $\overline A$ is disconnected. Thus we can
find two nonempry $Q$ and $W$ such that $Q \cup W = \overline A$.
Now let $A_1 = Q \cap A$, $A_2 = W \cap A$, $L_1 = Q \cap L$ and
$L_2 = W \cap L$.

$\overline Q \cap W = \emptyset$ implies that any limit points of $A_1$ are
not in $W$ and therefore not in $L_2$.
Also, by the same logic, any limit points of $A_2$ are not in $L_1$.

Let $A_1$ be empty. It follows, that $A \subseteq W$. Therefore $A_2 = A$.
Therefore $Q$ consists
of some of  limit points of $A$. But those limit points cannot be a limit
for any $A_2 = A$. Therefore $Q$ is empty, which is a contradiction.
(If $A_2$ is empty, then swap names and the same logic will apply; both
sets cannot be empty, because they have to contain at least $A$, which is
nonempty)

Therefore there exist two sets, such that $A_1 \cup A_2 = A$, and
$\overline A_1 \cap A_2 = \emptyset$ and $\overline A_2 \cap A_1 = \emptyset$.
Therefore we can write $A_1$ and $A_2$ are separated and nonempty.
Therefore $A$ is disconnected, which is a contradiction.


If $A$ is perfect, then it is closed, therefore $A = \overline A$. Therefore
$\overline A$ is perfect as well.

\section*{3.4.8}
\textit{A set $E$ is totally disconnected if, given two points $x, y \in E$,
  there exist separated $A$ and $B$  with $x \in A$, $y \in B$, and
  $E = A \cup B$}

I probably should add here, that we are also assuming that $x \neq y$,
otherwise this definition is meaningless.

\textit{(a) Show that $Q$ is totally disconnected}

We know, that between any two real numbers there exists an irrational number.
Thus for $x < y \in Q$ (swap names if it is not the case) there exists
an irrational number $i$ such that $x < i < y$. Thus, for those two points
there would exist two intervals $A = Q \cap (-\infty, i)$ and $B = Q \cap (i, \infty)$, which are separeted.

\textit{(b) Is the set of irrational numbers totally disconnected.}

Yes. It is also true that between two real numbers there exists a rational
number. Thus, by the same logic as in (a), the set of irrationals is totally
disconnected.

\section*{3.4.9}
\textit{Follow these steps to show that the Cantor set
  $C = \cap_{n = 0}^{\infty} C_n $ described in Section 3.1 is totally
  disconnected in the sense described in Exercise 3.4.8}

\textit{(a) Given $x, y \in C$, with $x < y$, set $\epsilon = y - x$. For
  each $n = 0, 1, 2,...$, the set $C_n$ consists of a finite number of closed
  intervals. Explain why there must exist an $N$ large enough so that it is
  impossible for $x$ and $y$ both to belong to same closed interval $C_N$.}

The length of an interval, that constitutes $C_n$ is $3^{-n + 1}$. The sequence
of lengths of those intervals converges to 0, therefore there exists
a $C_N$ such that maximum length of an interval in it is less than $\epsilon$.

\textit{(b) Argue that there exists a point $z \neq C$ such that $x < z < y$.
  Explain how this proves that there can be no interval of the form $(a,b)$
  with $a < b$ contained in $C$.}

Because for every $x, y \in C$ there exists $C_N$,
which intervals  cannot hold both
of $x$ and $y$, there exists a point $z \notin C_N$ such that $x < z < y$
(otherwise they would be in the same interval).

Thus for any two points $a< b \in C$ there exists $z \notin C$ such that
$a < z < b$.

\textit{(c) Show that $C$ is totally disconnected.}

We can take two intervals $A = (-\infty, z) \cap C$ and
$B = (-\infty, z) \cap C$, which would satisfy the constraint from the
definition.

\section*{3.4.10}
\textit{Let $\{r_1, r_2, r_3, ... \}$ be an enumeration of the rational
  numbers, and for each $n \in N$ set $\epsilon_n = 1/2$. Define
  $O = \cup_{n = 1}^{\infty}V_{\epsilon_n}(r_n)$, and let $F = O^c$.}

\textit{(a) Argue that $F$ is a closed, nonempty set consisting only of irrational numbers.}

$O$ is defined to be a union of arbitrary collection of open sets, therefore
it is open itself. Thus, its compliment, F,  must be closed.

The non-emptyness of this set might stem from the fact, that even if
every neigborhood is disjoint, then  length of $O$ is
equal to $2 \sum 1/2^n = 2$.

If $F$ is empty, then $O = \textbf{R}$. Therefore $O$ is closed and perfect.

Let $\{[0, 3] \setminus \cup_{i = 1}^{n}r_i\}$ be a collection of compact sets.
Then each one of them will be non-empty. Therefore their intersection
is nonempty. Therefore $F$, which is a superset of this set,  is nonempty.

% I'm not entirely satisfied with this proof, might return to it later.

% Suppose that $x \in F$ is a rational number. Then there exists a neighborhood
% in $O$ for this particular rational number. Therefore this number is not in
% $F$, which is a contradiction. Therefore $F$ contains only irrational numbers.


\textit{(b) Does F contain any nonempty open intervals? Is F totally
  disconnected? }

If $(a, b)$ is an interval in $F$, then it follows, that it contains a real
number, which is not if $F$. Therefore there are no open sets in $F$.

Therefore by this logic, $F$ is totally disconnected (pick a rational
number between $a$ and $b$, call it $r$, and produce
sets $(-\infty, r) \cup F)$ and $(r, \infty) \cup F$)

\textit{(c) Is it possible to know whether $F$ is perfect? If not, can we
  modify this construction to produce a nonempty perfect set of irrational
  numbers?}

It depends on how exactly do we define $r_1, r_2,...$ whether $F$ will be
perfect or not. We can define those neighborhoods to converge around some
irrational number, (i.e. make it so first one is has an upper bound of 3,
then define the  next one to be bound below by $3.2$, then upper to $3.14$,
then lower to $3.142$ and so on to $\pi$; Maybe we'll need to increase the
speed with which we converge in order to accomodate for the fact, that
epsilons are getting smaller pretty fast)

I don't think that we can somehow put a constraint on the epsilons, if we
don't define how to produce the $r$'s

I don't consider this particular exercise to be even close to being correctly
done, maybe i'll return to it later

\section*{3.5.1}
\textit{Argue that a set $A$ is a $G_\delta$ set if and only if its complement
  is an $F_\sigma$ set.}

\textbf{In one direction:}
Suppose that $G_\delta$ set is composed by $\cap_{n = 1}^{\infty} s_n$, where
$s_n$ is a closed set. Then by De Morgan rule
$$G_\delta^c = (\cap_{n = 1}^{\infty} s_n)^c =
\cup_{n = 1}^{\infty} s_n^c$$

Because $s_n$ is a closed set, then $s_n^c$ is an open set. Therefore 
$$G_\delta^c = \cup_{n = 1}^{\infty} s_n^c$$
is a $F_\sigma$ set.

\textit{In other direction:}
Suppose that $F_\sigma$ set is composed by $\cup_{n = 1}^{\infty} s_n$, where
$s_n$ is an open set. Then by De Morgan rule
$$F_\sigma^c = (\cup_{n = 1}^{\infty} s_n)^c =
\cap_{n = 1}^{\infty} s_n^c$$

Because $s_n$ is an open set, then $s_n^c$ is a closed set. Therefore 
$$F_\sigma^c = \cup_{n = 1}^{\infty} s_n^c$$
is a $G_\delta$ set.

\section*{3.5.2}
\textit{Replace each \rule{1cm}{0.15mm} with the word finite or countable,
  depending of which is more appropriate}

\textit{(a) The countable union of $F_\sigma$ sets is a $F_\sigma$ set}

\textit{(b) The finite intersection of $F_\sigma$ sets is an $F_\sigma$ set.}

\textit{(c) The finite union of $G_\delta$ sets is a $G_\delta$ set}

\textit{(d) The countable intersection of $G_\delta$ sets is an $G_\delta$ set.}

\section*{3.5.3}
\textit{(This exercise has already appeared as Exercise 3.2.14.)}

\section*{3.5.4}
\textbf{Theorem 3.5.2}  If $\{G_1, G_2, G_3, ... \}$ is a countable collections of dense,
open sets, then the intersection $\cap_{n = 1}^{\infty} G_n $ is not empty.

\textit{(a) Starting with $n = 1$, inductively construct a nested sequence of
  closed intervals $I_1 \supseteq I_2 \supseteq I_3 \supseteq ... $
  satisfying $I_n \subseteq G_n$. Give special attention to the issue of the
  endpoints of each $I_n$}

Let us pick any $x_1 \in G_1$ . Then, because it is open, there exists
$V_{\epsilon_1}(x) \subseteq G_1$. Let
$I_1 = [x_1 - \epsilon / 2, x_1 + \epsilon / 2]$. Then it follows, that there
exist $x_2 \in G_2$ such that $x_1 - \epsilon / 2 < x_2 < x_1 + \epsilon / 2$.
Then it follows that $x_2 \in  G_1 \cap G_2$.

In general  let
$I_n = [x_n - \epsilon / 2, x_n + \epsilon / 2]$. Then it follows, that there
exist $x_{n + 1} \in G_{n + 1}$ such that
$x_n - \epsilon / 2 < x_{n + 1} < x_1 + \epsilon / 2$.
Then it follows that $x_{n + 1} \in  G_{n} \cap G_{n + 1}$.

\textit{(b) Now, use Theorem 3.3.5 or the NIP to furnish the proof}

Therefore we have got the collection of nonempty sets, which satisties
$$I_n \subseteq G_n$$
and
$$I_{n + 1} \subseteq I_n$$

Therefore $\cap I_n \neq \emptyset$. Therefore
$$\cap_{n = 1}^{\infty} I_n \subseteq
\cap_{n = 1}^{\infty} G_n \neq \emptyset$$

\section*{3.5.5}
\textit{Show that it is impossible to write}
$$\textbf{R} = \cup_{n = 1}^{\infty}F_n$$
\textit{where for each $n \in N$, $F_n$ is a closed set containing
  no nonempty open intervals.}

Suppose that $\textbf{R} = \cup_{n = 1}^{\infty}F_n$ where $F_n$ are closed
sets. Then 
$$\textbf{R}^c = \emptyset = (\cup_{n = 1}^{\infty}F_n)^c =
\cap_{n = 1}^{\infty}(F_n)^c$$
Then $\cap_{n = 1}^{\infty}F_n^c = \emptyset $, where $F_n$ is a closed
set and therefore $F_n^c$ is an open set.

Let  $x,y \in \textbf{R} $ such that $x < y$. Then suppose that
there is no $z \in F_n^c$ such that $x < z < y$. Then it follows, that
$(x, y) \not \subseteq F_n^c$ Therefore $(x, y) \subseteq F_n$, which is a
contradiction. Therefore $F_n^c$ is dense in $R$.

Therefore $\cap_{n = 1}^{\infty}F_n^c$ is a countable intersection of dence,
open sets (countable because we sum from n to $\infty$, therefore on N).
Therefore it cannot be empty. Therefore $\cup_{n = 1}^{\infty}F_n \neq R$,
which is a contradiction.

\section*{3.5.6}
\textit{Show how the previous exercise implies that the set $I$ of irrationals
  cannot ber a $F_\sigma$ set, and $Q$ cannot be a $G_\delta$ set.}

We had already proven that $Q$ is a $F_\sigma$ set and $I$ is a
$G_\delta$ set. 

Suppose that $Q$ is a $G_\delta$ set. Then it follows that $Q^c = I$ is a
$F_\sigma$ set. Therefore $Q \cup Q^c = R = I \cup I^c$ is a $F_\sigma$ set.

Because $Q$ and $I$ is a $F_\sigma$ there exists collection $F_n$ and $S_n$
such that every $F_n$ and $S_n$ is a closed set. Also, for every
$x < y \in F_n$  it follows that $x, y \in Q$. Therefore, there exists
$z \in Q^c = I$ such that $x < z < y$. Therefore there does not exist
a nonempty open interval $(x, y) \subseteq F_n$. Same can be said
about $S_n$. Thus $Q \cup I = R$ can be written as the union of sets
$J_n$ where for each $n \in N$, $J_n$ is a closed set containing no nonempty
open intervals, which is a contradiction.

Thus $Q$ cannot be a $G_\delta$ set. Because  $I$ being $F_\sigma$ set implies
that $Q$ is a $G_\delta$ set, $I$ cannot be a $F_\sigma$ set.


\section*{3.5.7}
\textit{Using Exercise 3.5.6 and versions of the statements in Exercise 3.5.2,
  construct a set that is neithter in $F_\sigma$ nor a $G_\delta$.}

First of all, it is necessary to state that $\emptyset$ is in and of itself a
closed set. Therefore $\cup_{n = 1}^{\infty} \emptyset = \emptyset$. Thus
$\emptyset$ is a $F_\sigma$ set. Also, $\emptyset$ is an open set. Thus
$\cap_{n = 1}^{\infty} \emptyset = \emptyset$ is a $G_\delta$ set.

If $A$ is a closed set, then $A = \cup_{n = 1}^{\infty} A$ is a $F_\sigma$ set.
If $A$ is an open set, then $A = \cap_{n = 1}^{\infty} A$ is a $G_\delta$ set.

Let $W$ be a desired set. Then it follows that $W$ is not a $F_\sigma$, and
is not  a $G_\delta$ set. Also, $W^c$ has the same properties.

Any finite set is closed and therefore is a $F_\sigma$ set. Their compliment
is a $G_\delta$ set.

Therefore the set that we are looking for is neither closed nor open.

Cantor set won't do, because it is closed.

Set $\{1, 1/2, 1/3, ...\}$ won't do, because it can we written as a
countable union of closed singletons, and therefore it is $F_\sigma$ set.
Its compliment won't do for obvious reasons.

$Q$ is an $F_\sigma$ set. Then $Q \cap (0, 1)$ is not closed, because it doesn't contain 0 and 1. It can be still written as the countable union of closed
singletons, therefore it is still $F_\sigma$.

I decided to look up a hint in the internet saw the answer.
$((-\infty, 0) \cap I)) \cup ((0, \infty) \cap Q)$ is neither $F_\sigma$ nor
$G_\delta$. What a shameful waste of an exercise.


\section*{3.5.8}
\textit{Show that a set $E$ is nowhere-dense in $R$ if and only if the
  compliment of $\overline E$ is dense in $R$}

\textbf{In one direction:}
Suppose that $E$ is nowhere-dense. Then it follows, that $\overline E$ does
not contain any nonempty intervals. Thus, given any $x, y \in R$ it follows
that $(x, y) \not \subseteq \overline E$. Thus, there exists $k \in (x, y)$
such that $k \not \in \overline E$. Therefore $k \in (\overline E)^c$.
Thus, given two real numbers, there exists $k \in (\overline E)^c$ such
that $x < k < y$. Therefore $(\overline E)^c$ is dense in $R$, as desired.

\textbf{In other direction:}
Suppose that $(\overline E)^c$ is dense in $R$. Then, for any $x < y \in R$
there exists $z \in (\overline E)^c$ such that $x < z < y$. Therefore
there does not exist $x_1, y_1 \in R$ such that $(x, y) \subseteq \overline E$.
Therefore $E$ is nowhere-dense.

\section*{3.5.9}
\textit{Decide whether the following sets are dense in \textbf{R},
  nowhere-dense in \textbf{R}, or somewhere in between}

\textit{(a) $A = \textbf{Q} \cap [0, 5]$}

$\overline A = [0, 5]$, therefore the set is not nowhere-dense. It isn't
dense in $R$ though, because for $-1$ and $-1/2$ there is no number in the
$A$, that is between them.

\textit{(b) $B = \{1/n: n \in N\}$}

Nowhere-dense

\textit{(c) the set of irrationals}

Is dense in $R$.

\textit{(d) the Cantor set}

Cantor set is closed, and therefore $C = \overline C$. Also, it is totally
disconnected, and therefore $x < y \in C \to \exists z \in C^c: x < z < y$.
Therefore it does not contain any open intervals. Therefore it
is nowhere-dense.

\section*{3.5.10}
\textit{Finish the proof by finding a contradiction to the results in this
  section}

\textit{\textbf{Theorem 3.5.4 (Baire's Theorem)} The set of real numbers
  \textbf{R} cannot be written as the countable union of nowhere-dense sets.}

For contradition, assume that $E_1, E_2, E_3, ... $ are each nowhere-dense and
satisfy $\textbf{R} = \cup_{n = 1}^{\infty} E_n$. For each $E_n$ we can state
that $E_n \subseteq \overline E_n$. Thus, $\cup_{n = 1}^{\infty} E_n \subseteq
\cup_{n = 1}^{\infty} \overline E_n$. Therefore
$R \subseteq \cup_{n = 1}^{\infty} \overline E_n$. Therefore
$\cup_{n = 1}^{\infty} \overline E_n = R$. $E_n$ is nowhere-dense, therefore
$\overline E_n$ contains no nonempty intervals. Therefore $R$ can be written as
countable union of closed sets, where each of those sets contain no nonempty
open intervals, which is a contradiction.

\chapter{Functional Limits and Continuity}

\section*{4.2.1}
\textit{Use Definition 4.2.1 to supply a proof for the following limit statements.}

(a) $\lim_{x \to 2}(2x + 4) = 8$.

(b) $\lim_{x\to0} x^3 = 0$.

(c) $\lim_{x\to2} x^3 = 8$.

(d) $lim_{x\to\pi}[[x]] = 3$, where [[x]] denotes the greatest integer less than or
equal to x.

Let's first state Definition 4.2.1

\textbf{Definition 4.2.1.} Let $f : A \to \textbf{R}$, and let $c$ be a limit point of the domain $A$. We say that $\lim_{x\to c} f(x) = L$ provided that, for all $\epsilon > 0$, there exists
a $\delta > 0$ such that whenever $0 < |x - c| < \delta$ (and $x \in A$) it follows that $|f(x) - L| < \epsilon$.

(a):
$$ |f(x) - L| = |2 x + 4 - 8| = |2 x - 4| = 2|x - 2| < \epsilon $$
$$ |x-2| < \frac{\epsilon}{2}$$

$$  \delta = \frac{\epsilon}{2} \to |2 x + 4 - 8| < \epsilon $$

as desired.

(b):
$$ |f(x) - L| = |x^3 - 0| = |x^3| = |x|^3 < \epsilon $$
$$ |x| < \sqrt[3]{\epsilon}{} $$
$$ \delta = \sqrt[3]{\epsilon} \to |x^3| < \epsilon $$

as desired.

(c):
$$ |f(x) - L| = |x^3 - 8| = |(x - 2)(x^2 + 2x + 4)| = |x-2||x^2 + 2x + 4| < \epsilon $$
$$|x-2| < \frac{\epsilon}{|x^2 + 2x + 4|} $$

Suppose that we set the maximum delta at 1; then upper bound for $|x^2 + 2x + 4|$ is:

$$ |x^2 + 2x + 4| \leq |x^2| + |2x| + 4 = |x|^2 + 2|x| + 4 \leq (|c| + 1)^2 + 2(|c| + 1) + 4 =$$
$$= (2 + 1)^2 + 2(2 + 1) + 4 = 9 + 6 + 4 = 19
$$

Therefore

$$\delta = min\{1, \epsilon/19\} \to |x^3 - 8| = |x-2||x^2 + 2x + 4| < \frac{\epsilon}{19} * 19 = \epsilon $$

as desired.

(d):

$$ |[[x]] - 3| = [[0.1415926...]] = 0 < \epsilon $$

Suppose that we pick $\delta = 0.1$, then any $x \in V_{\delta}$ will satisfy $|[[x]] - 3| = 0 < \epsilon $
for any $\epsilon > 0$ as desired.

\section*{4.2.2}
\textit{Assume a particular $\delta > 0$ has been constructed as a suitable response
  to a particular $\epsilon$ challenge. Then, any larger/smaller (pick one) $\delta$ will also suffice.}

Smaller. This follows from the fact, that
$$\delta_1 < \delta_2 \to V_{\delta_1} \subset V_{\delta_2}$$

\section*{4.2.3}
\textit{Use Corollary 4.2.5 to show that each of the following limits does not exist.}

(a) $\lim_{x\to0} |x|/x$

(b) $\lim_{x\to 1} g(x)$ where $g$ is Dirichlets function from Section 4.1.

I'll not state corollary 4.2.5  function here, because it's tedious, but it'll be obvious which corollary I'm talking about by the context.


(a): let
$$(x_n) = 1/n$$
$$(y_n) = -1/n$$
then
$$(x_n) \to 0;(y_n) \to 0$$
but
$$|x_n| / x_n = 1$$
$$|y_n| / y_n = -1$$
therefore the limit does not exist.

(b):

The Dirichlet function is
\begin{equation}
D(x)=
    \begin{cases}
        1 & \text{if } x \in \textbf{Q}\\
        0 & \text{if } x \notin \textbf{Q}
    \end{cases}
\end{equation}

let
$$(x_n) = 2/n + 1$$
$$(y_n) = \sqrt{2}/n + 1$$

then

$$(x_n) \to 1;(y_n) \to 1$$

but

$$(x_n) = 2/n + 1 \in \textbf{Q}$$
$$(y_n) = \sqrt{2}/n + 1 \notin \textbf{Q}$$

therefore

$$ D(x_n) = 1$$
$$ D(y_n) = 0$$

thus the function is not continous at 1

\section*{4.2.4}
\textit{Review the definition of Thomaes function t(x) from Section 4.1.}

\textit{(a) Construct three different sequences $(x_n)$, $(y_n)$, and $(z_n)$, each of which converges to 1 without using the number 1 as a term in the sequence.}

\textit{(b) Now, compute $\lim t(x_n)$, $\lim t(y_n)$, and $\lim t(z_n)$.}

\textit{(c) Make an educated conjecture for $\lim_{x\to1} t(x)$, and use Definition 4.2.1B
  to verify the claim. Given $\epsilon > 0$, consider the set of points
$\{x \in \textbf{R} : t(x)  \epsilon\}$.  Argue that all the points in this set are isolated.}


The definition of  Thomae function is
\begin{equation}
t(x)=
    \begin{cases}
      1 & \text{if } x = 0\\
      1/n & \text {if } x = m/n \in \textbf{Q} \text{\textbackslash} \{0\} \\
      0 & \text{if } x \notin \textbf{Q}
    \end{cases}
\end{equation}

(a): Let our three sequences be

$$ (x_n) = n/(n + 1)$$
$$ (y_n) = (n + 1)/n$$
$$ (z_n) = \sum_{i=1}^{n}{\frac{1}{2^n}}$$

(b):
$$t(x_n) = \{1/2, 1/3, 1/4, 1/5, 1/6, 1/7 ...\}$$
$$t(y_n) = \{1, 1/2, 1/3, 1/4, 1/5, 1/6 ...\}$$
$$t(z_n) = \{1/2, 1/4, 1/8, 1/16 ...\}$$

(c): The educated conjecture here is that $\lim_{x \to 1} t(x) = 0$

In order to prove that conjecture author suggests, that we use $\epsilon-\delta$ definition. Let's try
it;

$$ |t(x)| < \epsilon$$

For all $\epsilon \in \textbf{R} > 0$

Therefore by archimedes property there exists a number $n \in \textbf{N}$ s.t. $\frac{1}{n} < \epsilon$.
Thus suppose that we have $\delta = 1/n$. Then our proposition is that 

$$\forall b \in (1 - 1/n; 1 + 1/n) \to |t(b)| < \epsilon$$

If $b \notin \textbf{Q} $ then $t(b) = 0$ and therefore $|t(b)| < \epsilon$; therefore we need to prove,
that any number $b = m_1/n_1 \in (1 - 1/n; 1 + 1/n) \cap \textbf{Q}$ is such, that $|t(b)| = 1/n_1 < 1/n$.
Also suppose $m_1 = n_1 + k$ (it's worth noting that in this case $k \in \textbf{Z}$); then

$$ 1 - \frac{1}{n} < \frac{m_1}{n_1} < 1 + \frac{1}{n}$$
$$ 1 - \frac{1}{n} < \frac{n_1 + k}{n_1} < 1 + \frac{1}{n}$$
$$ 1 - \frac{1}{n} < 1 + \frac{k}{n_1} < 1 + \frac{1}{n}$$
$$ - \frac{1}{n} <  \frac{k}{n_1} <  \frac{1}{n}$$
$$ |\frac{k}{n_1}| <  \frac{1}{n}$$
$$ |k||\frac{1}{n_1}| = |k||t(\frac{1}{n_1})| <  \frac{1}{n}$$

therefore because $k \in \textbf{Z}$

$$ |t(\frac{1}{n_1})| = |\frac{1}{n_1}| <  \frac{1}{n|k|} < \frac{1}{n}$$

thus for each $\epsilon > 0$ we can find a corresponding $\delta > 0$ as desired.

\section*{4.2.5}
Suppose that $\lim_{x \to c} f(x) = L$ and $\lim_{x \to c} g(x) = M$

$(ii)$ $\lim_{x \to c}[f(x) + g(x)] = L + M$

$(iii)$ $\lim_{x \to c}[f(x) g(x)] = L M$

(a)\textit{ Supply the details for how Corollary 4.2.4 part (ii) follows from the sequential criterion for functional limits in Theorem 4.2.3 and the Algebraic Limit Theorem for sequences proved in Chapter 2.}

From the algebraic limit theorem we know, that if $(a_n) \to a$ and $(b_n) \to b$ then

$$(a_n) + (b_n) = a + b$$

We also know, that for any sequence $(c_n) \to c$ it is true, that $f(c_n) \to L$ and $g(c_n) \to M$;
therefore by the algebraic limit theorem

$$f(c_n) + g(c_n) = L + M$$

for any sequence $(c_n) \to c$. Therefore we can state that

$$\lim_{x \to c}(f(x) + g(x)) = L + M $$

as desired

\textit{(b) Now, write another proof of Corollary 4.2.4 part (ii) directly from Definition 4.2.1 without using the sequential criterion in Theorem 4.2.3.}

$\lim_{x \to c} f(x) = L$ and $\lim_{x \to c} g(x) = M$; therefore for any $\epsilon_1 > 0$ we can find $\delta_1 > 0$ s.t.

$$ |x - c| < \delta_1 \to |f(x) - L| < \epsilon_1 $$

Also for the same $\epsilon_1$ there exist $\delta_2 > 0$ s.t.

$$ |x - c| < \delta_2 \to |g(x) - M| < \epsilon_1 $$

let $\delta_3 = min\{\delta_1, \delta_2\}$; then it is true that 

$$ |x - c| < \delta_3 \to |f(x) - L| < \epsilon_1 $$
$$ |x - c| < \delta_3 \to |g(x) - M| < \epsilon_1 $$

because $V_{\delta_1} \subseteq V_{\delta_3} $ and $V_{\delta_2} \subseteq V_{\delta_3} $

therefore

$$|f(x) - L| + |g(x) - M| < 2 \epsilon_1$$

Therefore 

$$ |f(x) + g(x) - L -  M| = |f(x) - L + g(x) - M| \leq |f(x) - L| + |g(x) - M| < 2 \epsilon_1 $$

Thus for any $\epsilon > 0$ there exist corresponding $\epsilon_1 = \frac{\epsilon}{2}$ for which there
exist corresponding $\delta = min\{\delta_1, \delta_2\}$ (where $\delta_1$ is a delta for $f(x)$ and
$\delta_2$ is a delta for $g(x)$) which satisfies

$$|x - c| < \delta \to |f(x) + g(x) - (L + M)| < \epsilon$$

therefore $\lim_{x \to c}(f(x) + g(x)) = L + M$ as desired.

\textit{(c) Repeat (a) and (b) for Corollary 4.2.4 part (iii).}

(a):

From the algebraic limit theorem we know, that if $(a_n) \to a$ and $(b_n) \to b$ then

$$(a_n) (b_n) = a  b$$

We also know, that for any sequence $(c_n) \to c$ it is true, that $f(c_n) \to L$ and $g(c_n) \to M$;
therefore by the algebraic limit theorem

$$f(c_n)  g(c_n) = L M$$

for any sequence $(c_n) \to c$. Therefore we can state that

$$\lim_{x \to c}(f(x)g(x)) = L M $$

as desired

(b):

$\lim_{x \to c} f(x) = L$ and $\lim_{x \to c} g(x) = M$;

In  order to prove the needed limit let's first use some algebra
$$ |f(x)g(x) - LM| = $$
$$ |f(x)g(x) + f(x)M - f(x)M - LM| = $$
$$|f(x)(g(x) - M) + M(f(x) - L)| \leq
|f(x)(g(x) - M)| + | M(f(x) - L)| = $$
$$|f(x)||g(x) - M| + |M||f(x) - L|  $$

our strategy is to show that both elements of the last sum are less or equal to $\epsilon/2$

Let $\epsilon > 0$.

$$ |M||f(x) - L| < \frac{\epsilon}{2}$$
If $M = 0$ then the abovementioned inequality always holds and we are free to choose any $\delta_1$;

Otherwise tet us pick $\delta_1$ such that inequality
$$ |f(x) - L| < \frac{\epsilon}{2|M|}$$
holds.

The next step is a little bit more complicated because we need to work with $f(x)$; let us pick y = 1;
then because $\lim_{x \to c}f(x) = L$ we know that there exists $\delta_2$ s.t. $|x - c| < \delta_2
\to |f(x) - L| < 1$. 

Therefore

$$|f(x) - L| < 1$$

Little sidenote: let's prove that 
$$ |a - b| < c \to |a| < |b| + c  $$

Firstly some preliminary stuff
$$|a - b| \geq 0 \to c > |a - b| > 0 \to c > 0$$

$$|a - b| < c \to -c < a - b < c$$
$$b -c < a  < b + c$$

Now let's see all the cases for $a, b \in \textbf{R}$

if $a \geq 0$ and $b \geq 0$ then
$$a < b + c$$
$$|a| < |b| + c$$

if $a < 0$ and $b \geq 0$ then
$$ b + c \geq 0 > a$$
$$a < b + c$$
$$|a| < |b| + c$$

if $a \geq 0$ and $b < 0$ then
$$b -c < a  < b + c$$
$$-b +c > -a  > -b - c$$
$$|b| +c > -a  > |b| - c$$
$$-|b| - c  < a  <  c - |b|$$
$$|a|  <  c - |b| \leq c + |b|$$
$$|a|  <  c + |b|$$

if $a < 0$ and $b < 0$ then
$$b -c < a  < b + c$$
$$-b + c > -a  > -b - c$$
$$|b| + c > |a|  > |b| - c$$
$$|b| + c > |a|$$
$$|a| < |b| + c$$

Therefore $\forall a,b\in \textbf{R}$
$$|a - b| < c \to |a| < |b| + c$$
as desired.

Back to our exercise: 

$$|f(x) - L| < 1$$
$$|f(x)| < |L| + 1$$

Therefore we can state that upper bound for our $|f(x)|$ with $\epsilon = 1$ is $|L| + 1$

Thus if we pick $\delta_2$ sufficient for

$$|g(x) - M| < \frac{\epsilon}{2(|L| + 1)}$$

therefore if we pick $\delta = min\{\delta_1, \delta_2\}$ then

$$|x - c| < \delta \to $$
$$|f(x)g(x) - LM| \leq |f(x)||g(x) - M| + |M||f(x) - L| <  \frac{\epsilon}{2} +
\frac{\epsilon}{2} = \epsilon  $$

therefore $\lim_{x \to c}[f(x) g(x)] = LM$ as desired

\section*{4.2.6}
\textit{Let $g: A\to \textbf{R}$ and assume that $f$ is bounded function on $A \subseteq \textbf{R}$
  (i.e. there exists $M > 0$ satisfying $|f(x| \leq M$ for all $x \in A$). Show that
  if $\lim_{x \to c}g(x) = 0$, then $\lim_{x \to c}g(x)f(x) = 0$ as well.}

Here we can't use an intuitive approach of just using algebraic limit theorem because $f(x)$ may
not hav limit at $c$.
Anyways we proceed by $\epsilon-\delta$ approach.

Therefore we need to show that
$$\exists \delta: |f(x)g(x)| < \epsilon$$

First of all,
$$ |f(x)g(x)| = |f(x)||g(x)|$$

Then we notice, that because $f(x)$ is bounded

$$\exists M \in \textbf{R} > 0: |f(x)| \leq M$$
therefore
$$|f(x)||g(x)| < |M||g(x)| = M|g(x)|$$

therefore if we pick $\delta$ sufficient for $|g(x)| < \frac{\epsilon}{M}$ then it follows that

$$|f(x)g(x)| \leq M|g(x)| < \epsilon$$

therefore
$$\forall \epsilon \in \textbf{R} \exists \delta : |x - c| < \delta \to |f(x)g(x)| < \epsilon$$

therefore

$$\lim_{x \to c}[f(x)g(x)] = 0$$
as desired.

\section*{4.2.7}
\textit{(a) The statement $\lim_{x \to 0}1/x^2 = \infty$ certainly makes intuitive sense. Construct a rigirius definition in the "challenge-response" style of Definition 4.2.1 for a limit statement of the form $\lim_{x \to c}f(x) = \infty$ and use it to prove the previous statement }

\textbf{Definition of limit to infinity}
Let $f : A \to \textbf{R}$, and let $c$ be a limit point of the domain $A$. We say that
$\lim_{x\to c} f(x) = \infty$ provided that, for all $\epsilon \in \textbf{R}$, there exists
a $\delta > 0$ such that whenever $0 < |x - c| < \delta$ (and $x \in A$) it follows
that $f(x) > \epsilon$.

Now we need to show that for  $f(x) = 1/x^2$
$$\lim_{x \to 0}f(x) = \infty$$

First
$$ f(x) > \epsilon$$
$$ \frac{1}{x^2} > \epsilon$$
$$ x^2 < \frac{1}{\epsilon}$$
$$ x < \sqrt{\frac{1}{\epsilon}}$$

therefore if we pick $\delta = \sqrt{\frac{1}{\epsilon}}$, then it follows that
$$ f(x) > \epsilon$$
as desired.

Quick (and insufficient) test in Python seems to corraborate  this statement

\textit{(b) Now construct a definition for the statement $\lim_{x \to \infty} f(x) = L$. Show
$\lim_{x \to \infty} 1/x = 0$}

\textbf{Definition of infinite limit}
Let $f : A \to \textbf{R}$, and let $c$ be a limit point of the domain $A$. We say that
$\lim_{x \to \infty} f(x) = L$ provided that, for all $\epsilon \in \textbf{R} > 0$, there exists
a $\delta$ such that whenever $x > \delta$ (and $x \in A$) it follows
that $|f(x) - c | < \epsilon$.

We start as ususal at the $\epsilon$
$$|f(x) - 0| < \epsilon$$
$$|1/x| < \epsilon$$
Given that we can pick any $\delta$ as we want, we can pick it at the very least at $0$ to get rid
of the absolute value
$$1/x < \epsilon$$
$$x > 1/\epsilon$$

therefore $\delta = 1/\epsilon$ then it follows that $$|f(x) - 0| < \epsilon$$ as desired.

\textit{(c) What would a rigorous definition for $\lim_{x \to \infty} f(x) = \infty$ would look like? Give an example of such a limit}

\textbf{Definition of infinite limit to infinity}
Let $f : A \to \textbf{R}$, and let $c$ be a limit point of the domain $A$. We say that
$\lim_{x \to \infty} f(x) = \infty$ provided that, for all $\epsilon \in \textbf{R}$, there exists
a $\delta$ such that whenever $x > \delta$ (and $x \in A$) it follows
that $f(x) > \epsilon$.

The corresponding example of such a limit  is $f(x) = x$.

\section*{4.2.8}
\textit{Assume $f(x) \geq g(x)$ for all $x$ in some set $A$ on which $f$ and $g$ are defined. Show that for any limit point $c$ of $A$ we must have }
$$\lim_{x \to c} f(x) \geq \lim_{x \to c} g(x) $$

I'm gonna do it by using contradiction; suppose that $f(x)$ and $g(x)$ are defined as in the
exercise, but
$$\lim_{x \to c} f(x) <  \lim_{x \to c} g(x) $$
% for some $c \in A$

% then
% $$\lim_{x \to c} f(x) -  \lim_{x \to c} g(x) < 0 $$

% Let $\lim_{x \to c} f(x) -  \lim_{x \to c} g(x) = M < 0$

% therefore for $\epsilon = 0 - M$ there exist $\delta$ s.t.

% $$|x - c| < \delta \to |f(x) - g(x) + M| < \epsilon = - M$$

% thus 

then it follows that there exist a sequence $(a_n) \to c$ such that $f(a_n) \geq g(a_n)$ for all
$n \in \textbf{N}$; Therefore $\lim(f(a_n)) \geq \lim(g(a_n))$ and  but it contradicts our
initial assumption.

\section*{4.2.9 (Squeeze Theorem)} Let $f,g$ and $h$ satisfy $f(x) \geq g(x) \geq h(x)$ for all
$x$ in some common domain $A$. If $\lim_{x \to c}f(x) = L$ and $\lim_{x \to c}h(x) = L$ at some
limit point $c$ of  $A$, show $\lim_{x \to c}g(x) = L$ as well

As proven in the previous exercise
$$\forall x \in A: f(x) > g(x) \to \lim_{x \to c} f(x) \geq \lim_{x \to c} g(x) $$

therefore

$$\lim_{x \to c} f(x) = L \geq \lim_{x \to c} g(x) $$
and
$$\lim_{x \to c} g(x) \geq \lim_{x \to c} h(x) = L $$
Thus 
$$ L \geq\lim_{x \to c} g(x) \geq L  $$
therefore
$$\lim_{x \to c} g(x) = L  $$
as desired.

\section*{4.3.1}
\textit{Let $g(x) = \sqrt[3]{x}$.}

\textit{(a) Prove that g is continous at c = 0}

We're gonna use $\epsilon-\delta$ definition. First of all, let's state that $g(0) = 0$. Therefore

$$|f(x) - f(c)| = |\sqrt[3]{x} - 0|  < \epsilon $$
$$ |\sqrt[3]{x}| < \epsilon $$

Here I would like to proof that $ \forall x \in \textbf{R}: |\sqrt[3]{x}| = \sqrt[3]{|x|}$: 
if $x \geq 0$ then $|\sqrt[3]{x}| = \sqrt[3]{x}= \sqrt[3]{|x|}$;
if $x < 0$ then $|\sqrt[3]{x}| = \sqrt[3]{-x}= \sqrt[3]{|x|}$. Therefore 
$$ |\sqrt[3]{x}| = \sqrt[3]{|x|} =  < \epsilon $$ is justified.

Therefore we can state that 
$$|x| =  < \epsilon ^ 3$$ 
Thus if we pick $\delta = \epsilon ^ 3$ then
$$|x - c| = |x| < \delta \to |f(x) - f(c)| = |\sqrt[3]{x} - 0| = |\sqrt[3]{x}|
= \sqrt[3]{|x|} < \sqrt[3]{\epsilon^3} =  \epsilon $$

Therefore $g$ is continous at 0

\textit{(b) Prove that $g$ is continous at a point $c \neq 0$. (The identity
  $a^3 - b^3 = (a - b)(a ^ 2 + ab + b^2)$ will be helpful)}

We're gonna use $\epsilon-\delta$ definition once again.
$$|f(x) - f(c)| = |\sqrt[3]{x} - \sqrt[3]{c}| < \epsilon$$
First, let's use a little algebra
$$|\sqrt[3]{x} - \sqrt[3]{c}| = |\sqrt[3]{x} - \sqrt[3]{c}| * 1 =
|\sqrt[3]{x} - \sqrt[3]{c}|\frac{(\sqrt[3]{x}^2 + \sqrt[3]{x}\sqrt[3]{c} +
  \sqrt[3]{c} ^ 2)}{(\sqrt[3]{x}^2 + \sqrt[3]{x}\sqrt[3]{c}
  + \sqrt[3]{c} ^ 2)} = \frac{|\sqrt[3]{x} - \sqrt[3]{c}|(\sqrt[3]{x}^2 + \sqrt[3]{x}\sqrt[3]{c} +
  \sqrt[3]{c} ^ 2)}{(\sqrt[3]{x}^2 + \sqrt[3]{x}\sqrt[3]{c}
  + \sqrt[3]{c} ^ 2)}$$

Let's look now at the sum  $\sqrt[3]{x} ^ 2 + \sqrt[3]{x}\sqrt[3]{c} +
\sqrt[3]{c}^2$: $\sqrt[3]{x} ^ 2 \geq 0 $ because it is a square. For
$\sqrt[3]{x} + \sqrt[3]{x} ^ 2$ we need to be able to articulate $\delta$ so
that both $x$ and $c$ are the same sign; if we fo that then it becomes
nonnegative. $\sqrt[3]{c} ^ 2 \geq 0 $ because it is a square

Therefore if right now we pinky-promise that we will account for unusual delta
in the future, then we are able to say that 
$$\sqrt[3]{x} ^ 2 + \sqrt[3]{x}\sqrt[3]{c} + \sqrt[3]{c}^2 \geq 0 $$
And therefore
$$\sqrt[3]{x} ^ 2 + \sqrt[3]{x}\sqrt[3]{c} + \sqrt[3]{c}^2 =
|\sqrt[3]{x} ^ 2 + \sqrt[3]{x}\sqrt[3]{c} + \sqrt[3]{c}^2| $$

Continuing with our initial algebra

$$\frac{|\sqrt[3]{x} - \sqrt[3]{c}|(\sqrt[3]{x}^2 + \sqrt[3]{x}\sqrt[3]{c} +
  \sqrt[3]{c} ^ 2)}{(\sqrt[3]{x}^2 + \sqrt[3]{x}\sqrt[3]{c}
  + \sqrt[3]{c} ^ 2)} =
\frac{|\sqrt[3]{x} - \sqrt[3]{c}||\sqrt[3]{x}^2 + \sqrt[3]{x}\sqrt[3]{c} +
  \sqrt[3]{c} ^ 2|}{(\sqrt[3]{x}^2 + \sqrt[3]{x}\sqrt[3]{c}
  + \sqrt[3]{c} ^ 2)} =
\frac{|x - c|}{(\sqrt[3]{x}^2 + \sqrt[3]{x}\sqrt[3]{c}
  + \sqrt[3]{c} ^ 2)} =
$$
$$ |x - c|\frac{1}{(\sqrt[3]{x}^2 + \sqrt[3]{x}\sqrt[3]{c}
  + \sqrt[3]{c} ^ 2)} < \epsilon
$$
As we disussed earlier $(\sqrt[3]{x}^2 + \sqrt[3]{x}\sqrt[3]{c}  +
\sqrt[3]{c} ^ 2) \geq 0$ and therefore 

$$ |x - c| < \epsilon(\sqrt[3]{x}^2 + \sqrt[3]{x}\sqrt[3]{c}
+ \sqrt[3]{c} ^ 2) $$

Thus, if we pick $\delta = min\{\epsilon(\sqrt[3]{x}^2 + \sqrt[3]{x}\sqrt[3]{c}
+ \sqrt[3]{c} ^ 2), |x - 0|\}$ (we need the second value
because we need the sum to be equal to its absolute value; ) then

$$|x - c| < \delta \to |f(x)  - f(c)| < \epsilon$$

Therefore $f(x) = \sqrt[3]{x}$ is continous on \textbf{R}.

\section*{4.3.2}

\textit{(a) Supply a proof for Theorem 4.3.9 using the $\epsilon-\delta$
  characterization of continuity.}

First, let's state the theorem

\textbf{Theorem 4.3.9 (Composition of Continuous Functions).} \textit{Given
  $f: A \to \textbf{R}$ and $g: B \to \textbf{R}$, assume that the range
  $f(A) =\{f(x): x \in A\}$ is contained in the domain $B$ so that the
  composition $g \circ f = g(f(x)) $ is well-defined on $A$. }

\textit{If $f$ is continous ac $c \in A$, and if $g$ is continous at
  $f(c) \in B$, then $g \circ f$ is continous at c.}

Firstly, the fact that both $f$ and $g$ are continous tells that

$$\forall \epsilon_1 \in \textbf{R}: \exists \delta_1:  |x - c| < \delta_1 \to
|f(x) - f(c)| < \epsilon_1$$
$$\forall \epsilon_2 \in \textbf{R}: \exists \delta_2: |x - c| < \delta_2 \to
|g(x) - g (c)| < \epsilon_2$$
And we need to prove that
$$\forall \epsilon \in \textbf{R}: \exists \delta: |x - c| < \delta \to
|g(f(x)) - g (f(c))| < \epsilon$$

% Let's pick the $c \in A$. 
% Firstly, because of all of the continuity and stuff, we are assured, that there
% exists partucular $M=g(f(c))$. Let's fix $\epsilon$. Therefore if
% we plug this $\epsilon$ into continuity of $g(x)$, then we'll get $\delta_1$.

The main strategy for this one is to plug some delta into some epsilon (or
vice versa), and get some use out of it.

Firstly, let's get some things out of the way: let us fix particular $c \in A$
and $\epsilon \in \textbf{R} > 0$. Then, let's plug this $\epsilon$ at $f(c)$
into the continuity of $g(x)$ so we can get a $\delta_g > 0$. Therefore
we will have
$$ \forall \epsilon \in \textbf{R}: \exists \delta_g: |x - f(c)| < \delta_g
\to |g(x) - g(f(c))| < \epsilon $$
which is kinda close to the thing, that we're trying to prove.

We also know that
$$ \forall \epsilon_f \in \textbf{R}: \exists \delta_f: |x - c | < \delta_f:
|f(x) - f(c)| < \epsilon_f$$
therefore it is true that
$$ \forall \epsilon \in \textbf{R}: \exists \delta_g: |y - f(c)| < \delta_g
\to |g(y) - g(f(c))| < \epsilon $$
$$ \exists \delta_f: |x - c | < \delta_f \to
|f(x) - f(c)| < \delta_g$$

From this we can state that
$$ \forall \epsilon \in \textbf{R} > 0: \exists \delta_f: |x - c | < \delta_f \to |f(x) - f(c)| < \delta_g \to
|g(f(x)) - g(f(c))| < \epsilon $$

This doesn't sound too persuasive for me, so I probably need to explore it a
little but more.

Suppose that with all the present assumptions, we get the given $\epsilon$.
If we plug it into definition of continuity for  $g(x)$ at $g(f(c))$, then
we'll get the neccesary $\delta_g$. If we plug $\delta_g$ as an $\epsilon$
for the definition of continuity of $f(x)$, then we'll get $\delta_f$.

We can probably prove it with a little bit more clarity. We need to prove that
$$\forall \epsilon \in \textbf{R}: \exists \delta: |x - f(c)| < \delta \to
|g(f(x)) - g (f(c))| < \epsilon$$

Firstly, definition of contonuity of $g(x)$ gives us the fact, that 
$$ \forall \epsilon \in \textbf{R}: \exists \delta_g: |x - f(c)| < \delta_g
\to |g(x) - g(f(c))| < \epsilon $$

then if $x \in f(A)$ then $\exists y \in A $ s.t. $f(y) = x$ therefore
$$ \forall \epsilon \in \textbf{R}: \exists \delta_g: |f(y) - f(c)| < \delta_g
\to |g(f(y)) - g(f(c))| < \epsilon $$

From the definition of continuity of $f$ we know that 

$$ \forall \epsilon_f \in \textbf{R}: \exists \delta_f: |x - c | < \delta_f:
|f(x) - f(c)| < \epsilon_f$$

Therefore 
$$\forall \epsilon \in \textbf{R}: \exists \delta: |x - f(c)| < \delta \to
|g(f(x)) - g (f(c))| < \epsilon$$
as desired.

\textit{(b) Give another proof of this theorem using the sequantial
  characterization of continuity (from Theorem 4.3.2 (iv)) }

Theorem 4.3.2 (iv) states that if $(x_n) \to c$ (with $x_n \in A$), then
$f(x_n) \to f(c)$.

Because $f(x)$ is continous we can state that for every sequence $(x_n) \to c$
it is true that $f(x_n) \to f(c)$. Therefore because $f(x_n)$ is a sequence
itself, we can state that $g(f(x_n)) \to g(f(c))$. Therefore it is true, that
for every sequence $(x_n) \to c$ it follows, that $g(f(x_n)) \to g(f(c))$.
Therefore $g(f(x))$ is continous, as desired.

\section*{4.3.3}
\textit{Using the $\epsilon-\delta$ characteriation of continuity (and tus using no previous results anbout the sequences), show that the linear function $f(x) = ax + b$ is continous at every poinnt of \textbf{R}.}

Let's start with our usual stuff

$$ |f(x) - f(c)| < \epsilon $$
$$ |ax + b - (ac + b)| = |ax + b - ac - b| = |a||x - c| < \epsilon $$
$$|x - c| < \epsilon / a $$

Therefore if we pick $\delta = \epsilon / a$ then it follows that $|f(x) - f(c)| < \epsilon$, as desired.

\section*{4.3.4}
\textit{(a) Show using Definition 4.3.1 that any function $f$ with domain
  \textbf{Z} with necessarily be continous at every point in its domain.}

Suppose that $f: Z \to R$. We need to prove that 

$$\forall \epsilon: \exists \delta: |x - c| < \delta \to |f(x) - f(c)| < \epsilon $$

Suppose that we pick $\delta = 0.1$ (or any other value, such that the only one
of the domain values will be in the needed neighborhood). Then there will be
only one number in the domain neighhborhood, and because of that we can state
that
$$|f(x) - f(c)| = |f(c) - f(c)|  = 0 < \epsilon$$

Therefore the fucntion is continous, as desired.

\textit{(b) Show in general that if $c$ is an isolated point of $A \subseteq \textbf{R}$, then $f: A \to \textbf{R}$ is continous at c.}

Because $c$ is an isolated point,  we can follow, that it is not a limit
point. Therefore there exists $\delta$ such that neighborhood
$V_\delta(c) \cap A = \{c\}$. Then it follows that
for all $\epsilon > 0$ we have $\delta > 0$ such that
$$x \in V_\delta(c) \to f(x) = f(c) \in V_\epsilon(c)$$
as desired.

\section*{4.3.5}
\textit{In Theorem 4.3.4, statement (iv) says that $f(x) / g(x)$ is continous
  at $c$ if both $f$ and $g$ are, provided that the quotent is defined. Show
  that if $g$ is continous at $c$ and $g(c) \neq 0$, then there exists an open
  interval containing $c$ on which $f(x) / g(x)$ is always defined.}

Suppose that $f(x) / g(x)$ is continous at $c$. Then it follows, that for any
$\epsilon > 0$ there exists $\delta > 0$ such that
$$ x \in V_\delta \to f(x)/g(x) \in V_\epsilon$$

Thus, $f(x)/g(x)$ is defined on open interval $V_\delta$.

\section*{4.3.6}
\textit{(a) Reffering to the proper theorems, give a formal argument that
  Dirichlet's function from Section 4.1 is nowhere-continous at $R$.}

Let $c \in R$ and $\epsilon = 0.5$

Suppose now that $\delta > 0$. Then it follows, that there exists both
$z_1 \in I$ and $z_2 \in Q$ such that
$$c - \delta < z_1 < c + \delta$$
$$c - \delta < z_2 < c + \delta$$
therefore exists $f(z_1) = 0$ and $f(z_0) = 1$.
Therefore for any $c \in R$ there does not exist a $\delta$, such that
$$x \in V_\delta(c) \to f(x) \in V_\epsilon(f(c))$$. Therefore Dirichlet's
function is not continous at any point in $R$.

\textit{(b) Review the definition of Thomae's function in Section 4.1 and
  demonstrate that it fails to be continous at every rational point.}

Thomae's function is defined as 
\begin{equation}
  t(x)=
  \begin{cases}
    1 & \text{if } x = 0\\
    1/n & \text {if } x = m/n \in \textbf{Q} \text{\textbackslash} \{0\} \\
    0 & \text{if } x \notin \textbf{Q}
  \end{cases}
\end{equation}

Suppose now that $c \in Q$. Then it follows that if we set $\epsilon = f(c)$.
For every $\delta > 0$ there exists $z \in I$ such that $z \in V_\delta$.
Therefore, for $\epsilon = f(c)$ $f(z) = 0$. Therefore
$$|f(c) - f(x)| < \epsilon$$
$$- \epsilon < f(x) - f(c) < \epsilon$$
$$- \epsilon < f(x) - f(c) < f(c)$$
$$- \epsilon < f(x) - f(c) < f(c)$$
$$f(x) - f(c) < f(c)$$
$$f(x) < 0$$
$$0  < 0$$
Which is false. Therefore for every $c \in Q$ there exist a $\epsilon$,
such that there is no $delta > 0$ for which it is true that
$$x \in V_\delta \to f(x) \in V_\delta(f(c))$$

\textit{(c) Use the characterization of continuity in Theorem 4.3.2 (iii) to
  show that Thomae's function is continous at every irrational point in
  \textbf{R}. (Given $\epsilon > 0$, consider the set of points
  $\{x \in R: t(x) \geq \epsilon\}$. Argue that all the points in this set
  are isolated )}

Let $\epsilon > 0$ and $z \in I$. Therefore we can state, that there exists
$N \in \textbf{N}$ such that $1/n < \epsilon$. Now let us look at
$V_{1/n}(z)$. 
There exists a finite amount of natural numbers, that
are less then $N$. If there exist natural numbers $n_1 < n$, such
that there exists $m_1 \in Z$ fow which $m_1 / n_1 \in V_{1/n}(z)$, then pick
one, for which $|m_1 / n_1 - z|$ is the lowest and let $\delta
= |m_1 / n_1 - z|$.
If such numbers do not exist, pick $\delta = 1/n$.
That insures, that any rational number in $V_b(z)$ has the property,
that if we put it as $m_1/n_1$, then $n_1 > n$.

Then it follows, that for any  $\epsilon > 0$ there exists $\delta > 0$
such that $x \in V_\delta \to f(x) \in V_\epsilon(t(z))$ (if $x \in I$, then
$x = 0 < \epsilon$; if $x \in Q$, then $f(x) < 1/n < \epsilon$ because of
how had
we constructed $\delta$ in the previous paragraph). Therefore Thomae's function
is continous at any irrational point.

In the text of the exercise we were asked to consider a specific set and
all that other stuff,
and that method would certainly work, but I consider this particular method
to be a little more straightforward.

\section*{4.3.7}
\textit{Assume $h: R \to R$ is continous on $R$ and let $K = \{x: h(x) = 0\}$.
  Show that $K$ is a closed set.}

Suppose that $K$ is not closed. Then it follows, that there exists a
limit point of $K$, that is not in $K$. Let $y \in R$ be a limit point
of $K$, that is not in $K$. It follows from equivalence of limit point,
that there exists a sequence
$((x_n)) \to y$, such that $x_n \in K$ and $x_n \neq y$ for all
$n \in N$.

$y \not \in K$ implies that $h(y) \neq 0$. Thus
for every
sequence $(x_n) \to y$ it is true that  $(h(x_n)) \to h(y) $. Because
$x_n \in K$ it follows
that $h(x_n) = 0$. Thus $\lim 0 = h(y) \neq 0$, which
is a contradiction. Therefore $K$ is closed.

\section*{4.3.8}
\textit{(a) Show that if a function is continous on all of \textbf{R} and
  equal to 0 at every rational point, then it nust be identically 0 on all
  of \textbf{R}}

We are going to prove this one by contradiction. Suppose that there exists a
point $x \in \textbf{R}$ such that $f(x) \neq 0$. Then let
$\epsilon = |f(x)|$. Then it follows, because of the continuity of $f$ on $R$,
that for every $\delta \in \textbf{R}$
it is true that there exists a rational point $q$ in $V_\delta(x)$.
Thus
$$f(x) - \epsilon < f(q) < f(x) + \epsilon$$
$$f(x) - \epsilon < 0 < f(x) + \epsilon$$
$$f(x) - |f(x)| < 0 < f(x) + |f(x)$$
If $f(x) \geq 0$ then $f(x) - |f(x)| = f(x) - f(x) = 0$.
If $f(x) < 0$ then $f(x) + |f(x)| = f(x) - f(x) = 0$.
Thus, $0 < 0$, which is a contradiction.

Therefore $f$ cannot be continous at $R$ if $f(x) = 0$ at every rational point
and there exists $f(x) \neq 0$. Thus only possibility when $f(x)$ is
continous under those circumstances is if $f(x) = 0$ for every $x \in R$.

\textit{(b) If $f$ and $g$ are defined on all of $R$ and $f(r) = g(r)$ at
  every rational  point, must $f$ and $g$ be the same functions? }

Only if both  $f$ and $g$ are continous. Also, it doesn't have to be for
rationals only, it also can be applied to any dense set in $R$.

\section*{4.3.9 (Contraction Mapping Theorem)}
\textit{Let $f$ be a function defined on all of $R$, and assume there is a
  constant $c$ such that $0 < c < 1$ and}
$$|f(x) - f(y)| \leq c|x - y|$$
\textit{for all $x,y \in R$.}

\textit{(a) Show that $f$ is continous on $R$.}

Fix $y \in R$ then pick $\epsilon > 0$.
Then it follows, that $1/c \epsilon > 0$. Let $\delta = 1/c \epsilon$
Therefore for every  $x \in V_\delta$ it follows that
$$|x - y| < \delta$$
$$|x - y| < 1/c \epsilon$$
$$c |x - y| <  \epsilon$$
Thus it follows, that
$$|f(x) - f(y)| < c |x - y| <  \epsilon$$
Thus for every $\epsilon > 0$ there exists $\delta > 0$ such that
$$x \in V_\delta \to f(x) \in V_\epsilon(f(x))$$
Thus $f$ is continous at $y$ . Thus for every $y \in R$ $f$ is continous.
Thus, $f$ is continous at $R$, as desired.

\textit{(b) Pick some point $y_1 \in R$ and construct the sequence }
$$(y_1, f(y_1), f(f(y_1)), ... )$$
\textit{In general, if $y_{n + 1} = f(y_n)$, show that
  the resulting sequence $(y_n)$
  is a Cauchy sequence. Hence we may let $y = \lim y_n$.}

% $$|y_{n + 2} - y_{n + 1}| \leq c|y_{n + 1} - y_n|$$

% $$|y_{n + 1} - y_n| \leq c|y_n - y_{n - 1}|$$
% $$c|y_{n + 1} - y_n| \leq c^2|y_n - y_{n - 1}|$$
% $$|y_{n + 2} - y_{n + 1}| \leq c^2|y_n - y_{n - 1}|$$
% $$|y_{n + 2} - y_{n + 1}| + |y_{n + 1} - y_{n}| \leq
% |y_{n + 2} - y_{n + 1} + y_{n + 1} - y_{n}| =
% |y_{n + 2} - - y_{n}|
% $$



% $$|y_m - y_n| < \epsilon$$


Let us firstly observe that
$$|y_{n + 2} - y_{n + 1}| \leq c|y_{n + 1} - y_n|$$
implies that 
$$|y_3 - y_1| = |y_3 + y_2 - y_2 - y_1| = |y_3 - y_2 + y_2 - y_1| \leq $$
$$ \leq  |y_3 - y_2| + |y_2 - y_1| \leq$$
$$\leq c|y_2 - y_1| + |y_2 - y_1| =
(c + 1) |y_2 - y_1| = (c + c^0) |y_2 - y_1|$$

and in general for $m > n \in N$ 
$$|y_m - y_n| = 
|y_m - y_{m - 1} + y_{m + 1} - y_{m - 2} + y_{m + 2} ...  - y_n| \leq$$
$$ \leq |y_m - y_{m - 1}| + |y_{m + 1} - y_{m - 2}| + ... +
|y_{n + 1}  - y_n| \leq $$
$$ \leq c^{m - n - 1}|y_{n + 1} - y_{n - 1}| +
c^{m - n - 2}|y_{n + 1} - y_{n}| + ... +
|y_{n + 1}  - y_n| \leq  $$
$$ \leq c^{m  - 1}|y_{2} - y_{1}| +
c^{m - 2}|y_{2} - y_{1}| + ... +
c^n|y_{2}  - y_1| =  $$
$$\left(\sum_{j = n}^{m - 1}c^j\right)|y_2 - y_1|$$

We know, that for $0 < c < 1$ the sum
$$\sum_{j = 1}^{\infty}c^j$$
converges to some limit. Because $c > 0$ it follows that
$$\left(\sum_{j = n}^{m - 1}c^j\right)
\leq
\left(\sum_{j = n}^{\infty}c^j\right)$$
and thus
$$|y_m - y_n| \leq \left(\sum_{j = n}^{m - 1}c^j\right)|y_2 - y_1|
\leq \left(\sum_{j = n}^{\infty}c^j\right) |y_2 - y_1|$$

Also, because the series is convergent, 
$$\lim_{n \to \infty} \left(\sum_{j = n}^{\infty}c^j\right) \to 0$$
That statement finishes preliminaries for our proof, so let us begin

Let $\epsilon > 0$. Let us also calculate first $y_1$ and $y_2$.
Now let $N$ be such a number, that it satisfies 
$$\left(\sum_{j = N}^{\infty}c^j\right) < \frac{\epsilon}{|y_2 - y_1|}$$
thus for every $n \geq N$ it is true that
$$\left(\sum_{j = n}^{\infty}c^j\right) < \frac{\epsilon}{|y_2 - y_1|}$$
$$\left(\sum_{j = n}^{\infty}c^j\right)|y_2 - y_1| < \epsilon$$

Therefore we can state that for every $m > n > N \in \textbf{N}$ it follows
that 
$$|y_m - y_n| \leq 
\left(\sum_{j = n}^{\infty}c^j\right)|y_2 - y_1| < \epsilon$$
% |y_m - y_{m - 1} + y_{m + 1} - y_{m - 2} + y_{m + 2} ...  - y_n| \leq$$
% $$ \leq |y_m - y_{m - 1}| + |y_{m + 1} - y_{m - 2}| + ... +
% |y_{n + 1}  - y_n| \leq $$
% $$ \leq c^{m - n - 1}|y_{n + 1} - y_{n - 1}| +
% c^{m - n - 2}|y_{n + 1} - y_{n}| + ... +
% |y_{n + 1}  - y_n| \leq  $$
% $$ \leq c^{m  - 1}|y_{2} - y_{1}| +
% c^{m - 2}|y_{2} - y_{1}| + ... +
% c^n|y_{2}  - y_1| =  $$
% $$\left(\sum_{j = n}^{m - 1}c^j\right)|y_2 - y_1| \leq

Therefore $(y_n)$ is a Cauchy sequence, as desired.

\textit{(c) Prove that $y$ is a fixed point of $f$ (i.e., $f(y) = y$) and that
  it is unique in this regard}

Just to remind myself, I'll state here that we have  defined $y$ in
the text of previous exercise.

Because $(y_n) \to y$, it follows that $(f(y_n)) \to f(y)$.
$$(f(y_n)) = \{f(y_1), f(y_2), ... \} = \{y_2, y_3, ... \}$$
Therefore $(f(y_n))$ is a subseqeunce of $(y_n)$. Therefore
it converges to $y$ as well. Therefore $f(y) = y$, as desired.

Suppose that $f(y) \neq y$. Then let $b = |f(y) - y|$. Then it follows that


Suppose that there exists  $f(y_1) = y_1$, $f(y_2) = y_2$ and $y_1 \neq y_2$.
Then it follows that 
$$|f(y_1) - f(y_2)| \leq c|y_1 - y_2|$$
$$|y_1 - y_2| \leq c|y_1 - y_2|$$
$$1 \leq c$$
which is a contradiction.

\textit{(d) Finally, prove that if $x$ is any arbitrary point in $R$ then the
  sequence $(x, f(x), f(f(x)), ...)$ converges to $y$ defined in (b).}

Suppose that $x \in R$. then it follows, that
$(x, f(x), f(f(x)), ... )$ converses to some $z \neq y \in R$. Then
it follows, that $f(z) = z$ by the same reasoning as in (c). Thus
$$|f(z) - f(y)| \leq c|z - y|$$
$$|z - y| \leq c|z - y|$$
$$1 \leq c$$
which is a contradiction. Therefore the only number, to which $z$ is convergent
is $0$ (contradiction does not arive when $z = y$, because $|z - y| = 0$,
thus the equation becomes $0 = 0$).

\section*{4.3.10}
\textit{Let $f$ be a function defined of all of $R$ that satisfies the additive
  condition $f(x + y) = f(x) + f(y)$ for arr $x, y \in R$.}

\textit{(a) Show that $f(0) = 0$ and that $f(-x) = -f(x)$ for all $x \in R$ }

$$f(0) = f(0 + 0) = f(0) + f(0)$$
$$f(0) = f(0) + f(0)$$
$$0  = f(0)$$

$$f(x) + f(-x) = f(x - x)  = f(0) = 0$$
$$f(x) + f(-x)  = 0$$
$$ f(-x)  = - f(x)$$

\textit{(b) Show that if $f$ is continous at $x = 0$, then $f$ is continous
  at every pont in \textbf{R}}

Let $f$ be continous at $0$. Therefore we can state that
$$\epsilon > 0: \exists \delta > 0: |x - 0| < \delta \to |f(x) - f(0)| < \epsilon$$
$$\epsilon > 0: \exists \delta > 0: |x - 0| < \delta \to |f(x) - 0| < \epsilon$$
$$\epsilon > 0: \exists \delta > 0: |x + c - c| < \delta \to |f(x)| < \epsilon$$

Let $c \in R$. Let $\epsilon > 0$. Then
$$\epsilon > 0: \exists \delta > 0: |x + 0| < \delta \to |f(x) + 0| < \epsilon$$
$$\epsilon > 0: \exists \delta > 0: |x + c - c| < \delta \to |f(x) + f(c) - f(c)| < \epsilon$$
$$\epsilon > 0: \exists \delta > 0: |x + c - c| < \delta \to |f(x) + f(c) - f(c)| < \epsilon$$
$$\epsilon > 0: \exists \delta > 0: |(x + c) - c| < \delta \to |f(x + c) - f(c)| < \epsilon$$


Thus for any $y \in R$ we can set it as $y = c + x$. Thus, $f$ is continous
at $y \in R$.

\textit{(c) Let $k = f(1)$. Show that $f(n) = kn$ for all $n \in N$, and
  then prove that $f(z) = kz$ for all $z \in Z$. }

Suppose that $k = f(1)$. Then it follows that
$$k + k = f(1) + f(1) = f(1 + 1) = f(2)$$
Thus
$$kn = \sum_{j = 1}^{n} k = \sum_{j = 1}^{n} f(1) =
f\left(\sum_{j = 1}^{n} 1\right) = f(n)$$
for all $n \in \textbf{N}$.

$f(-x) = -f(x)$. Thus if $z < 0 \in Z$ then
$$kz = \sum_{j = 1}^{-z} -k = \sum_{j = 1}^{-z} f(-1) =
f\left(\sum_{j = 1}^{-z} -1\right) = f(z)$$
thus $f(z) = kz$ for all $z \in Z$ (taking into account positive values of $N$)

Let $r \in Q$. Then ther exists $n/m = r$ such that $n \in N$ and $m \in Z$.
Thus
$$r = n/m = \sum_{j = 1}^{n} 1/m$$
We know, that $f(m) = km$ for $m \in Z$. Suppose that $m > 0$. Then
$$f(1) = k$$
$$f\left(\sum_{j = 1}^m \frac{1}{m}\right) = k$$
$$\sum_{j = 1}^m f\left(\frac{1}{m}\right) = k$$
$$mf\left(\frac{1}{m}\right) = k$$
$$f\left(\frac{1}{m}\right) = \frac{k}{m}$$
If $m < 0$ then
$$f(1) = k$$
$$f\left(\sum_{j = 1}^{-m} \frac{1}{m}\right) = k$$
$$\sum_{j = 1}^{-m} f\left(\frac{1}{m}\right) = k$$
$$mf\left(\frac{1}{m}\right) = k$$
$$f\left(\frac{1}{m}\right) = \frac{k}{m}$$
Thus, for $m \in Z \setminus \{0\}$,
$$f(1/m) = \frac{k}{m}$$

Therefore
$$f(r) = f\left(\frac{n}{m}\right) = f\left(\sum_{j = 1}^n\frac{1}{m}\right) =
\sum_{j = 1}^nf\left(\frac{1}{m}\right) = n f\left(\frac{1}{m}\right) =
n \frac{k}{m} = k \frac{n}{m} = kr$$
as desired.

\textit{(d) Use (b) and (c) to conclude that $f(x) = kx$ for all $x \in R$.
  Thus, any additive function that is continous at $x = 0$ must neecssarily
  be a linear function through origin}

From continuity of $f$ on $R$ it follows, that it is continous on $Q$.
Let $h(x) = kx$ for all $x \in R$. 
Then let us define  $g(x) = f(x) - h(x)$. It follows that $g(x) = 0$ for all
$x \in Q$. Also, because of the continuity of both $f$ and $h$ on $R$ it
follows, that $g$ is continous on $R$.
Thus, from out discussion in Exercise 4.3.8 we can conclude,
that $g(x) = 0$ for all $x \in R$. Thus, $f(x) = h(x)$ for all $x \in R$.
Therefore $f(x) = kx$ for all $x \in R$, as desired.

\section*{4.3.11}
\textit{For each of the following choices of $A$, construct a function
  $f: R \to R$ that has discontinuities at every point $x \in A$ and is
  continous on $A^c$}

\textit{(a) $A = Z$}
$$f(x) = [[x]]$$
where $[[]]$ is the floor function.

\textit{(b) $A = \{x: 0 < x < 1\}$}
\begin{equation}
  f(x) =
  \begin{cases}
    0 \text{ if } x \in (-\infty, 0) \cup (1, \infty) \cup Q \\
    |x| \text{ if } x \in I \cap [0, 0.5] \\
    |x - 1| \text{ if } x \in I \cap (0.5, 1)
  \end{cases}
\end{equation}
(i.e. its a modified Dirichlet's function, that has sort of a diamond shape
in $[0, 1]$)

\textit{(c) $A = \{x: 0 \leq x \leq 1\}$}
\begin{equation}
  f(x) =
  \begin{cases}
    0 \text{ if } x \in (-\infty, 0) \cup (1, \infty) \cup Q \\
    1 \text{ if } x \in I \cap [0, 0.1] \\
  \end{cases}
\end{equation}
(same idea as in (b), but no diamond shape here, just the partial Dirichlet's
and partial 0)

\textit{(d) $A = \{1/n: n \in N\}$}
\begin{equation}
  f(x) =
  \begin{cases}
    0 \text{ if } x \in A \\
    x \text{ if } x \in A^c \\
  \end{cases}
\end{equation}
(here we took into account 0, it is convergent from every direction)

\section*{4.3.12}
\textit{Let C be the Cantor set constructed in Section 3.1. Define
$g: [0, 1] \to R$ by}
\begin{equation}
  g(x) =
  \begin{cases}
    1 \text{ if } x \in C \\
    0 \text{ if } x \not \in C \\
  \end{cases}
\end{equation}
\textit{(a) Show that $g$ fails to be continous at any point $c \in C$}

Suppose that $c \in C$ and let $\epsilon = 1$. Then it follows, that
for every $\delta$-neighborhood $V_\delta(c)$ there would exist
$x \in V_\delta(c)$ such that $x \notin C$ (this follows from our discussion
from previous chapter that $C$ is totally disconnected. Exercise 3.4.9
contains rigorous proof for validity of my last statement).
Therefore
$$|g(x) - 1| < \epsilon$$
$$|0 - 1| < 1$$
$$1 < 1$$
for every $\delta > 0$. Thus for some $\epsilon$ there does not exist
$\delta > 0$, that satisfies continuity constraint. Therefore
$C$ is not continous at every $c \in C$.

\textit{(b) Prove that $g$ is continous at every point $c \notin C$}

Suppose that $c \notin C$. Then $c \in C^c$. We know (see chapter 3), that
$C$ is a closed set. Thus $C^c$ is an open set.
$[0, 1] = (0, 1) \cup \{0, 1\}$ and $\{0, 1\}  \subseteq C$, therefore
$\{0, 1\}  \not \subseteq C^c$. Thus
$$[0, 1] \cap C^c = ((0, 1) \cup \{0, 1\}) \cap C^c =$$
$$= ((0, 1) \cap C^c) \cup (\{0, 1\}) \cap C^c) =
((0, 1) \cap C^c) \cup \emptyset 
= (0, 1) \cap C^c$$.
Therefore the set, for every element of which $g(x) = 0$, is a union of open sets, and therefore is 
itself an open set. Thus, for every $c \in C^c$ and every
$\epsilon > 0$ we can find a $V_\delta(c) \subseteq C^c \cap [0, 1]$ (see
definition of open sets) such that
$$|g(x) - g(c)| = |0 - 0| = 0 < \epsilon$$
for every $\epsilon > 0$. Therefore $g(x)$ is continous at every point
$c \notin C$, as desired.


\section*{Preliminaries and notes for text of 4.4}

One thing, that was buggging me is how $x \in (c - 1, c + 1)$ implies that
$|x| < c + 1$. Here's the proof of it.
$$x \in (c - 1, c + 1)$$
$$c - 1 < x < c + 1$$
$$- 1 < x - c <  1$$
$$|x - c| <  1$$
$$||x| - |c|| \leq |x - c| < 1 $$
$$||x| - |c||  < 1 $$
$$-1 < |x| - |c|  < 1 $$
$$-1 + |c| < |x| < 1 + |c|$$

\section*{4.4.1}

\textit{(a) Show that $f(x) = x^{3}$ is continuous on all of \textbf{R}.}

In order to show, that $f$ is continous we need to show, that $\forall
\epsilon \in \textbf{R}$ $\exists \delta$ s.t.
$$|x - c| < \delta \to |f(x) - f(c)| < \epsilon$$

Let's rewrite the first formula

$$ |f(x) - f(c)| = |x^{3} - c^{3}| = |(x - c)(x^{2} + cx + c^{2})| =
|x - c||x^{2} + cx + c^{2}|$$

We can put $|x - c|$ can be as small as we want it to be. Therefore we need
an upper bound for $|x^{2} + cx + c^{2}|$.

$$|x^{2} + cx + c^{2}| \leq |x^{2}| + |cx| + |c^{2}| \leq (|c| + 1)^{2} +
|c|(|c| + 1) + |c|^{2}$$



Therefore if we take
$\delta = min\{1, \epsilon/((|c| + 1)^{2} + |c|(|c| + 1) + |c|^{2})\}$
then
$$|x^3 - c^3| = |x-c||x^2 + cx + c^2| \leq \epsilon \frac{((|c| + 1)^{2} +
  |c|(|c| + 1) + |c|^{2}) }{ ((|c| + 1)^{2} + |c|(|c| + 1) + |c|^{2}})
= \epsilon$$

Therefore $f(x) = x ^3$ is continous on \textbf{R}.

\textit{(b) Argue, using Theorem 4.4.6, that f is not uniformly continuous
  on \textbf{R}}


\textbf{Theorem 4.4.6 (Sequential Criterion for Nonuniform Continuity).} A
function $f:A \to \textbf{R}$ fails to be uniformly continuous on $A$ if
$\exists \epsilon > 0 $ and  two sequences $(x_n)$ and $(y_n)$ in $A$
satisfying

$|x_n - y_n| \to 0$ $but$ $|f(x_n) - f(y_n) \leq \epsilon_0$

In order to show that $f(x) = x^3$ is not uniformly continous on
\textbf{R} let us use sequences

$$x_n = n$$
$$y_n = (n + 1/n)$$

Firstly
$$ |x_n - y_n| = |n - n - 1/n| = |-1/n| = 1/n \to 0$$

on the other hand

$$|f(x_n) - f(y_n)| = |n ^ 3 - (n + 1/n) ^ 3| = |n^3 - (n^3 + 3 \frac{n^2}{n}
+ 3 \frac{n}{n^2} + \frac{1}{n^3})| = $$
$$ = |-3n - \frac{3}{n} - \frac{1}{n^3} | \leq |3n| \to \infty$$

rmaxima seems to eraborate this statement, therefore  $|x_n - y_n| \to 0$
but $|f(x_n) - f(y_n) \to \infty$

Therefore $f(x) = x^3$ is not uniformly continous on \textbf{R}.

\textit{(c) Show that $f$ is uniformly continuous on any bounded subset
  of \textbf{R}.}

Suppose that $A \subset \textbf{R}$ and $\exists M \in \textbf{R}$ s.t.
$\forall x \in A$ $x \leq M$ (i.e. $A$ is bounded $M$)

Then, $\forall c \in A$ and $\forall \epsilon \in \textbf{R}$
$$\frac{\epsilon}{((|M| + 1)^{2} + |M|(|M| + 1) + |M|^2}  \leq
\frac{\epsilon}{((|c| + 1)^{2} + |c|(|c| + 1) + |c|^2} $$

Therefore if we take
$$\delta = min\{1, \frac{\epsilon}{((|M| + 1)^{2} + |M|(|M| + 1) + |M|^2}\} $$
then $|x - c| < \delta$ implies, that $ |f(x) - f(c)| < \epsilon$, therefore
making $f(x)$ uniformly continous by definition

There is also another way to show that $f$ is uniformly continous:
if set $A$ is bounded, then $\overline A$ is closed and bounded and therefore
compact. Therefore $f$ is uniformly continous at $\overline A$. Therefore
it is continous at $A$, as desired.

The last statement in previous paragraph can be justified by proof by
contradiction and using 4.4.6

\section*{4.4.2}
\textit{Show that $f(x) = 1/x^3$  is uniformly continous on the set
  $[1, \infty)$, but is not on the set $(0, 1]$}

In order to show, that $f(x)$ is continous on the set $[1, \infty)$ let us
first prove that it is just continous, with the hope that $\delta$ is not
depentant on $x$

$$ |\frac{1}{x^3} - \frac{1}{c^3}| = |\frac{c^3 - x^3}{x^3c^3}| =
|\frac{(c - x)(x^2 + cx + c^2)}{x^3c^3}|
= |(c - x) \frac{x^2 + cx + c^2}{x^3c^3}| =
|c - x||\frac{x^2 + cx + c^2}{x^3c^3}| =$$
$$ = |x - c||\frac{x^2 + cx + c^2}{x^3c^3}| $$

Therefore we need to show that if $\delta$ is bounded above at 1, then
$|\frac{x^2 + cx + c^2}{x^3c^3}|$ is bounded above at $[1, \infty)$ by
some constant, but  $(0, 1]$ isn't.

$$ |\frac{x^2 + cx + c^2}{x^3c^3}| = |\frac{1}{c^3 x} + \frac{1}{c^2x^2}
+ \frac{1}{cx^3}| \leq
|\frac{1}{c^3 x}| + |\frac{1}{c^2x^2}| + |\frac{1}{cx^3}|
$$

for $x \in [1, \infty)$ each of those fractions are bounded above by 1,
therefore for $x \in [1, \infty)$

$$ |\frac{x^2 + cx + c^2}{x^3c^3}| \leq 3 $$

therefore if we pick $\delta < \epsilon / 3 $ then it follows, that
$|f(x) - f(c)| < \epsilon$ for $x \in [1, \infty)$

on the other hand,

$$ \lim_{x \to 0}(|\frac{x^2 + cx + c^2}{x^3c^3}|) \to \infty  $$

Therefore we will need smaller deltas as we approach 0; to put it more
concretely let's use the theorem for
\textbf{Sequential Criterion for Nonuniform Continuity}.

Let us pick
$$x_n = 1/n$$
$$y_n = 1/(n + 1)$$

then

$$|x_n - y_n| = |\frac{1}{n} - \frac{1}{n + 1}| = |\frac{n + 1 - n}{n(n + 1)}|
= |\frac{1}{n ^ 2 + 1}| \to 0$$

but

$$ |f(x_n) - f(y_n)| = |1/(\frac{1}{n})^3 - 1/(\frac{1}{n + 1})^3| = |1/(\frac{1}{n^3}) - 1/(\frac{1}{(n + 1)^3})| =  |n^3 - (n + 1)^3| =  $$
$$ = |n^3 - (n ^ 3 + 3 n^2 + 3 n + 1)| = |3n^2 + 3n + 1| \to \infty$$

therefore by \textbf{4.4.6} $f(x)$ is not uniformly continous on $(0, 1]$, as desired

\section*{4.4.3}
\textit{Furnish the details (including an argument for Exercise 3.3.1 if it is not already done) for the proof of the Extreme Value Theorem (Theorem 4.4.3).}

Let me restate here exercise 3.3.1

\textit{Show that if $K$ is compact, then $\sup K$ and $\inf K$ both exist
  and are elements of $K$}

If $K$ is compact, then it is bounded (i.e. bounded above and below), and
thus have supremum and infinum.

Now let us take into account the fact, that for every $\epsilon > 0$ it is
true, that there exist elements $k_1$ and $k_2$ (with a possibility
that $k_1 = k_2$) such that

$$k_1 > \sup K - \epsilon$$
$$k_2 < \inf K + \epsilon$$
thus
$$\sup K - k_1 <  \epsilon$$
$$k_2 - \inf K < \epsilon$$
because of the properties of supremum and infinum $\sup K - k_1 \geq  0$ and
$k_2 - \inf K \geq 0$. Thus
$$|\sup K - k_1| < \epsilon$$
$$|k_2 - \inf K| < \epsilon$$

Therefore we can state, that every neighborhood around supremum and infinum
has a member of $K$ in it. Thus $\sup K$ and $\inf  K$ are limit points of $K$.
Thus, because $K$ is closed, we can conclude that $\sup K \in K$ and
$\inf K \in K$, as desired.

Now back to our exerice. Let us firstly state EVT here

\textbf{Theorem 4.4.3 (Extreme Value Theorem)}
\textit{If $f: K \to R$ is continous on a compact set $K \subseteq R$, then
  $f$ attains a maximum and minimum value. In other words, there exists
  $x_0, x_1 \in K$ such that $f(x_0) \leq f(x) \leq f(x_1)$ for all $x \in K$.}

First of all, because $K$ is compact we can state that $f(K)$ is compact
as well ($f(K)$ in this sense is the range of $f$ on $K$). Therefore
there exist $\sup f(K) \in f(K)$ and  $\inf f(K) \in f(K)$.
For both supremum and infinum therefore there exists at least one of
$x_1 \in K$ such that $f(x_1) = \sup f(K)$ and $x_2 \in K$
such that $f(x_2) = \inf f(K)$. Now let $y \in K$. It follows that
$$\inf f(K) \leq f(y) \leq \sup f(K) $$
$$f(x_2) \leq f(y) \leq f(x_1) $$
as desired.

\section*{4.4.4}
\textit{Show that if $f$ is continous on $[a, b]$ with $f(x) > 0$ for all
  $a \leq x \leq b$, then $1/f$ is bounded on $[a, b]$.}

Firstly, $[a, b]$ is a closed set. Therefore $f([a, b])$ is closed set as well.
By virtue of the fact, that $f(x) > 0$ we can state, that $f(x) \neq 0$.
Therefore function $g \circ f = 1/f(x)$ is defined on all $f([a, b])$.
Therefore $g(f([a, b]))$ is compact. Therefore it is bounded, as desired.

\section*{4.4.5}
\textit{Using the advice that follows Theorem 4.4.6, provide a complete
  proof for criterion for noninform continuity}

Let us firstly state the theorem itself here

\textbf{Theorem 4.4.6 (Sequential Criterion for Nonuniform Continuity).}
\textit{A function $f: A \to R$ fails to be uniformly continous on $A$ if
  there exists a particular $\epsilon_0 > 0$ and two sequences $(x_n)$ and
  $(y_n)$ in $A$ satisfying}
$$|x_n - y_n| \to 0 \text{ but } |f(x_n) - f(y_n)| \geq \epsilon_0$$.

Now let us take an advice from the text of proof and take a negation of
definition of uniform continuity.

\textit{A function $f:A \to R$ is not uniformly
 continous on $A$ if there exists $\epsilon > 0$ such that
 for every $\delta > 0$ there exist  $|x - y| < \delta$ for which it 
 it is true  that  $|f(x) - f(y)| \geq \epsilon$.}

Therefire in order for function to fail to be uniformly continous on $A$ we
need an $epsilon$, for which there whould be no suitable $\delta$ for all
$x, y \in A$

Now fuppose that for some $f: A \to R$ there exist two sequences $(x_n)$ and
$(y_n)$ such that
$$|x_n - y_n| \to 0$$
and there exists $\epsilon_0 > 0$ such that 
$$|f(x_n) - f(y_n)| \geq \epsilon_0$$.

Then it follows from convergence of $(|x_n - y_n|)$ that
for every $\delta > 0$ there exists
$N \in \textbf{N}$ such that $n \geq N$ implies
that $|x_n - y_n| < \delta$. Thus, for given $\epsilon$ there does not
exits $\delta > 0$, such that $|x - y| < \delta$ and
$|f(x) - f(y)| < \epsilon$. Therefore $f$ does not converge uniformly on
$A$.

I do not have a clue on why do we need to consider $\delta_n = 1/n$. Maybe I've
skipped something, but it doesn't look like it.

Update: found out, that there is an error in the theorem's definition. It
is supposed to state that
\textit{A function $f: A \to R$ fails to be uniformly continous on $A$ if
  and only if
  there exists a particular $\epsilon_0 > 0$ and two sequences $(x_n)$ and
  $(y_n)$ in $A$ satisfying}
And therefore we have equivalence instead of implication.

In sight of new developments it is imperative then to prove the
theorem in other deirection.

Because $f$ is not uniformly continous on $R$, there exists $\epsilon_0 > 0$
such that for every $\delta > 0$ there exists $|x - y| < \delta$ such that
$|f(x) - f(y)| \geq \epsilon_0$. Therefore let us have a sequence
$\delta_n = 1/n$. Then it follows, that there exists $x_n, y_n$ for which it
is true that
$$|x_n - y_n| < \delta_n$$
and
$$|f(x_n) - f(y_n)| < \delta_n$$

Therefore we have two sequences $(x_n)$ and $(y_n)$ with desired properties.


\section*{4.4.6}
\textit{Give an example of each of the following, or state that such a request is impossible. For any that are impossible, supply a short explanation (perhaps referencing the appropriate theorem(s)) for why this is the case.}

\textit{(a) A continous function $f: (0, 1) \to R$ and a Cauchy sequence
  $(x_n)$ such that $f(x_n)$ is not a Cauchy sequence;}
$$f = 1 / x - 1$$
$$(x_n) = 1 - 1/n$$

\textit{(b) A continous function $f: [0, 1] \to R$ and a Cauchy sequence
  $(x_n)$ such that $f(x_n)$ is not a Cauchy sequence;}

Impossible. Because $[0, 1]$ is a closed set it follows that a limit
to which $(x_n)$ converges  is in $[0, 1]$ (because the limit of a given
sequence is a limit point). Therefore for 
$(x_n) \to c$ it follows that $f(x_n) \to f(c)$ by Characterization of
Continuiity (iv).

\textit{(c) A continous function $f: [0, \infty) \to R$ and a Cauchy sequence
  $(x_n)$ such that $f(x_n)$ is not a Cauchy sequence;}

Impossible. Same reason as in (b).

\textit{(d) A continous bounded function $f$ on $(0, 1)$ that attains a
  maximum value on this open interval but not a minimum value.}

$$f(x) = -|x - 0.5| + 0.5$$

\section*{4.4.7}
\textit{Assume that $g$ is defined on an open interval $(a, c)$ and it is
  known to be uniformly continous on $(a, b]$ and $[b, c)$, where
  $a < b < c$. Prove that $g$ is uniformly continous on $(a, c)$.}

Suppose that $g$ is not uniformly continous. Then it follows, that
there exist $\epsilon_0 > 0$ and
two sequences $(x_n)$ and $(y_n)$ such that
$$|x_n - y_n| \to 0 \text{ but } |f(x_n) - f(y_n)| \geq \epsilon_0$$

Let us pick $a < a_1 < b$ and $b < c_1 < c$. Then it follows that
$g$ is contninous at $[a_1, b] \subset (a, b] $ and $[b, c_1] \subset [b, c)$,
both of which are compact. Therefore it is also uniformly continous at
$[a_1, c_1]$, which is also compact.

Then it follows that there exists $\delta_1, \delta_2, \delta_3$ for
$(a, b], [a_1, c_1], [b, c)$ for which it follows that
$$|x - y| < \delta_n \to |f(x) - f(y)| < \epsilon_0$$
for $n \in \{1, 2, 3\}$.

Let $\delta = \min\{\delta_1, \delta_2, \delta_3, |a_1 - b|, |c_1 - b|\}$.
Then it follows, that there exist $x_j, y_j$ from original sequences
such that $|x_j - y_j| < \delta$ and 
$$|f(x_j) - f(y_j)| \geq \epsilon_0$$
but $|x_j - y_j| < \delta$ implies that $x_j, y_j$ are in one of the
intervals $(a, b], [a_1, c_1], [b, c)$, therefore 
$$|f(x_j) - f(y_j)| < \epsilon_0$$
which is a contradiction.

\section*{4.4.8}
Firsty I should state that in order to confirm my previous exercise I looked
up an answer to it online. Their proof was not simular to mine, it employed
triangular inequality, but the main stuff is the same. This exercise is
simular at times to the previous one, so I employ proof with triangular
inequality here.

\textit{(a) Assume that $f: [0, \infty) \to R$ is continous at every point in
  its domain. Show that if there exists $b > 0$ such that $f$ is uniformly
  continous on the set $[b, \infty)$, then $f$ is uniformly continous on
  $[0, \infty)$.}

Because $f$ is continous in $[0, \infty)$ and $[0, b]$ is a
compact set we can state that
it is uniformly continous at $[0, b]$

Therefore let $\delta_1, \delta_2$ be responses for $\epsilon / 2$ for
sets $[0, b]$ and $[b, \infty)$ respectively. If $x, y \in [0, \infty)$ are
both in $[0, b]$ or $[b, \infty)$, then
$$|f(x) - f(y)| < \epsilon / 2 < \epsilon$$
If one of them (let it be $x$) is in $[0, b]$ and another is in $[b, \infty)$,
then it follows that
$$|f(x) - f(y)| = |f(x) - f(b) + f(b) - f(y)| \leq
|f(x) - f(b)| + |f(b) - f(y)| < \epsilon/2 + \epsilon/2 = \epsilon$$
as desired.

\textit{(b) Prove that $f(x) = \sqrt{x}$ is uniformly continous on
  $[0, \infty)$}
I've got a feeling, that we need to employ somehow part (a) here, but I don't
see the connection.

$$|\sqrt{x} - \sqrt{c}| =
|\sqrt{x} - \sqrt{c}|\left(\frac{\sqrt{x} + \sqrt{c}}{\sqrt{x} +
    \sqrt{c}}\right) = 
\frac{|x - c|}{\sqrt{x} + \sqrt{c}} \leq  \frac{|x - c|}{\sqrt{c}}  $$

Let $f(x) = \sqrt{x}$. Then it follows that $f$ is uniformly continous at
$[0, 1]$. For $[1, \infty)$ it follows that
$$\frac{|x - c|}{\sqrt{c}} \leq |x - c|$$
Therefore if we set $\delta = \epsilon$ then
$$|\sqrt{x} - \sqrt{c}| \leq |x - c| < \epsilon$$
therefore it is uniformly convergent at $[1, \infty)$.
Therefore, using part (a), we can concluse that $f$ is uniformly continous at
$[0, \infty)$, as desired.

\section*{4.4.9}
\textit{A function $f: A \to R$ is called Lipschitz if there exists a bound
  $M > 0$ such that }
$$\left|\frac{f(x) - f(y)}{x - y}\right| \leq M$$
for all $x, y \in A$. Geometrically speaking, a function $f$ is Lipshitz if
there is a uniform  bound on the magnitude of the slopes of lines drawn
through any two point of the graph of $f$.

\textit{(a) Show that if $f: A \to R$ is Lipshitz, then it is uniformly
  continous on $A$. }

Suppose that $f$ is Lipschitz. Then it follows that
$$\left|\frac{f(x) - f(y)}{x - y}\right| \leq M$$
$$\frac{|f(x) - f(y)|}{|x - y|} \leq M$$
$$|f(x) - f(y)| \leq M|x - y|$$
Thus if we set $\delta = \epsilon/M$, then it follows that
$$|x - y| < \epsilon/M$$
$$M|x - y| < \epsilon$$
$$|f(x) - f(y)| \leq M|x - y| < \epsilon$$
Therefore any Lipschitz is uniformly continous.

\textit{(b) Is the converse true? Are all uniformly continous functions
  necessatily Lipschitz?}

First feeling is that it is not. Because if uniform continuity is equivalent
to being Lipschitz, then that would appear earlier and would be massively more
important.

In order to create a more concrete proof then just
"it's too good to be true", we need to come up with counterexample.

For $f(x) = \sqrt{x}$ we have 

$$\left|\frac{\sqrt{x} - \sqrt{y}}{x - y}\right| =
\frac{|\sqrt{x} - \sqrt{y}|}{|x - y|} =
\frac{|\sqrt{x} - \sqrt{y}|}{|\sqrt{x} - \sqrt{y}||\sqrt{x} + \sqrt{y}|} =
\frac{1}{|\sqrt{x} - \sqrt{y}|}$$
which is unbound. Therefore not all uniform functions are Lipschitz.

\section*{4.4.10}
\textit{Do uniforly continous functions preserve boudedness? If $f$ is
  uniformly continous on a bounded set $A$, is $f(A)$ necessarily bounded?}

I want to say yes on this one. In order to make it concrete let's look at
the definition.

Suppose that the function is not bounded on $A$. Then
let $\epsilon_0 > 0$. 
Now let us pick $f(j_0)$ for some $j \in A$.
Then pick $f(j_{n + 1}) > f(j_n) + \epsilon_0$ for the case if the function is
not bound above and $f(j_{n + 1}) < f(j_n) - \epsilon_0$ for the case if the
function is not bound below. It follows then
that $|f(j_{m}) - f(j_n)| > \epsilon_0$ for all $m \neq n \in N$.
% Thus we'll have a sequence $\{f(x_1), f(x_2), ...\}$.

Now let us look at the sequence $(j_n)$. Each of them will be in $A$, and
therefore we can state that the sequence is bounded.
By Bolzano-Weierstrass theorem we'll have a convergent 
subsequence $(j_{n_k})$. Then pick two subsequences of $(j_{n_k})$ which
we'll call $(x_n)$ and $(y_n)$ such that $x_n \neq y_n$ (for example pick
$(x_n)$ to be odd elements and $(y_n)$ be even elements)
Because they are subsequences of convergent
sequence it follows that they converge to the same limit. Thus,
$(x_n - y_n)$ converges to 0. Therefore $(|x_n - y_n|) \to 0$. Thus there
exist $\epsilon_0$ and  two sequences $(x_n)$ and $(y_n)$ such that
$$|x_n - y_n| \to 0 \text{ and } |f(x_n) - f(y_n)| \geq \epsilon_0$$
therefore $f$ is not uniformly convergent.

Therefore if $A$ is convergent and $f$ is uniformly continous on $A$, then
$f(A)$ is bounded as well.

\section*{4.4.11 (Topological Characterization of continuity)}
\textit{Let $g$ be defined on all of $R$. If $A$ is a subset of $R$, define the
  set $g^{-1}(A)$ by }
$$g^{-1}(A) = \{x \in R: g(x) \in A\}$$
\textit{Show that $g$ is continous if and only if $g^{-1}(O)$ is open whenever
  $O \subseteq R$ is an open set.}

\textbf{In one direction: }
Supppose that $g$ is continous. Now let $O \subseteq R$ be an open subset of
$R$. Let $x \in g^{-1}(O)$. It follows then that $g(x) \in O$. Because
$O$ is open there exists $V_\epsilon(g(x)) \subseteq O$. Because $g$ is
continus on $R$ there exists $V_\delta(x)$ such that
$$y \in V_\delta(x) \to g(y) \in V_\epsilon(g(x)) \to
g(y) \in O \to y \in g^{-1}(O)$$
thus
$$V_\delta(x) \subseteq  g^{-1}(O)$$
for all $x \in g^{-1}(O)$.

Therefore for every $x \in g^{-1}(O)$ there exists a neighborhood
$V_\epsilon(x)$ such that $V_\epsilon \subseteq g^{-1}(O)$. Therefore
for any continous $g$ it follows that $g^{-1}(O)$ is an open set  whenever
$O$ is an open set, as desired.

\textbf{In another direction: }
Suppose that whenever $O$ is an open set it follows that $g^{-1}(O)$ is an
open set as well. Let us pick some $\epsilon > 0$. It follows that for
every $x \in R$
$$g^{-1}(V_\epsilon(g(x)))$$
is an open set. Because $g(x) \in V_\epsilon(g(x))$ we can state that
$x \in g^{-1}(V_\epsilon(g(x)))$. Therefore there exists a neighborhood
$V_\delta(x) \subseteq g^{-1}(V_\epsilon(g(x)))$. Thus, for every $x \in R$ and
all $\epsilon > 0$ there exists $\delta > 0$ such that
$$x \in V_\delta(x) \to x \in V_\epsilon(g(x))$$

Therefore $g$ is continous on $R$, as desired.

\section*{4.4.12}
\textit{Conctruct an alternate proof of Theorem 4.4.8 using the open cover
  characterization of compactness from Theorem 3.3.8 (iii)}

Theorem 3.3.8 (iii) states that a set is compact if and only if any open conver
for $K$ has a finite subcover

Theorem 4.4.8 states that a function is continous on a compact set $K$ is
uniformly continous on $K$.

Because $K$ is compact it follows that $f(K)$ is compact as well.

Suppose that $f$ is not uniformly continous. It follows then that there exist
two sequences $(x_n)$ and $(y_n)$ in $K$ such that
$$|x_n - y_n| \to 0 \text{ but } |f(x_n) - f(y_n)| \geq \epsilon_0$$

Let $\epsilon = \epsilon_0 / 3$. Then let us define open cover
$$\{(y - \epsilon, y + \epsilon): y \in f(K)\}$$
It follows that $f(K)$ is covered by finite subsover. Let
us call sets, that compose this finite subcover $\{I_1, I_2, ... I_n\}$.
Then it follows that at least one of those sets has infinite amount of
elements of sequence $(f(x_n))$ (otherwise the amount of elements in the
sequence is finite). Let us call the set in which there is
infinite amount of elements $I_m$, and subsequence, that is contained in this
set $(f(x_{n_j}))$. Because $|f(x_n) - f(y_n)| \geq \epsilon_0 > \epsilon$
we can state that there are no elements $(f(y_{n_j}))$ in $I_m$. Thus
let us pick a set $I_l$ such that it has infinite amount of $(f(y_{n_j}))$,
and call the subsequence that is contained in $I_l$ $(f(y_{n_l}))$.
For this sequence find corresponding $(f(x_{n_l})) \subseteq (f(x_{n_j}))$.

After that let $x_m$ be a number, such that
$$I_m = (f(x_m) - \epsilon, f(x_m) + \epsilon)$$
define $x_l$ in the same order for $I_l$. Then it follows (by continuity of
$f$ on $K$), that there exist $\delta_m$ and $\delta_l$, that correspond
to $x_m$ and $x_l$ for given $\epsilon$. Thus, it is true that
$$x_{n_l} \in V_{\delta_l}(x_l)$$
and
$$y_{n_l} \in V_{\delta_l}(x_m)$$

We know that $(|x_{n_l} - y_{n -l}|) \to 0$. By BW there exist convergent
subsequences in $x_n$, $y_n$. Therefore limit of those subsequences are equal
Therefore
$$\overline{V_{\delta_l}(x_l)} \cap \overline{V_{\delta_l}(x_m)} \neq
\emptyset$$
Let $x_v \in \overline{V_{\delta_l}(x_l)} \cap \overline{V_{\delta_l}(x_m)}$.
Then we'll have
$x_n, y_n \in V_\delta(x)$ for every $\delta > 0$. Therefore
for every $\delta > 0$ it follows that $\epsilon > epsilon_0$. Therefore
$f$ is not continous at $x_v$, which is a contradiction. 

This proof is FUBAR. It's probably wrong, and it took me too much time.
I found the correct one it the internet, and I thought about the way
they've done it there before I wrote this one, but failed to recognize
it as a valid strategy and threw that idea away.


\section*{4.4.13}

\textit{(a) Show that a uniformly continous function preserves Cauchy
  sequences; that is, if $f: A \to R$ is uniformly continous and
  $(x_n) \subseteq A$ is a Cauchy sequence, then show $f(x_n)$ is a
  Cauchy sequence}

Suppose that $(x_n)$ is a Cauchy sequence and $f$ is a uniformly continous function.

Let $\epsilon > 0$. Then there exists $\delta > 0$ such that whenever
$|x - y| < \delta$ it follows that $|f(x) - f(y)| < \epsilon$.
Because $(x_n)$ is Cauchy there exists 
 $N \in \textbf{N}$ such that whenever
$m,n \geq N$ it follows that
$$|x_m - x_n| < \delta$$
Thus $|f(x_m) - f(x_n)| < \epsilon$ for all $m, n \geq N$. Therefore
$(f(x_n))$ is a Cauchy sequence.

\textit{(b) Let $g$ be a continous function on the open interval $(a, b)$.
  Prove that $g$ is uniformly continous on $(a, b)$ if and only if it is
  possible to define values $g(a)$ and $g(b)$ at the endpoints so that the
  extended function $g$ is continous on $[a, b]$.}

\textbf{In one direction: }

Suppose that $g$ is a uniformly continous function defined on $(a, b)$.

Let us define $g(a) = \lim(g(x_n))$ and $g(b) = \lim(g(y_n))$
for some sequences $(x_n) \to a$ and $(y_n) \to b$.

% Let $\epsilon > 0$. Then there exists $N \in \textbf{N}$ for sequence
% $(f(x_n))$ such that
% $$n \to N \to f(x_n) \in V_\epsilon(f(a))$$.
% Because $x_n$ is convergent it follows that it is also bounded. Thus
% there exists $\delta > 0$ such that $x_n \in V_\delta(a)$ for some
% $n \geq N$.

Suppose now that there exists a sequence $(z_n) \to a$ that is contained in
$(a, b)$ , but
$(f(z_n))$ does not converge to $f(a)$. Because $(z_n)$ is Cauchy it follows
that $(f(z_n))$ is also Cauchy and therefore it converges to some limit
$a_1 \neq a$. Then let $\epsilon = |a_1 - a| / 3$. It follows then that
there exists $N_1 \in \textbf{N}$ such that
$$n_1 \geq N_1 \to |f(z_{n_1}) - a_1| < \epsilon$$

For the same $\epsilon$ there exists $N_2$ such that 
$$n_2 \geq N_2 \to |f(x_{n_1}) - f(a)| < \epsilon$$
for our original sequence, that is convergent to $f(a)$.
Let $N = \max\{N_1, N_2\}$. It follows then that
$$n \geq N \to |f(z_n) - a_1| < \epsilon$$
$$n \geq N \to |f(x_n) - f(a)| < \epsilon$$

Because $\epsilon = |f(a) - a_1| / 3$ it  follows that there exist
two sequences such that 
$$|f(x_n) - f(z_n)| \geq |f(a) - a_1| / 3$$
(I still think that this particular implication is correct, but cannot
prove it rigorously)

Also, $(x_n) \to a$ and $(z_n) \to a$, which means that
$(x_n - z_n) \to 0$. Therefore we can state that there exist two sequences in
$(a, b)$ such that 
$$\lim|x_n - z_n| = 0 \text{ but } |f(x_n) - f(z_n)| \geq \epsilon_0$$
where $\epsilon_0 = |a - a_1| / 3$. Therefore the function is not uniformly
continous at $(a, b)$.

It follows then for every sequence $(x_n) \to a$ it follows that
$(f(x_n)) \to f(a)$. Therefore, by Characterization of Continuity it follows
that $f$ is continous at $a$.

Same reasoning can be applied to $f(b)$, therefore $f$ is continous at
$f(b)$ as well.

Therefore $f$ is continous at an interval $[a, b]$, which is a compact set.
Therefore $f$ is uniformly continous at $[a, b]$, as desired.


\textbf{In other direction: }
This case is trivial, so I'll be short.

Suppose that it is possible to define values $g(a)$ and $g(b)$ such that
$g$ is continous at $[a, b]$. It follows then that $g$ is uniformly continous
at $[a, b]$. Therefore for any $\epsilon > 0$ there exists $\delta > 0$
such that for any $x, y \in (a, b) \subseteq [a, b]$
it follows that
$$|x - y| < \delta \to |f(x) - f(y)| < \epsilon$$
therefore the function is uniformly continous on $(a, b)$.

I still have some reservation abous some concreteness of implication for
one of the implication in part (a), specifically with
the problem that if $a \neq b \in R$ and $p > 2$  then it follows that
for all $a_1$ and $b_1$ that satisfies
$$|a - a_1| < \frac{|a - b|}{p}$$
and
$$|b - b_1| < \frac{|a - b|}{p}$$
it follows that.
$$|a_1 - b_1| \geq |a - b| - (2 \frac{|a - b|}{p})$$


Right now I don't feel like pondering on that problem, therefore I'll
mark this paragraph with TODO and return to it later

\section*{4.5.1}
\textit{Show how the Itermediate Value Theorem follows as a corollary
  to Theorem 4.5.2}

Theorem 4.5.2 states that continous functions preserve connected sets.
Therefore for $f: [a, b] \to  R$ it follows that
$f([a, b])$ is connected. We know, that a set is connected if and only if
for any $a < c < b$ where $a, b$ it follows that $c$ is in this set as well.
Therefore for some number $f(a) < L < f(b)$ (or $f(a) > L > f(b)$) it follows
that $L \in f([a, b])$. Therefore there exists $x \in [a, b]$ such that
$f(x) = L$, as desired.

\section*{4.5.2}
\textit{Decide the validity of the following conjectures.}

\textit{(a) Continous functions take bounded open intervals to bounded open
  intervals }

False. $f(x) = x^2$ on $(-1, 1)$ is $(1, 0]$, which is not open.

\textit{(b) Continous functions take bounded open intervals to open sets }

False. For the same reason as in (a).

\textit{(c) Continous functions take bounded closed intervals to bounded
  closed intervals.}

True. Continous functions preserve compact sets and preserve connected
sets, therefore they preserve closed intervals.

\section*{4.5.3}
\textit{Is there a continous function on all of $R$ with range $f(R)$ equal
  to $Q$?}

No. This follows from dencity of $Q$ in $R$. For two numbers $a > b \in Q$
there exists $c \in I$ such that $a > c > b$, therefore 
$c = f(x)$ for some $x \in R$, but $x \in R \to f(x) \in Q$. Therefore
$c \notin f(R)$. But $f$ is continous on $R$, therefore $c \in f(R)$, which is
a contradiction.

\section*{4.5.4}
\textit{A function $f$ is increasing on $A$ if $f(x) \leq f(y)$ for all
  $x < y$ in $A$. Show that the IVT does have a converse if we assume $f$ is
  increasing on $[a, b]$.}

We need to show that if a function is increasing on $[a, b]$
and for every  $L$  between $f(x)$ and $f(y)$ it is always possible to find
a point $c \in (x, y)$, where $f(c) = L$, then the function is continous
on $[a, b]$.

Firstly we need to prove some preliminary things.
Suppose that for some connected set $E$ there exists $x \in E$ such
that for some $\epsilon > 0$ there does not exist $y \neq x \in E$ such
that $|y - x| < \epsilon$. Then $x$ is an isolated point of $E$.
Therefore it is not a limit point of $E$.
Therefore $E$ can be divided into $E \setminus \{x\}$ and $\{x\}$, which are
separated. Therefore the set is disconnected. Therefore we have a
contradiction. Thus we can state that if $E$ is connected,
then for every $\epsilon > 0$ it follows that 
$$V_\epsilon(x) \cap E \neq \emptyset$$

Also, we know that constant functions are continous. Thus for
any noncontinous functions, that are not defined on singletons
or on emptysets it follows that ranges of those functions have
at least two numbers, that are not equal to each other.

Now back to our business:

Suppose that $x \in [a, b]$.

Let us look at $y_1 < y_2 \in f([a, b])$. Because of the IVP
it follows that there
exists $L \in f([a, b])$ such that $ y_1 < L < y_2$. Therefore
$f([a, b])$ is connected.


% We need to show that there exists $\delta > 0$ such
% that $|x - y| < \delta \to |f(x) - f(y)| < \epsilon$

Let $\epsilon > 0$.
Thus we can state that there exists some $f(y) \neq f(x)$ such that
$|f(x) - f(y)| < \epsilon$. Let $f(x_1) = \sup{V_\epsilon(f(x))}$
and $f(x_2) = \inf{V_\epsilon(f(x))}$. It follows that
$f(x_1) \geq f(x_2)$. Therefore $x_1 \geq x_2$. Therefore by IVP of $f$ 
it follows that
$$l \in (x_1, x_2) \to f(l) \in (f(x_1), f(x_2)) \subseteq V_\epsilon(f(x))$$
Because $x \in (x_1, x_2)$ and $(x_1, x_2)$ is an open set it follows that
there exists $\delta > 0$ such that neighborhood
$V_\delta(x) \subseteq (x_1, x_2)$. Therefore we can state that for every
$c \in [a, b]$ and
for every $\epsilon > 0$ there exists $\delta > 0$ such that
$$x \in V_\delta(c)  \to f(x) \in V_\epsilon(f(c))$$
Therefore the function is continous at $[a, b]$.

\section*{4.5.5}
\textit{Finish the proof of the IVT using the AoC started previously}

Let us consider a case $f(a) < 0 < f(b)$. (Same idea holds for any other
number other than 0, but the notation will be messy)

Let
$$K = \{x \in [a, b]: f(x) \leq 0\}$$

$K$ is bounded above by $b$, and $a \in K$, so $K$ is not empty. Therefore
by AoC there exists $c = \sup{K}$

Suppose that $f(c) < 0$ or $f(c) > 0$. Then it follows, that there exists
$\epsilon = |f(c)| / 2$. By continuity of $f$ it follows that there exists
$\delta > 0$ such that
$$|x - c| < \delta \to |f(x) - f(c)| < \epsilon$$

Thus for some $x_1 > c$ and $x_2 < c$. Then it follows, that there exists
$\delta > 0$ such that
$$|x - c| < \delta \to |f(x) - f(c)| < \epsilon$$


Let us look at the case $f(c) < 0$. It follows, that $f(c) < 0 < f(b)$.
Therefore $f(c) \neq f(b)$. Therefore $c \neq b$. Then it follows, that
$c \in [a, b)$. 

Because $a \in [a, c]$ and $f(a)$ is the only number such that $f(a) \leq 0$.
By assumptions  of our exercise it follows that $f(a) < 0$.
Let $\epsilon = |a|$. It follwos then that there exists $V_\delta(a)$
such that
$$x \in V_\delta(a) \to f(x) \in V_\epsilon(f(a))$$
Because $a$ is a limit point of $[a, c]$ it follows that
$$V_\delta(a) \cap (a, b] \neq \emptyset$$
Therefore let $a_1 \in V_\delta(a) \cap (a, b]$ . It follows then
that
$$f(a_1) \in V_\epsilon(f(a))$$
$$|f(a_1) - f(a) | < |f(a) |$$
$$-|f(a)| < f(a_1) - f(a) < |f(a)|$$
$f(a) \leq 0 \to |f(a)| = -f(a)$. Therefore
$$f(a) < f(a_1) - f(a) < - f(a)$$
$$f(a_1) < 0 $$
Therefore $a$ is not the only number for which $f(a) < 0$. Also, because
$f(a_1) \in (a, b]$ it follows that $a_1 > a$. Therefore
$a \neq \sup{K}$. 

Therefore $a < c < b$. Thus, let $\epsilon = |f(c)|$. It follows that
there exists $\delta_1 > 0$ such that 
$$|x - c| < \delta_1 \to |f(x) - f(c)| < \epsilon$$
Because $a < c < b \to c \in (a, b)$, there exists $\delta_2 > 0$ such that
$V_{\delta_2}(c) \subseteq (a, b)$. Therefore let
$\delta = \min\{\delta_1, \delta_2\}$. It follows then that
$$|x - c| < \delta \to |f(x) - f(c)| < \epsilon$$
Therefore there exist numbers $c_1 < c < c_2$ such that
$f(c_1) < 0$ and $f(c_2) < 0$.
Therefore we can't state that $c < 0$, because there exists $c_2 > c$ such that
$f(c_2) < 0$, and therefore $c_2 \in K$. Thus $c$ is not a lower bound for
$K$. If $c > 0$, then there exists
$c_1 < c$ such that $f(c_2) > 0$ and for any number between $c$ and $c_2$ it
follows, that $f(x) > 0$. Therefore $c$ is now the lowest bound of $K$ and
therefore is not a supremum.

Thus the only viable option left is to $f(c) = 0$. Therefore if
$f: [a, b] \to R$ is continous and $f(a) < 0 < f(b)$, then there
must exist $c \in (a, b)$ such that $f(c) = 0$. Same argument can be
applied to any number other then $0$, therefore any continous
function has IVP, as desired.


\section*{4.5.6}
\textit{Finish the proof of the Intermediate Value Theorem using the Nested Interval Property started previously.}

Consider a case when $L = 0$. 
Suppose that there exists $f$ is continous at $[a, b]$. Then let
$I_0 = [a, b]$. Then take a point $z_1 = (a + b)/2$. If $f(z_1) > 0$ then
let $I_1 = [a, z]$. Otherwise let $I_1 = [z, b]$. In general for
$I_n = [a_n, b_n]$ pick $z_n = (a_n + b_n) / 2$. If $f(z_n) > 0$
let $I_{n + 1} = [a_n, z]$. Otherwise let $I_{n + 1} = [z, b_n]$.
It followss then that $\lim |I_n| = 0$ and $I_{n + 1} \subseteq I_n$, thus
they are nested.

Because of the NIP we know, that
$$\cap_{n = 0}^{\infty} I_n \neq \emptyset$$
Therefore we can let $c \in \cap_{n = 0}^{\infty} I_n$.

Let us consider a sequence $(x_n)$ where $x_n \in I_n$. Then
for every $\epsilon > 0$ there exists $N \in \textbf{N}$  such that
$n \geq N \to
x_n \in I_n \subset V_\epsilon(c)$ by virtue of the fact that
$\lim|I_n| = 0$ and that intervals are nested. Thus this sequence converge to $c$.

Now let us look at $f(c)$. If $f(c) > 0$, then let $\epsilon = |f(c)|$.
It follows that ther exists $\delta> 0$ such that
$$|x - c| < \delta \to |f(x) - f(c)| < \epsilon$$
It follow then that there exists neighborhood around $c$ for which
there exists $I_n$ such that both bounds of $I_n$ are more than $0$, which
is a contradiction.
If $f(c) < 0$, then for the same $\epsilon = |f(c)|$ we'll have a contradiction
that both bounds are lower than $0$, which is also a contradiction (for a more
rigorous approach with equations with absolute values and all that
meaty stuff goto previous exercise)

Therefore the only option which is left is $f(c) = 0$, as desired. 

\section*{4.5.7}
\textit{Let $f$ be a continous function on the closed interval $[0, 1]$ with
  range also contained in $[0, 1]$. Prove that $f$ must have a fixed point;
  that is, show $f(x) = x$ for at least one value of $x \in [0, 1]$.}

We are going to proceed with a proof by contradiction.
Suppose $f: [0, 1] \to R$ is
continous at $[0, 1]$, $[0, 1] \subseteq f([0, 1])$ and for all 
$x \in [0, 1]$ it follows that $f(x) \neq x$. Then let 
$g(x) = f(x) - x$.
It follows that for all $x \in [0, 1]$ $g(x) \neq 0$.
Because $f(x) \neq x$ it follows that $f(0) \neq 0$ and $f(1) \neq 1$.
Because $[0, 1] \subseteq f([0, 1])$ it follows that there exist
$f(x_1) = 0$ and $f(x_2) = 1$ for $x_1, x_2 \in (0, 1)$. Therefore
there exist $g(x_1) = 0 - x$ and $g(x_2) = 1 - x_2$. Because $x \in (0, 1)$
it follows that $g(x_1) < 0$ and $g(x_2) > 0$. Thus, by IVT, there
exists $g(x) = 0$, for which it follows that $f(x) = x$, which is a
contradiction. (Holy moly, that was surprisingly fast)

By the way, same reasoning applies not only to $[0, 1]$, but (at least) for
any closed and bounded interval in $R$.

\section*{4.5.8}
\textit{Imagine a clock where the hour hand and the minute hand are
  indistinguishable from each other.
  Assuming the hands move continuously around
  the face of the clock, and assuming their positions can be measured with
  perfect accuracy, is it always possible to determine the time?
}

No, it isn't

12 o'clock is an obvious case when the hands meet, and this is possible
to tell time from it (assuming that we are talking about 12-hour time periods).

Suppose now that we are looking at the distance between two hours (i.e.
between 1 and 2 or some other such interval).
Now for given interval there exist 12 small intervals, such that if
an hour hand is in one of those 12 intervals, then a minute hand is in a
given interval (e.g. 1/12th of any hour interval right after a  number,
for interval between 12 and 1). Thus, let us put the hour hand
in a given interval. It follows, that a minute hand will go through all of the
12 intervals on the clock at some time. Some of those 12 intervals are
guaranteed to be not in the same hour, as the hour hand. Thus, let us pick
such an interval. Now if we represent this interval as $[0, 1]$, then
$x$ will represent the position of hour hand if the minute hand is
in the interval, in which we have hour number. Because minute hand will pass
through this interval we can state that $[0, 1] \subset f([0, 1])$, where
$f$ is the position of minute clock. Thus, there will exist a point at
which $f(x) = x$, and therefore the hour and minute clock will be
indistinguishable.

This proof is a bit sloppy in the language department, but to my credit
I can say, that it's pretty hard to explain positions and rotation and whatnot
of different things without using pictures and in general.


\section*{4.6.1}
\textit{Using modifications of therse functions, construct a function
  $f: R \to R$ so that }

\textit{(a) $D_f = Z$}

Actulaly, floor function will do, but I'll do another one, simular to
Dirichlet's as well

\begin{equation}
  f(x) =
  \begin{cases}
    1 \text{ if } x \in Z \\
    0 \text{ otherwise}
  \end{cases}
\end{equation}

\textit{(b) $D_f = \{x: 0 < x \leq 1\}$}

\begin{equation}
  f(x) =
  \begin{cases}
    0 \text{ if } x \in (-\infty, 0) \cup (Q \cap [0, 1]) \\
    x \text { otherwise}
  \end{cases}
\end{equation}

\section*{4.6.2}
\textit{State a similar definition for the left-hand limit}
$$ \lim_{x \to c^-}{f(x)} = L$$

Given a limit point $c$ of a set $A$ and a function $f: A \to R$, we
write
$$ \lim_{x \to c^-}{f(x)} = L$$
if for all $\epsilon > 0$ there exists a $\delta > 0$ such that
$f(x) - L < \epsilon$ whenever $- \delta   < x - c < 0$.

\section*{4.6.3}
\textit{Supply a proof for this proposition}

We are talking here about Theorem 4.6.3, which reads as follows:

\textbf{Theorem 4.6.3}
\textit{Given $f: A \to R$ and a limit point $c$ of $A$,
  $\lim_{x \to c}{f(x)} = L$ if and only if }
$$ \lim_{x \to c^+}{f(x)} = L \text{ and } \lim_{x \to c^-}{f(x)} = L$$

\textbf{In forward direction:}

This case is kind of trivial: suppose that $\lim_{x \to c} f(x) = L$.
It follows that for every $\epsilon > 0$ there exists $\delta > 0$ such that
$$0 < |x - c| < \delta \to |f(x) - L| < \epsilon$$
from this it follows that
$$0 < x - c < \delta \to |f(x) - L| < \epsilon$$
and
$$-\delta > x - c < 0 \to |f(x) - L| < \epsilon$$
therefore $\lim_{x \to c^{+}}{f(x)} = \lim_{x \to c^{-}}{f(x)} = L$,
as desired.

\textbf{In backward direction:}

Suppose that $\lim_{x \to c^{+}}{f(x)} = \lim_{x \to c^{-}}{f(x)} = L$
Now suppose that $(x_n) \to c$ is a convergent sequence, such that
$x_n \in A$ and $x_n \neq c$. This follows that there exists
and infinite subsequence in either  $A \cap (-\infty, c)$ or
$A \cap (c, \infty)$. Because it is a subsequence of convergent
sequence, it follows that it is convergent to $c$. Now it is trivial to show
that for a this subsequence and therefore original sequence in
$A \setminus \{c\}$ it follows that $\lim f(x) \to L$.


Also, another way to show the same is:
$$0 < x - c < \delta  \to |f(x) - L| < \epsilon $$
$$0 > x - c > -\delta \to |f(x) - L| < \epsilon$$
therefore 
$$0 < |x - c| < \delta \to |f(x) - L| < \epsilon$$
as desired.

\section*{4.6.4}
\textit{Let $f: R \to R$ be increasing. Prove that $\lim_{x \to c^{+}{f(x)}}$
  and $\lim_{x \to c^{-}{f(x)}}$ mush exist at every point $c \in R$. Argue
  that the only type of discontinuity a monotone function can have is a
  jump discontinuity }

Suppose that a function is increasing. That means that for every  $x \geq y$
it is true that  $f(x) \geq f(y)$. 

Let $c \in R$. It is true that $x < c \to f(x) \leq f(c)$

Let us construct a sequence $x_n = c - 1/n$. It follows that
$f(x_n) \leq f(c)$. Thus $(f(x_n))$ is bounded. Because $x_n \geq x_{n_1}$
it follows that $f(x_n) \geq f(x_{n + 1})$, and thus the sequence is
increasing. By MCT we can state that the sequence is convergent. Therefore
let $L = \lim f(x_n)$. Suppose that
$\epsilon > 0$. It follows, that there exists $N \in \textbf{N}$ such
that $|f(x_n) - L| < \epsilon$.

For any $c - 1/n =  x_n < x < c$ it follows that $f(x_n) \leq f(x) \leq f(c)$. Therefore for any $c \in R$ and
any $\epsilon > 0$ there exists $\delta$ such that
$-\delta < x  - c< 0 \to |f(x) - L| < \epsilon$
or in other words, there exists a left-hand-limit.

Let us also construct a sequence $x_n = c + 1/n$. Because $x_n < x_{n + 1}$
it follows that $f(x_n) \leq f(x_{n + 1})$. Therefore $f(x_n)$ is decreasing
and bounded below. Therefore is has a limit $L$. Therefore for any
$\epsilon > 0$ we can find $f(x_n)$ such that
$$|f(x_n) - L| < \epsilon$$
For any  $c < x < x_n = c  + 1/n$ it us true that $L < f(x) < f(x_n)$.
Therefore for any $c \in R$ there exists $L$ such that
for any $\epsilon > 0$  we can find $\delta = 1/n$ such that
$$0 < x - c < \delta \to |f(x) - L| < \epsilon$$
or in other words, function has right-hand limit.

Same reasoing can be applied to a monotone function, or we can just see, that
by multiplying decreasing function by -1 we'll get a incrasing function, and
use the argument, which is presented above.

Suppose now that $f$ is monotone and there exists a discontinuity set $D_f$.
From what we've proven above we can state that on this set, despite the
fact that there is no continuity, there still will be a right-hand limit
and left-hand limit. Therefore on this set those limits cannot be equal.
Therefore this discontinuity will be a jump discontinuity, as desired.

\section*{4.6.5}
\textit{Construct a bijection between the set of jump discontinuities of a
  monotone function $f$ and a subset of $Q$. Conclude that $D_f$ for a monotone
  function $f$ must  either be finite or countable, but not uncountable.}

$\lim_{x \to c^+} f(x) > \lim_{x \to c^-}$ by the virtue
of the fact that function is increasing and therefore $x < y \to f(x) < f(y)$
and $\lim_{x \to c^+} f(x) \neq \lim_{x \to c^-}$  because they are jump
discontinuities.

Therefore there exists a rational number $q$ such that
$$\lim_{x \to c_1^-} f(x) < q < \lim_{x \to c_1^+}$$
$$\lim_{x \to c_2^-} f(x) < q < \lim_{x \to c_2^+}$$
It follows then that $f(c_1) = f(c_2)$ and thus $c_1 = c_2$

By this bijection the set of discontinuity of monotone $f$ to subset of  $Q$
it follows that set of discontinuities of monotone function is either
countable, or subset of countable, which is either
finite (which includes empty) or countable.

This is not my own proof, I've spent too much time on this exercise and looked
up the answer.


\section*{4.6.6}
\textit{Show that in each case we set an $F_\sigma$ set as the set where
  each function is discontinous.}

$R$ is closed itself.

$R \setminus \{0\}$ can be written as
$$\cup_{n=1}^{\infty} (-\infty, -1/n] \cup [1/n, \infty)$$

$Q$ can be written as a countable union of singletons

$Z$ can be written as a countable union of singletons

For $(0, 1]$ we have 
$$(0, 1] = \cup_{n = 2}^{\infty} [1/n, 1]$$

\section*{4.6.7}
\textit{Prove that, for a fixed $\alpha > 0$, the set $D_\alpha$ is closed}

Set $D_\alpha$ is defined as
$$D_\alpha = \{x \in R: f \text{ is not }\alpha\text{-continous at }x\}$$

Suppose that $f$ is $\alpha$-continous at $x$. It follows that there exists
$\delta$ such that whenever $y, z \in (x - \delta, x + \delta)$ it follows that
$|f(y - f(z)| < \alpha$. Thus, if we pick any
$x_1 \in (x - \delta, x + \delta)$, because $(x - \delta, x + \delta)$ is open
we can state that there exists $\delta_1$ such that
$V_{\delta_1}(x_1) \subseteq (x -\delta, x + \delta)$, for which it is
true that $|f(y - f(z)| < \alpha$. Thus $f$ will  
 also be $\alpha$-continous at  $x_1$ .

Now let us create a collection of open intervals
$$\{I_n = (x - \delta, x + \delta): x \text{ is $\alpha$-continous and
  $\delta$ is corresponding number for given $\alpha$})\}$$
an let us take its union and denote it as $J$. From our discution earlier
it follows that only $\alpha$-continous points are contained in $J$.
Also, all the $\alpha$-continous points are contained in $J$.
Because $J$ is a union of open sets (open intervals are open sets), therefore
it is itself open. Thus complement of it (which is equal to $D_f$) is closed,
as desired.

\section*{4.6.8}
\textit{If $a_1 < a_2$, show that $D_{\alpha_2} \subseteq D_{\alpha_1}$}

Suppose that $f$ is $\alpha_1$-continous at $x$. It follows that there exists
$\delta_1$, for which it is true that whenever
$y, z \in (x - \delta_1, x + \delta_1$ it follows that 
$$|f(y) - f(z)| < \alpha_1 < \alpha_2$$
Therefore $x$ is $\alpha_2$-continous at $x$. Therefore
$D_{\alpha_2} \subseteq D_{\alpha_1}$
as desired.

\section*{4.6.9}
\textit{Let $\alpha > 0$ be given. Show that if $f$ is continous at $x$, then
  it is $\alpha$-continous at $x$ as well. Explain how it follows that
  $D_\alpha \subseteq D_f$.}

Suppose that $f$ is continous at $c$. It follows that for any $\epsilon > 0$
there exists $\delta > 0$ such that 
$$|x - c| < \delta \to |f(x) - f(c)| < \epsilon$$

Let $\epsilon = \alpha / 2$. it follows that for any number $x, y < \delta$
$$|y - c| < \delta \to |f(y) - f(c)| < \epsilon$$
$$|z - c| < \delta \to |f(z) - f(c)| < \epsilon$$


$$|f(y) - f(c)| + |f(z) - f(c)| < 2\epsilon$$
$$|f(y) - f(c)| + |f(c) - f(z)| < \alpha$$
$$|f(y) - f(c) + f(c) - f(z)| \leq |f(y) - f(c)| + |f(c) - f(z)| < \alpha$$
$$|f(y)  - f(z)|  < \alpha$$
Thus $f$ is $\alpha$-continous at $c$.

Thus $x \in D_f \to x \in D_\alpha$ for any $\alpha > 0$. Thus
$D_\alpha \subseteq D_f$ for any $\alpha > 0$.

\section*{4.6.10}
\textit{Show that if $f$ is not continous at $x$, then $f$ is not
  $\alpha$-continous for some $\alpha > 0$. Now explain why this guarantees
  that}
$$D_f = \cup_{n = 1}^{\infty}D_{\frac 1 n }$$

Suppose that $f$ is not continous at $x$. This means that there exists some
$\epsilon > 0$ such that for all $\delta > 0$ it is true that
there exists $x$ such that 
$$|x - c| < \delta \text{ and } |f(x) - f(c)| \geq \epsilon$$


Function defined on $R$ is not $\alpha$-continous at $c$
if for every $\delta > 0$ there exists
$y, z \in (-\delta + c, \delta + c)$ such that 
$|f(y) - f(z)| \geq \alpha$

Therefore for $\alpha = \epsilon$ and for $y = x$ and $z = c$
$$|f(y) - f(z)| = |f(x) - f(c)|  \geq \epsilon =  \alpha$$
Thus the function is not $\epsilon$-continous.

It follows that for any discontinuity in $f$  at $c$
there would exist $\epsilon$, for which there would exist $1/n < \epsilon$
such that $c \in D_{\frac 1 n}$ because
$c \in D_{\epsilon}$ and $D_{\frac 1 n} \supseteq D_\epsilon$ implies
$c \in  D_{\frac 1 n}$ Thus
$$D_f = \cup_{n = 1}^{\infty}D_{\frac 1 n }$$
as desired.

\chapter{The Derivative}

\section*{5.2.1}
\textit{Supply proofs for parts (i) and (ii) of Theorem 5.2.4}

Part (i)  of 5.2.4 states that
$$(f + g)'(c) = f'(c) + g'(c)$$

$$(f + g)'(c) = \lim_{x \to c}\frac{(f + g)(x) - (f + g)(c)}{x - c} 
= \lim_{x \to c}\frac{f(x) + g(x) - f(c) - g(c)}{x - c} = $$
$$=  \lim_{x \to c}\frac{f(x) - f(c)}{x - c} + \frac{g(x) - g(c)}{x - c} =
\lim_{x \to c}\frac{f(x) - f(c)}{x - c} +
\lim_{x \to c}\frac{g(x) - g(c)}{x - c} = f'(c) + g'(c)
$$

Part (ii) of 5.2.4 states that
$$(kf)'(c) = kf'(c)$$

$$(kf)'(c) = \lim_{x \to c}\frac{kf(x) - kf(c)}{x - c} =
k \lim_{x \to c}\frac{f(x) - f(c)}{x - c} = k f'(c)$$

as desired.

\section*{5.2.2}
\textit{(a) Use Definitions 5.2.1 to produce the proper formula for the
  derivative of $f(x) = 1/x$.}

$$f'(c) = \lim_{x \to c}\frac{f(x) - f(c)}{x - c} =
\lim_{x \to c}\frac{\frac 1 x  - \frac 1 c}{x - c} =
\lim_{x \to c}\frac{\frac{c}{cx}  - \frac{x}{cx}}{x - c} =
\lim_{x \to c}\frac{\frac{c - x}{cx}}{x - c} =
\lim_{x \to c}\frac{(c - x)\frac{1}{cx}}{x - c} =$$
$$ = \lim_{x \to c}\frac{- (x - c)\frac{1}{cx}}{x - c} =
\lim_{x \to c} - \frac{1}{cx} =  - \frac{1}{x^2} = - x^{-2}$$

\textit{(b) Combine the result in part (a) with the chain rule (Theorem 5.2.5)
  to supply a proof for part (iv) of Theorem 5.2.4}

By Chain Rule
$$(1/g)'(c) = -\frac{1}{g^2(c)} g'(c)$$
Thus
$$(f/g)'(c) = (f * 1/g)'(c) =  \frac{f'(c)}{g(c)}  -
\frac{f(c) g'(c)}{g^2(c)}  =
\frac{f'(c)g(c)}{g^2(c)}  - \frac{f(c) g'(c)}{g^2(c)}  = $$
$$\frac{f'(c)g(c) - f(c) g'(c)}{g^2(c)}$$
as desired

\textit{(c) Supply a direct proof of Theorem 5.2.4 (iv) be algebraically
  manipulating the difference quotent for $(f/g)$ in a style similar to
  the proof of Theorem 5.2.4 (iii)}

$$(f/g)'(c) = 
$$

$$\frac{(f/g)(x) - (f/g)(c)}{x - c} =
\frac{\frac{f(x)}{g(x)} - \frac{f(c)}{g(c)}}{x - c} =
\frac{\frac{f(x)g(c) - f(c)g(x)}{g(x)g(c)} }{x - c} =
\frac{f(x)g(c) - f(c)g(x) }{(x - c)g(x)g(c)} =
\frac{f(x)g(c) - f(c)g(x) }{(x - c)g(x)g(c)} =
$$

$$\frac{f(x)g(c) - f(c)g(c) + f(c)g(c)- f(c)g(x) }{(x - c)g(x)g(c)} =
\frac{g(c)(f(x) - f(c)) - f(c)(g(x)- g(c)) }{(x - c)g(x)g(c)} =$$
$$ =
\frac{g(c)(f(x) - f(c)) - f(c)(g(x)- g(c)) }{(x - c)}\frac{1}{g(x)g(c)} =
\left[\frac{g(c)(f(x) - f(c))}{(x - c)}
  - \frac{f(c)(g(x)- g(c)) }{(x - c)}\right]\frac{1}{g(x)g(c)} =
$$
$$=
\left(g(c)\left[\frac{(f(x) - f(c))}{(x - c)}\right] -
f(c)\left[\frac{(g(x)- g(c)) }{(x - c)}\right]\right)\frac{1}{g(x)g(c)} = 
$$

Therefore
$$(f/g)'(c) =
\lim_{x \to c}
\left(g(c)\left[\frac{(f(x) - f(c))}{(x - c)}\right] -
  f(c)\left[\frac{(g(x)- g(c)) }{(x - c)}\right]\right)\frac{1}{g(x)g(c)} =
\frac{g(c)f'(c) - f(c) g'(c)}{g^2(c)}
$$
as desired.

\section*{5.2.3}
\textit{By imitating the Dirichlet construction in Section 4.1, construct a
  function on $R$ that is differentiable at a single point}

So a function, that was comprised, that is continous at a single point
is
\begin{equation}
  g(x) =
  \begin{cases}
    x \text{ if } x \in Q \\
    0 \text{ otherwise }
  \end{cases}
\end{equation}

$$\frac{g(x) - g(c)}{x - c}$$
for this function is undefined for any rational point

Something along the lines of
\begin{equation}
  g(x) =
  \begin{cases}
    x^2 \text{ if } x \in Q \\
    -x^2 \text{ otherwise }
  \end{cases}
\end{equation}
might work

So around 0 for rationals
$$\lim_{x \to c}\frac{g(x) - g(c)}{x - c} =
\lim_{x \to c}\frac{x^2 - 0}{x - 0} = \lim_{x \to c}\frac{x^2}{x} = c = 0$$
and for irrationals
$$\lim_{x \to c}\frac{-x^2 - 0}{x - 0} = \lim_{x \to c}\frac{-x^2}{x} = -c
= 0$$

I think that it'll do. Not sure, that it is correct, but don't see
anything wring with it.

\section*{5.2.4}
\textit{Let }
\begin{equation}
  f(x) =
  \begin{cases}
    x^a \text{ if } x \geq 0 \\
    0 \text{ if } x < 0
  \end{cases}
\end{equation}
\textit{(a) For which values of $a$ is $f$ continous at zero?}
First of all, $a$ cannot be less than 0 or zero itself, because those powers
are not defined for $0$. As far as I can tell, $0^0$ is not defined, but
if we define it to be 1, then it'll be a trivial case.

I want to say that a function is continous at $a > 0$.

Suppose that $\epsilon > 0$ It follows that $f(c) = 0$. From the
left the limit will be equal to $0$. From the right we need to think about it
for a while. Suppose
$$|x - c| < \delta$$
$$|x - 0| < \delta$$
$$x < \delta$$
$$x^a < \delta^a$$
$$|x^a| < \delta^a$$
thus if we pick $\delta = \epsilon^{1/a}$ it follows that 
$$|x^a| < \epsilon$$
$$|x^a - 0| < \epsilon$$
$$|f(x) - 0| < \epsilon$$
Therefore the function has a right-hand limit of $0$ as well. Therefore for
any $a > 0$ it follows that $f$ is continous.

\textit{(b) For which values of $a$ is $f$ differentiable at zero? In this
  case, is the derivative function continous?}

Derivative of a given function is defined as
\begin{equation}
  f'(x) =
  \begin{cases}
    ax^(a - 1) \text{ if } x \geq 0 \\
    0 \text{ if } x < 0
  \end{cases}
\end{equation}

We can't put  $x = 0$ into a negative power, therefore  for
function to be differentiable we need to set $a > 1$.

\textit{(b) For which values of $a$ is $f$ twice-differentiable?}

Same idea,
\begin{equation}
  f''(x) =
  \begin{cases}
    a(a - 1)x^(a - 2) \text{ if } x \geq 0 \\
    0 \text{ if } x < 0
  \end{cases}
\end{equation}

Therefore $a > 2$.

\section*{5.2.5}
\textit{Let }
\begin{equation}
  g_a(x) =
  \begin{cases}
    x^a sin(1/x) \text{ if } x \neq 0 \\
    0 \text{ if } x = 0
  \end{cases}
\end{equation}
\textit{Find a particular (potentially noninteger) value for $a$ so that}

\textit{(a) $g_a$ is differentiable on $R$ but such that $g'_a$ is unbounded
  in $[0, 1]$}

In general, the derivative of a given function is calculated as
\begin{equation}
  g'_a(x) =
  \begin{cases}
    ax^{a - 1} \sin(1/x) - x^{a - 2}\cos(1/x) \text{ if } x \neq 0 \\
    0 \text{ if } x = 0
  \end{cases}
\end{equation}

By using the method of probes and errors (mostly errors), I came up with
a possible answer of 0; suppose that a = 0, then
\begin{equation}
  g'_0(x) =
  \begin{cases}
    - x^{-2}\cos(1/x) \text{ if } x \neq 0 \\
    0 \text{ if } x = 0
  \end{cases}
\end{equation}
which is unbounded, when we try to converge to 0; But this thing is not
continous at 0 (original function), therefore it is not differentiable at
0, therefore it is not differentiable on $R$,  therefore $0$ won't do.

Let us go back to the grass roots and derive the derivative (heh) from the
definition.

$$g'(c) = \lim_{x \to c}\frac{g(x) - g(c)}{x - c}$$
for 0 we'll have
$$\lim_{x \to c}\frac{g(x) - g(c)}{x - c} =
\lim_{x \to c}\frac{g(x) - 0}{x - 0} =
\lim_{x \to c}x^a\sin(\frac{1}{x}) \frac{1}{x} =
\lim_{x \to c}x^{a - 1}\sin(\frac{1}{x}) $$
We know, that thing will exist as long as $a > 1$.

For $x \neq 0$ we'll use a more general approach
\begin{equation}
  g'_a(x) =
  \begin{cases}
    ax^{a - 1} \sin(1/x) - x^{a - 2}\cos(1/x) \text{ if } x \neq 0 \\
    0 \text{ if } x = 0
  \end{cases}
\end{equation}
This thing will not have a limit near zero for $a < 2$. Thus,
we can set $1 < a <2 $ for the desired effect (Looked up the answer in the
book, but practically solved it myself with exception of justifying
the answer)

\textit{(b) $g_a$ is differentiable on $R$ with $g'_a$ continous but not
  differentiable at zero.}

One again, in order for $g'$ to exist at $0$ we need to set it to at least to
$1$. In order for $g'$ to be continous at $0$ we need to set $a > 2$,
otherwise the limit will not exist at zero.

Thus we need a $a$ such that $g''$ does not exist at $0$. In order for it to
happen let us think about
$$g''(x) = \lim_{x \to 0} \frac{g'(x) - g'(c)}{x - c} =
\lim_{x \to 0} \frac{g'(x)}{x} =
$$
$$= \lim_{x \to 0} \frac{ax^{a - 1} \sin(1/x) - x^{a - 2}\cos(1/x)}{x} =
\lim_{x \to 0} ax^{a - 2} \sin(1/x) - x^{a - 3}\cos(1/x) $$
Thus if we set $a = 3$ it follows that the oscilations for
$ax^{a - 2} \sin(1/x)$ are negligable, but for $x^{a - 3}\cos(1/x)$
are present, therefore the limit does not exist. Thus the desired number is
$3$. (it's actually possible to set $a$ to $(2, 3]$, but 3 will do as well).

\textit{(c) $g_a$ is differentiable on $R$ and $g'_a$ is differentiable on
  $R$, but suh that $g''_a$ is not continous at $0$.}

The limit of $g''_4$ is not defined, so I'll go with $4$.

\section*{5.2.6}
\textit{(a) Assume that $g$ is differentiable on $[a, b]$ and satisfies
  $g'(a) < 0 < g'(b)$. Show that there exists a point $x \in (a, b)$ where
  $g(a) > g(x)$ and a point $y \in (a, b)$ where $g(y) < g(b)$}

Because $g$ is defferentiable on $[a, b]$ it follows that it is continous
at $[a, b]$ as well. Thus, because $[a, b]$ is compact, it follows that
it attains maximum and minimum values at $[a, b]$. Thus, there exists
$x_0, x_1 \in [a, b]$ such that $f(x_0) \leq f(x) \leq f(x_1)$ for
all $x \in [a, b]$.

Suppose that there does not exist a point $x \in (a, b)$ such that
$g(a) > g(x)$ or $g(x) > g(b)$.
It follows that
$g(a) \geq g(x)$ and $g(x) \leq g(b)$ for every $x \in (a, b)$.
$$g(a) \leq g(x) \to g(a) - g(x) \leq 0 \to g(x) - g(a) \leq 0 $$
$$g(x) \leq g(b) \to g(x) - g(b) \leq 0 $$
for all $x \in (a, b)$ Then it
follows that for any sequence, that is contained in $a$
$(x_n) \to a$ $f(x_n) \geq f(a)$; and for every sequence $(y_n) \to b$
$f(y_n) \leq f(b)$
Therefore
$$f'(a) = \lim_{n \to \infty} \frac{f(x_n) - f(a)}{x_n - a} \geq 0$$
$$f'(b) = \lim_{n \to \infty} \frac{f(y_n) - f(b)}{y_n - b} \leq 0$$
but $f'(a) < 0 < f'(b)$. Therefore we have a contradiction.

Thus there exists point $x \in (a, b)$ such that $g(a) > g(x)$ and
$y \in (a, b)$ (with a possibility that $x = y$) such that $g(y) < g(b)$.

\textit{(b) Now complete the proof of Darboux's Theorem started earlier.}

Darboux's Theorem states that

\textit{If f is differentiable on an interval $[a, b]$, and if $\alpha$
  satisfies $f'(a) < \alpha < f'(b)$ (or the oter way around), then there
  exists a point $c \in (a, b)$ where $f'(c) = \alpha$}

We firstly simplify  matters with defining $g(x) = f(x) - \alpha x$ on
$[a, b]$. We can state that on $[a, b]$
$$g'(x)  = f'(x) - \alpha$$
by algebraic properties of derivatives. Now our hypothesis states that
$g'(a) < 0 < g'(b)$ for some $c \in (a, b)$. Using conclusion from
previous part we can state that there exists $c \in (a, b)$ such that
$g(c) < g(a)$ and $g(b) < g(c)$. Thus $g$ does not attain minimum at
$a$ or $b$. Thus it attains minimum at some point $j \in (a, b)$. Thus, by
IET. $g'(j) = 0$. Thus, $0  = f'(j) - \alpha \to f'(j) = \alpha$, as desired.

\section*{5.2.7}
\textit{Review the definition of uniform continuity (Definition 4.4.5)
  and also the content of
  Theorem 4.4.8, which states that continous functions on compact sets are
  uniformly continous.}

\textit{(a) Propose a definition for what it should mean to say that
  $f: A \to R$ is uniformly differentiable on $A$.}

Differentiability of a function at $c$ means that
$\lim_{x \to c}\frac{f(x) - f(c)}{x - c}$
exists. 

A function $f$ is uniformly differentiable at $A$ if for every $\epsilon > 0$
there exists $\delta > 0$ such that for every $x, y \in A$ such that
$|x - y| < \delta$ it follows that 
$$\left|\frac{f(x) - f(y)}{x - y} - f'(x)\right| < \epsilon$$

\textit{(b) Give an example of a uniformly differentiable function on $[0, 1]$}

Something along the lines of constant functions will probably suffice.

Suppose that $f: R \to R$ and  $f = 0$. It follows that for any
$x, y \in R$ $f'(x) = 0$ , $f(y) = 0$ and $f(x) = 0$. It follows that
$$|0| < \epsilon$$
for any $\epsilon > 0$

\textit{(c) Is there a theorem analogous to Theorem 4.4.8 for differentiation?
  Are functions that are differentiable on a closed interval $[a, b]$
  necessarily uniformly differentiable? The class of examples discussed in
  Section 5.1 may be useful.}

Something tells me, that this is not the case. Let us think about
function
\begin{equation}
  g_2(x) =
  \begin{cases}
    x^2 \sin(1/x)\text{ if } x \neq 0 \\
    0 \text{ if } x = 0
  \end{cases}
\end{equation}
as it follows from the discussion in the section, its derivative is not
continous. Thus, there exists $\epsilon$ (namely 1)
such that $|f(x)| \geq \epsilon$, for any given $\delta$. Thus,
$$|\frac{f(x) - f(y)}{x - y} - f'(x)| > \epsilon$$
for some $x \in [0, 1]$

\section*{5.2.8}
\textit{Decide whether each conjecture is true or false. Provide an argument
  for those that are true ad a counterexample for each one that is false.}

\textit{(a) If a derivative function is not constant, then the derivative must
  take on some irrational values.}

If derivative function is not constant, then there exist some sumbers
$a, b \in f'([a, b])$. By density of irrationals it follows that there exists
an irrational number  $i \in (a, b)$. By Darboux's Theorem, there exists
a number in the domain such that $f'(c) = i$.

\textit{(b) If $f'$ exists on an open interval, and there is some point $c$
  where $f'(c) > 0$, then there exists a $\delta$-neighborhood
  $V_\delta(c)$ around $c$ in which $f'(x) > 0$ for all
  $x \in V_\delta(c)$.}

\begin{equation}
  f(x) =
  \begin{cases}
    x/2 + x^2 \sin(1/x)\text{ if } x \neq 0 \\
    0 \text{ if } x = 0
  \end{cases}
\end{equation}

For $0$ we will have
$$f'(0) = \lim_{x \to 0} \frac{x/2 + x^2 \sin(1/x)}{x} =
\lim_{x \to 0} 1/2 + x \sin(1/x) = 1/2$$

But for any other $c \neq 0$
$$f'(c) = 2c\sin(1/x) - \cos(1/x) + 1/2$$
For which $cos(1/x)$ will plunge derivative around any neighborhood below 0.
(I don't have a clue on how should I come up with this answer, never crossed
my mind)

\textit{(c) If $f$ is differentiable on an interval containing zero and if
  $\lim_{x \to 0}f'(x) = L$, then it must be that $L = f'(0)$.}

Suppose that it is not the case. Then it follows that there exists
$\epsilon = |L - f'(0)|$. Because $\lim_{x \to 0}f'(x) = L$ it follows that
there exists $\delta > 0$ such 
$|x| < \delta \to |f'(x) - L| < \epsilon$
But $|0| = 0 < \delta$ and $|f'(0) - L| = \epsilon$, which is a contradiction.

\textit{(d) Repeat conjecture (c) but drop the assumption that $f'(0)$
  necessarily exists. If $f'(x)$ exists for all $x \neq 0$ and if
  $\lim_{x \to 0} f'(x) = L$, then $f'(0)$ exists and equal to $L$.}

Not necessarily. We can construct a function
\begin{equation}
  f(x) =
  \begin{cases}
    0 \text{ if } x \neq 0 \\
    1 \text{ if } x = 0
  \end{cases}
\end{equation}

Then our proposition falls apart.

\section*{5.3.1}
\textit{Recall from Exercise 4.4.9 that a function $f: A \to R$ is called
  Lipschitz on $A$ if there exists an $M > 0$ such that }
$$\left|\frac{f(x) - f(y)}{x - y}\right| \leq M$$
\textit{for all $x, y \in A$. Show that if $f$ is differentiable on a closed
  interval $[a, b]$ and if $f'$ is continous on $[a, b]$ then $f$ is
  Lipschitz on $[a, b]$.}

Suppose that $f$ is differentiable on $[a, b]$ and $f'$ is continous at
$[a, b]$. Because continous functions preserve compact sets we can state that
$f'([a, b])$  is compact as well. Therefore $f'([a, b])$ contains its maximum
and miniumum. Thus, for every  $x > y \in f'([a, b])$ there exists
$c \in (a, b)$ such that
$$f'(c) = \frac{f(x) - f(y)}{x - y}$$
Because $f'([a, b])$ is compact  we can state that for some $M \in R$
$$|f'(c)| \leq M$$
therefore 
$$|\frac{f(x) - f(y)}{x - y}| \leq M$$
for all $x, y \in [a, b]$. Therefore the function is Lipschitz on $[a, b]$,
as desired.

\section*{5.3.2}
\textit{Recall from Exercise 4.3.9 that a function $f$ is contractive on a
  set $A$ if there exists a constant $0 < s < 1$ such that }
$$|f(x) - f(y)| \leq s|x - y|$$
\textit{for all $x, y \in A$. Show that if $f$ is differentiable and $f'$ is
  continous and satistfies $|f'(x)| < 1$ on a closed interval, then $f$ is
  contractive on this set.}

Suppose that a function $f$ is differentiable and $f'$ is continous and
satisfies $|f'(x)| < 1$ on a closed interval $[a, b]$. Because $f'$ is
continous at a closed interval (which is a compact set) thus it attains
maximum at it. Thus, there exists a point  $s_p = [a, b]$ such that
$f'(s_p) \geq f'(x)$ for every $x \in [a, b]$. Thus let $s = f'(s_p) < 1$.

It follows, that for
any pair of numbers $x< y \in [a, b]$ there exist $c \in (a, b)$ such that
$$f'(c) = \frac{f(x) - f(y)}{x - y}$$
Because $c \in (a, b) \subset [a, b]$ it follows that $|f'(c)| \leq s$. Thus
$$|f'(c)| = |\frac{f(x) - f(y)}{x - y}| \leq s$$
$$\left|\frac{f(x) - f(y)}{x - y}\right| < s$$
$$\frac{|f(x) - f(y)|}{|x - y|} \leq s$$
$$|f(x) - f(y)| \leq s|x - y|$$
for some $0 < s < 1$, as desired.


\section*{5.3.3}
\textit{Let $h$ be a differentiable function defined on the interval $[0, 3]$,
  and assume that $h(0) = 1$, $h(1) = 2$, and $h(3) = 2$.}

\textit{(a) Argue that there exists a point $d \in [0, 3]$ where $h(d) = d$.}

First of all, on $[0, 1]$ there may be no point such that $h(d) = d$
(for example if $h$ is defined $h(x) = 1 + x$ for this section or something
that is greater that this). Thus we need to concentrate on the interval
$[1, 3]$.

Let us look at the function
$$f(x) = h(x) - x$$
it follows that if $f(x) = 0$, then there exists a point where $h(x) = x$.
We know that
$$f(0) = 1 - 0 = 1$$
and
$$f(3) = 2 - 3 = -1$$
Thus, by IVT and diffetentiability (and therefore continuity) of $h$ (and
therefore $f$) on $[0, 3]$,
there exists a point $c \in [0, 3]$  where $f(c) = 0$ and
thus $h(c) = c$, as desired.


\textit{(b) Argue that at some point $c$ wehave $h'(c) = 1/3$}

By MVT there exists a point $c \in (0, 3)$ such that
$$f'(c) = \frac{f(3) - f(0)}{3 - 0} = \frac{2 - 1}{3} = \frac{1}{3}$$
as desired.

\textit{(c) Argue that $h'(x) = 1/4$ at some point in the domain}

We know, that there exists $h'(c_1) = 0$ and $h'(c_2) = 1/3$. Thus by Darboux's
Theorem there exists a point in $(c_1, c_2)$ (or $(c_2, c_1)$, whichever is
appropriate) such that  $h'(c_1) < 1/4 < h'(c_2) \to h'(c_3) = 1/4$


\section*{5.3.4}
\textit{(a) Supply the details for the proof of Cauchy's Generalized Mean
  Value Theorem (Theorem 5.3.5)}

Theorem 5.3.5 states that
\textit{If $f$ and $g$ are continous on the closed interval $[a, b]$ and
  differentiable on the open inverval $(a, b)$, then there exists a point
  $c \in (a, b)$ where}
$$[f(b) - f(a)]g'(c) = [g(b) - g(a)]f'(c)$$
If $g'$ is never zero on $(a, b)$, then the conclusion can be stated as
$$\frac{f'(c)}{g'(c)} = \frac{f(b) - f(a)}{g(b) - g(a)}$$

Let
$$h(x)  = [f(b) - f(a)]g(x) - [g(b) - g(a)]f(x)$$
by standart rules of differentiations it follows that
$$h'(x)  = [f(b) - f(a)]g'(x) - [g(b) - g(a)]f'(x)$$
It follows by MVT that there exits  $c \in (a, b)$ such that
$$h'(c) = \frac{h(b) - h(a)}{b - a}$$
$$[f(b) - f(a)]g'(c) - [g(b) - g(a)]f'(c) = \frac{h(b) - h(a)}{b - a}$$
$$[f(b) - f(a)]g'(c) - [g(b) - g(a)]f'(c) =
$$
$$ = 
\frac{[f(b) - f(a)]g(b) - [g(b) - g(a)]f(b) -
  [f(b) - f(a)]g(a) + [g(b) - g(a)]f(a)}{b - a}$$
$$[f(b) - f(a)]g'(c) - [g(b) - g(a)]f'(c) =
$$
$$ = 
\frac{g(b)f(b) - f(a)g(b) - f(b)g(b) + g(a)f(b) -
  f(b)g(a) + f(a)g(a) + g(b)f(a) - g(a)f(a)}{b - a}$$
$$[f(b) - f(a)]g'(c) - [g(b) - g(a)]f'(c) = \frac{0}{b - a}$$
$$[f(b) - f(a)]g'(c) - [g(b) - g(a)]f'(c) = 0$$
$$[f(b) - f(a)]g'(c) = [g(b) - g(a)]f'(c)$$
as desired.

\textit{(b) Give a graphical interpretation of the Generalized Mean
  Value Theorem analogous to the one given for the Mean Value Theorem at
  the beginning of Section 5.3. (Consider f and g as parametric equations
  for a curve.)}

I didn't do one, but I'm sure, that wikipedia will have one. Maybe ine day,
when I'll be studying GNUPlot or something of sorts, I'll do one just for
fund and ammend this exercise. For now I'll mark this exercise as TODO

\section*{5.3.5}
\textit{ A fixed point of the function $f$ is a value $x$ where $f(x) = x$.
  Show that if $f$ is differentiable on an interval with $f'(x) \neq 1$, then
  $f$ can have at most one fixed point.}

$f(x) = 1 - x$ at $x = 0.5$ is a proof, that there exists a function for
1 fixed point where $f'(x) \neq 1$.

Suppose that $f'(x) \neq 1$, $f(x_1) = x_1$, $f(x_2) = x_2$ and $x_1 > x_2$.
It follows that by MVT there exists $c \in (x_1, x_2)$ such that
$$f'(c) = \frac{f(x_2) - f(x_1)}{x_2 - x_1}$$
$$f'(c) = \frac{x_2 - x_1}{x_2 - x_1}$$
$$f'(c) = 1$$
which is a contradiction.

\section*{5.3.6}
\textit{Let $g: [0, 1] \to R$ be a twice-differentiable (i.e., both $g$ and
  $g'$ are differentiable functions) with $g''(x) \geq 0$ for all
  $x \in [0, 1]$. If $g(0) > 0$ and $g(1) = 1$, show that $g(d) = d$ for some
  point $d \in (0, 1)$ if and only if $g'(1) > 1$. (This geometrically
  plausible fact is used in the introductory discussion to Chapter 6).}

\textbf{In forward direction: }
Suppose that there exists $d \in (0, 1)$ such that $g(d) = d$. It follows, that
there exists a point $c \in (d, 1)$ such that
$$g'(c) = \frac{g(1) - g(d)}{1 - d}$$
$$g'(c) = \frac{1 - d}{1 - d}$$
$$g'(c) = 1$$
Thus, there exists a point  $j \in (c, 1)$ such that
$$g''(j) = \frac{g'(1) - g'(c)}{1 - c}$$
Because $g''(x) \geq 0$ for all $x \in [0, 1]$ it follows that
$$\frac{g'(1) - g'(c)}{1 - c} > 0$$
$$g'(1) - g'(c) > 0$$
$$g'(1) > g'(c)$$
$$g'(1) > 1$$
as desired.

\textbf{In opposite direction: }

Define
$$f(x) = g(x) - x$$
It follows that $f'(x) = g'(x) - 1$ and $f''(x) = g''(x) > 0$. Thus
$f(1) = 0$, $f(0) > 0$ and $f'(1) > 0$

Now suppose that there is no element $k \in (0, 1)$, for which
$f(k) \leq 0$. It follows that for every sequence $(x_n) \to 1$ where
$x_n \neq 1$, $f(x_n) > 0$. Thus,
$$f'(1) = \lim_{n \to \infty}\frac{f(1) - f(x_n)}{1 - x_n}$$
but
$$\frac{f(1) - f(x_n)}{1 - x_n} < 0$$
for all $n \in N$, and thus
$$f'(1) \leq 0$$
which is a contradiction of given fact that $g'(1) = 1 \to f'(1) > 0$. Thus
we can state, that there exists a point $k \in (0, 1)$ such that
$f(k) \leq 0$. Therefore by IVT there exists $k_1: f(k_1) = 0$
(for  $f(k) = 0$ the case is trivial and doesn't require IVT).
Therefore $g(k_1) - k_1 = 0 \to g(k_1) = k_1$, as desired.

\section*{5.3.7}
\textit{(a) Recall that a function $f: (a, b) \to R$ is increasing on $(a, b)$
  if $f(x) \leq f(y)$ whether $x < y$ in $(a, b)$. Assume $f$ is differentiable
  on $(a, b)$. Show that $f$ is increasing on $(a, b)$ if and only if
  $f'(x) \geq 0$ for all $x \in (a, b)$}

\textbf{In forward direction: }
Suppose that a function is increasing and differentiable on  $(a, b)$.
Suppose that $c \in (a, b)$. It follows that
$$f'(c) = \lim_{x \to c}\frac{f(x) - f(c)}{x - c}$$
If $x > c$ then $f(x) - f(c) \geq 0$ and therefore
$\frac{f(x) - f(c)}{x - c} \geq 0$.
If $x < c$ then $f(x) - f(c) \leq 0$ and therefore
$\frac{f(x) - f(c)}{x - c} \geq 0$.
Thus $\frac{f(x) - f(c)}{x - c} > 0$ for all $x \neq c$. Thus
$f'(c) \geq 0$, as desired.

\textbf{In backward direction: }
Suppose that a function is differentiable and $f'(c) \geq 0$. It follows that
for any $x >  y \in (a, b)$ there exists $k \in (a, b)$ such that
$$f'(k) = \frac{f(x) - f(y)}{x - y}$$
thus
$$\frac{f(x) - f(y)}{x - y} \geq 0$$
$$\frac{f(x) - f(y)}{x - y} \geq 0$$
$$f(x) - f(y) \geq 0$$
$$f(x) \geq f(y)$$
Thus for any $x > y$ it follows that $f(x) \geq f(y)$ as desired.

\textit{(b) Show that the function}
\begin{equation}
  g(x) = 
  \begin{cases}
    x / 2 + x^2 \sin(1/x) \text{ if } x \neq 0
    0 \text{ if } x = 0
  \end{cases}
\end{equation}
\textit{is differentiable on $R$ and satisfies $g'(0) > 0$. Now
  prove that $g$ is not increasing over any interval containing 0.}

At $0$ we've got
$$f'(0) = \lim_{x \to 0}\frac{f(x) - f(0)}{x - 0} =
\lim_{x \to 0}\frac{f(x)}{x} =
\lim_{x \to 0}\frac{x / 2 + x^2 \sin(1/x)}{x} =
\lim_{x \to 0} 1 / 2 + x \sin(1/x) = 1/2$$
thus it is continous at $0$ and $f'(0) > 0$ (I forgot that we are using $g$
as a name for the function and I wont change a thing because of it)

For any other number other than $0$ we've got
$$f'(x) =     1 / 2 + 2x \sin(1/x) - x^2  \cos(1/x)  x^{-2}$$
$$f'(x) =     1 / 2 + 2x \sin(1/x) - \cos(1/x)$$
it follows that there does not exist a neighborhood around zero such that
$f'(x) \geq 0$ for all of the neighborhood (because of the discontinuity of
derivative around zero because of the term $\cos(1/x)$). Thus, there
does not exist a neighborhood around zero such that the function is
increasing on it, as desired (we needed to prove for any open interval, but
this proof will do for it because of the definition of open sets).

\section*{5.3.8}
\textit{Assume $g: (a, b) \to R$ is differentiable at some point
  $c \in (a, b)$. If $g'(c) \neq 0$, show that there exists a
  $\delta$-neighborhood $V_\delta(c) \subseteq (a, b)$ for which
  $g(x) \neq g(c)$ for all $x \in V_\delta(c)$. Compare this result with
  Exercise 5.3.7}

We are going to use a proof by contradiction on this one. Suppose that
there exists a $\delta$ and by extension $V_\delta(c)$ such that
$x , y \in V_\delta(c) \to g(x) = g(y)$. Let $(x_n) \to c$ be such that
$x_n \neq c$. It follows that $f(x_n) \to f(c)$. Thus there
exists $N \in \textbf{N}$ such that $n \geq N$ implies that
$$x_n \in V_\delta(c)$$
Now it follows that
$$f(x_n) = f(x_m) $$
for all $m \geq n \geq N$. Thus
$$f'(c) = \lim_{n \to \infty}{\frac{f(x_n) - f(c)}{x_n - c}}$$
$$f'(c) = \lim_{n \to \infty}{\frac{f(x_n) - f(c)}{x_n - c}}$$
$x_n, c \in V_\delta(c) \to f(x_n) = f(c) \to f(x_n) - f(c) = 0$. Thus
$$f'(c) = 0$$
which is a contradiction.

It folllows that function from 5.3.7(b) has no neighborhood around zero s.t.
$g(x) = g(c)$.

\section*{5.3.9}
\textit{Assume that $\lim_{x \to c}{f(x)} = L$, where $L \neq 0$, and assume
  $\lim_{x \to c}{g(x)} = 0$. Show that $\lim_{x \to c}|f(x)/g(x)| = \infty$}

Suppose $\epsilon > 0$

By convergence of $f$ there exists 
$\delta_1$ s.t.
$$x \in V_{\delta_1}(c) \to |f(x) - L| < L/2$$

$$|f(x) - L| < L/2$$
$$L - L/2 < f(x) < L + L/2$$
$$L/2 < f(x) < 3L/2$$

By convergence of $g$ there exists 
$\delta_2$ s.t.
$$x \in V_{\delta_2}(0) \to |g(x) | < |L/2|\epsilon$$

Pick $\delta = \min\{\delta_1, \delta_2\}$. It follows that 
$$x \in \delta \to |\frac{f(x)}{g(x)}| = \frac{|f(x)|}{|g(x)|} >
\frac{|f(x)|}{|L/2|/\epsilon)} > \frac{|L/2|}{ |L/2|/|\epsilon|} = \epsilon$$
therefore for any $\epsilon > 0$ there exists  $\delta$ s.t.
$$x \in \delta \to |\frac{f(x)}{g(x)}| = \frac{|f(x)|}{|g(x)|} > \epsilon$$
or in other words
$$\lim_{x \to c}{f(x)/g(x)} = \infty$$
as desired.

\section*{5.3.10}
\textit{Let $f$ be a bounded function and assume $\lim_{x \to c}g(x) = \infty$.
  Show that $\lim_{x \to c}f(x)/g(x) = 0$
}

Suppose that $f$ is bounded by $M > 0$. Thus $|f(x)| < M$

Let $\epsilon > 0$. By continuity of $g$ it follows that there exists
$\delta_1$ such that
$$x \in V_\delta(c) \to g(x) > M/\epsilon$$
Thus
$$x \in V_\delta(c) \to \frac{|f(x)|}{|g(x)|} < \epsilon$$
for every $\epsilon > 0$
Thus $\lim_{x \to c}f(x)/g(x) = 0$, as desired.

\section*{5.3.11}
\textit{Use the Generalized Mean Value Theorem to furnish a proof of the
  $0/0$ case of L'Hopital's rule (Theorem 5.3.6)}

Theorem 5.3.6 states that

\textit{Assume $f$ and $g$ are continous functions defined on an interval
  containing $a$, and assume that $f$ and $g$ are differentiable on this
  interval, with the possible exception of the point $a$. If $f(a) = 0$ and
  $g(a) = 0$, then }
$$\lim_{x_\to a}\frac{f'(x)}{g'(x)} = L \text{ implies }
\lim_{x_\to a}\frac{f(x)}{g(x)} = L$$

$$|x - a| < \delta \to \left|\frac{f'(x)}{g'(x)} - L\right| < \epsilon$$

Now let us think about
$$\frac{f(x)}{g(x)} = \frac{f(x) + f(a)}{g(x) + g(a)} = \frac{f'(c)}{g'(c)}$$
for some $c \in (a, x)$. Thus
$$\left|\frac{f'(c)}{g'(c)} - L\right| < \epsilon$$
$$\left|\frac{f(x) + f(a)}{g(x) + g(a)} - L\right|  < \epsilon$$
$$\left|\frac{f(x) }{g(x) } - L\right|  < \epsilon$$
as desired.

\section*{5.3.12}
\textit{Assume $f$ and $g$ are as described in Theorem 5.3.6, but now add the
  the assumption that $f$ and $g$ are differentiable at $a$ and $f'$ and $g'$
  are continous at $a$. Find a short proof for the $0/0$ case of L'Hopital's
  rule under this stronger hypothesis}

$$\lim_{x \to c}\frac{f'(x)}{g'(x)} = \frac{f'(c)}{g'(c)} =
\frac{\lim_{x \to c}\frac{f(x) - f(c)}{x - c}}
{\lim_{x \to c}\frac{g(x) - g(c)}{x - c}} =
\lim_{x \to c} \frac{\frac{f(x) - f(c)}{x - c}}
{\frac{g(x) - g(c)}{x - c}} =
\lim_{x \to c} \frac{f(x) - f(c)} {g(x) - g(c)} =
\lim_{x \to c} \frac{f(x) - 0} {g(x) - 0} =$$
$$ = \lim_{x \to c} \frac{f(x)} {g(x)}$$
as desired.

\section*{5.3.13}
\textit{Review the hypothesis of Theorem 5.3.6. What happens if we do not
  assume that $f(a) = g(a) = 0$, but assume only that $\lim_{x \to a}f(x) = 0$
  adn $\lim_{x \to a} g(x) = 0$? Assuming we have a proof for Theorem 5.3.6
  as it is written, explain how to construct a valid proof under this
  slightly weaker hypothesis.}

We can use a modified function $h$ such that it satisfies the requirements of
$5.3.6$ to show, that every sequence is convergent to the same limit and thus
sequences of $f$ converge to the same limit as well.


\section*{5.4.1}

\textit{Sketch a graph of $(1/2)h(2x)$ on $[-2, 3]$. Give a qualitative
  description of the functions}
$$h_n(x) = \frac{1}{2^n}h(2^nx)$$
\textit{as $n$ gets larger.}

I haven't draws a graph, but as I understand the main idea here is that the
functtion becomes more and more "compressed" when the values as values
of $n$ increase.

\section*{5.4.2}
\textit{Fix $x \in R$. Argue that the series}
$$\sum_{n = 0}^{\infty}{\frac{1}{2^n}h(2^nx)}$$
\textit{converges absolutely and thus $g(x)$ is properly defined.}

$0 \leq h(x) \leq 1$. Therefore
$$\sum_{n = 0}^{\infty}{\frac{1}{2^n}h(2^nx)}$$
is increasing and bounded above by $\sum{1/2^n}$, therefore it is convergent.

\section*{5.4.3}
\textit{Taking the continuity of $h(x)$ as given, reference the proper
  theorems from Chapter 4 that imply that the finite sum}
$$g_m(x) = \sum_{n = 0}^{m}{\frac{1}{2^n} h (2^nx)}$$
\textit{is continous on $R$}

The only two theorems that come to mind are Alebraic Continuity Theorem (4.3.4)
and Composition of Continous Functions (4.3.9).
They are sufficient enough to make this function continous
(i.e $h$ and $2^nx$ are continous, therefore (CoCF) $h(2^nx)$ is continous,
therefore (ACT) $\frac{1}{2^n} h(2^nx)$ is continous, therefore (ACT)
$\sum_{n = 0}^{m} \frac{1}{2^n} h(2^nx)$ is continous).


\section*{5.4.4}
\textit{Show that}
$$\frac{g(x_m) - g(0)}{x_m - 0} = m + 1$$
\textit{and use this to prove that $g'(0)$ does not exist.}

Firstly,
$$(x_m) = 1/2^m$$
$$g(x) = \sum_{n = 0}^{\infty}{h_n(x)} =
\sum_{n = 0}^{\infty}{\frac{1}{2^n}h(2^nx)}$$

Thus
$$g(x_m) = \sum_{n = 0}^{\infty}{\frac{1}{2^n}h(2^n \frac{1}{2^m})} = 
\sum_{n = 0}^{\infty}{2^{-n}h(2^{n - m})} =
\sum_{n = 0}^{\infty}{2^{-m}2^{-n + m} h(2^{n - m})} =
2^{-m} \sum_{n = 0}^{\infty}{2^{-n + m} h(2^{n - m})} = $$
$$
= x_m \sum_{n = 0}^{\infty}{2^{-n + m} h(2^{n - m})}
= x_m \left(\sum_{n = 0}^{m}{2^{-n + m} h(2^{n - m})} + 
\sum_{n = m + 1}^{\infty}{2^{-n + m} h(2^{n - m})}\right)
$$
For every even $j$ $h(j) = 0$. Thus we can state that
$$\sum_{n = m + 1}^{\infty}{2^{-n + m} h(2^{n - m})} =
\sum_{n = m + 1}^{\infty}{2^{-n + m} * 0 } = 0
$$
and for $x \in [0, 1]$ $x = |x| = h(x)$. Thus
$$\sum_{n = 0}^{m}{2^{-n + m} h(2^{n - m})} =
\sum_{n = 0}^{m}{2^{-n + m} 2^{n - m}} =
\sum_{n = 0}^{m}{2^0} =
\sum_{n = 0}^{m}{1} =
1 + \sum_{n = 1}^{m}{1} =
m + 1
$$
thus
$$
x_m \left(\sum_{n = 0}^{m}{2^{-n + m} h(2^{n - m})} + 
  \sum_{n = m + 1}^{\infty}{2^{-n + m} h(2^{n - m})}\right) =
x_m \left(m + 1 + 0\right) =
x_m \left(m + 1\right)
$$
thus
$$g(x_m) = x_m(m + 1)$$
$$\frac{g(x_m)}{x_m} = m + 1$$
$$\frac{g(x_m) + 0}{x_m + 0} = m + 1$$
and because $g(0) = 0$
$$\frac{g(x_m) + g(0)}{x_m + 0} = m + 1$$

Because of this we can state that there exists sequence $(x_m) \to 0$
where $x_m \neq 0$ and $\lim{f(x_m) } = \infty$. Thus the limit of 
$$\frac{g(x_m) + g(0)}{x_m + 0}$$
does not exist. Therefore $g'(0)$ does not exist, as desired.



\end{document}
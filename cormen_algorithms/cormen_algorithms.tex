\documentclass[11pt,oneside,titlepage]{book}
\title{My algorithms exercises}
\usepackage{amsmath,amssymb}
\usepackage{geometry}
\usepackage[linesnumbered, ruled]{algorithm2e}
\usepackage{hyperref}
\author{Evgeny Markin}
\date{2023}
\DeclareMathOperator \map {\mathcal {L}}
\DeclareMathOperator \pow {\mathcal {P}}
\DeclareMathOperator \topol {\mathcal {T}}
\DeclareMathOperator \basis {\mathcal {B}}
\DeclareMathOperator \ns {null}
\DeclareMathOperator \range {range}
\DeclareMathOperator \fld {fld}
\DeclareMathOperator \inv {^{-1}}
\DeclareMathOperator \Span {span}
\DeclareMathOperator \lra {\Leftrightarrow}
\DeclareMathOperator \eqv {\Leftrightarrow}
\DeclareMathOperator \la {\Leftarrow}
\DeclareMathOperator \ra {\Rightarrow}
\DeclareMathOperator \imp {\Rightarrow}
\DeclareMathOperator \true {true}
\DeclareMathOperator \false {false}
\DeclareMathOperator \dom {dom}
\DeclareMathOperator \ran {ran}
\newcommand{\eangle}[1]{\langle #1 \rangle}
\newcommand{\set}[1]{\{ #1 \}}
\newcommand{\qed}{\hfill $\blacksquare$}

\begin{document}
\maketitle
\tableofcontents

\chapter*{Preface}
Exercises for Introduction to Algorithms by Cormen et al., 4th ed. It has
exercises, that should be written down, moslty in math and whatnot.

Some of the exercises require that you code something (sometimes it's not
explicitely required, but that would be nice to code it anyways), and this
code is presented in the progs folder. Everything is written in C, because I'm most
familiar with it. 

Pseudocode is written by using package \textbf{algorithm2e}, which does not really correspond
to the one, that is used in the book, but it still does the job.

Same rules as usual apply, if you want to use this book for any reason -- go right ahead, it's
free, just be aware that it is full of mistakes.

\chapter*{Usefull stuff}

\subsection{Triangular numbers}
Triangular numbers refer to the sum
$$1 + 2 + 3 + 4 ...$$
They are called so because when we stack 1, 2, 3, etc, things on top of the other, we get a
triangle. For a useful visual reference, google (or just imagine) bowling pins.

The sum evaluates to
$$S(n) = \sum_{i = 1}^{n}{i} = \frac{n(n + 1)}{2}$$
for which it is true that
$$\Theta(S(n)) = \Theta(n^2)$$.

\part{Foundations}

\chapter{The Role of Algorithms in Computing}

\section{Algorithms}

\subsection{}

\textit{Describe your own real-world example that required sorting. Describe one
  that required finding the shortest distance between two points.}

I've needed both when I was creating 8-puzzle program

\subsection{}

\textit{Other than speed, what other measures of efficiency migh you need to consideer
  in a real-world setting?}

Memory and parallellability.

\subsection{}

\textit{Select a data structure that you've seen, and discuss its strengths and limitations.}

Linked lists. They are perfect in everything, apart from sorting; but even then you can define
any data structure through linked lists, which makes them just perfect (especially
omnidirectional ones).

\subsection{}

\textit{Suggest a real-world problem in which only the best solution will do. Then come up
  with one in which "approximately" the best solution is good enough.}

Sorting has to be perfect, otherwise it's borderline useless. Estimated time to
complete the task can tolerate imperfections.

\subsection{}

\textit{Describe a real-world problem in which sometimes the entire input is available
  before you need to solve the problem, but other times the input is not
  entirely available in advance and arrives over time}

Traffic on maps does this thing. Sometimes you have all the input, sometimes it changes.


\section{Algorithms as a technology}

\subsection{}

\textit{Give an example of an application that requires algorithmic content at the
  application level, and discuss the function of the algorithms involved.}

Path finding on maps will do. It requires to triverse graphs and whatnot.

\subsection{}

\textit{Suppose that for inputs of size $n$ on a particular computer, insertion sort
  runs in $8n^2$ steps and merge sort runs in $64n \lg n$ steps. For which values of $n$
  does insertion sort beat merge sort?}

For
$$8 n^2 < 64 n \lg n$$
$$n <8 \lg n$$
$$\frac{n}{lg n} < 8$$
$$n \approx 44$$
cases.


\subsection{}

\textit{What is the smallest value of $n$ such that an algorithm whose running time is $100n^2$
  runs faster than an algorithm whose running time is $2^n$ on the same machine?}

Calculator says 15

\chapter{Getting Started}


\section{Inserition sort}

\subsection{}

\textit{Using Figure 2.2 as a model, illustrate the operation of Insertion-Sort on an
  array initially containing the sequence $[31, 41, 59, 26, 41, 58]$}
$$[31, 41, 59, 26, 41, 58]$$
$$[26, 31, 41, 59, 41, 58]$$
$$[26, 31, 41, 41, 59, 58]$$
$$[26, 31, 41, 41, 58, 59]$$

\subsection{}

\textit{State loop invariant for the Sum-Array procedure.}

At the start of each iteration of the for loop, the $sum$ variable contains the sum of elements
in $A[1: i]$.

\textbf{Initialization: }

Firstly, we've got 0 as the sum. Given that we've summed 0 elements so far, we can conclude
that this is indeed a correct value to set it.

\textbf{Maintenance: }

For each iteration of $i$ we've got that we add a i'th element from the array to our sum and
incrementing $i$. Thus before iterating through $i$ we had a sum of all of the elements
before $i$, and after iterating through it we create a sum of elements before $i$ and
the $i$'th element as well. Thus the sum after iterating through $i$ is correct.

\textbf{Termination: }

Given that the array is finite, we follow that because we are incrementing $i$ at each
iteration the algoritm will terminate. Because we increment through elements, we follow that
we've added every element of the array to the sum at the point of termination.

\subsection{}

\textit{Rewrite the Insertion-Sort procedure to sort into monotonically decreasing instead
  of monotonically increasing order.}

Done it in the progs section; long story short: reverse the ordering function in the inner loop,
replace  $A[j] > key$ with $A[j] < key$.

\subsection{}

\textit{Consider the seqrching problem}

\textit{Input: A sequence of $n$ numbers $[a_1, ... a_n]$ stored in array $A[1:n]$ and a
  value x.}

\textit{Output: An index $i$ such that $x$ equals $A[i]$ or the special value $NIL$ if $x$
  does not appear in $A$.}

\textit{Write pseudocode for linear search, which scans through the array from beginning to
  end, looking for $x$. Using a loop invariant, prove that your algorythm is correct.
  Make sure that your loop invariant fufills the three necessary properties.}

\begin{function}
  \caption{Linear-search (A, x)}
  \For{($i = 1 \to n$)}{
    \If{$A[i] = x$} {
      \Return {i} \;
    }
  }
  \Return{NIL}\;

\end{function}

At each iteration of the for loop, we've got that elements $A[1:i]$ do not contain $x$.

\textbf{Initialization: }
Null case is when we haven't gone through any of the elements yet;
because we haven't searched anything, we
can follow that we haven't found anything, thus the base case is correct

\textbf{Maintenance: }
Suppose that $i = j + 1$. Then we follow that $A[1:j]$ does not contain our element by our
induction hypothesis (not sure that we can use this kind of language here, but it's
my book and I can do whatever I want). Then $i$'th element is cheched for the necessary
equality and returned in case of the equality; otherwise we increment $i$ and make it so
$A[1: j + 1]$ is the array of processed values. Thus before and after the loop iteration
we have a correct solution.

\textbf{Termination: }
We terminate either after going through every element, or some time before it.

Example of this thing in C is presented in progs directory


\subsection{}

\textit{Consider the problem of adding two $n$-bit binary integers $a$ and $b$, stored in
  two $n$-element arrays $A[0: n - 1]$ and $B[0: n - 1]$, where each element is either 0 or 1,
  $a = \sum_{n = 0}^{n - 1}A[i] * 2^i$ and $b = \sum_{n = 0}^{n - 1}B[i] * 2^i$. The
  sum $c = a + b$ of the two integers should be stored in binary form in an $(n + 1)$-element array
  $C[0:n]$, where   $c = \sum_{n = 0}^{n - 1}C[i] * 2^i$. Write a procedure Add-Binary-Integers
  that takes an input arrays $A$ and $B$, along with the length $n$, and returns array $C$ holding
  the sum.}


\begin{function}
  \caption{Add-Binary-Integers (A, B, n)}
  define C[0: n + 1] and fill it with zeroes\;
  \tcc{carry stores overflow from the previous iteration} 
  $carry \leftarrow 0$\;
  \For{($i = 0 \to n + 1$)}{
    \tcc{Initializing temporary variable with carry bit; we need to sum carry, A[i], and B[i],
    so we can just initialize temp with carry}
  $r \leftarrow carry$ \;
  \tcc{if it is not the last bit, where A nor B are not defined; We can just define it to be zero
  with the same result}
    \If{$i \neq n + 1$} {
      $r \leftarrow A[i] + B[i] + r$ \;
    }
    \tcc{If we've got an overflow as the result, set carry bit and result of summation
      appropriately  }
    \eIf{$r > 1$}{
      $carry \leftarrow 1$ \;
      $r \leftarrow r \% 2$\;
    }{
      \tcc{otherwise zero the carry bit}
      $carry \leftarrow 0$\;
    }
    \tcc{lastly, put the result of the partial summation into the resulting array}
    $C[i] \leftarrow r$\;
  }
  \Return{C}\;

\end{function}

\section{Analyzing algorithms}

\subsection{}

\textit{Express the function $n^3/1000 + 100n^2 - 100n + 3$ in terms of $\Theta$-notation}

$$n^3/1000 + 100n^2 - 100n + 3 \in \Theta(n^3)$$

\subsection{}

\textit{Consider sorting $n$ numbers strored in array $A[1: n]$ by first finding the smallest
  element of $A[1:n]$ and exchanging it with the element in $A[1]$. Continue in this manner for
  the first $n - 1$ elements of $A$. Write a pseudocode for this algorithm, which is known as
  selection sort. What loop invarian does this algorithm maintain? Why does it need to run
  for only the first $n - 1$ elements, rather than for all $n$ elements? Give the worst-case
  running time fo selection sort in $\Theta$-notation. Is the best-case running time
  any better?}


\begin{function}
  \caption{Selection-Sort (A, n)}
  \For{$i = 1 \to n - 1$} {
    $minPos \leftarrow i$\;
    \For {$j = i \to n$} {
      \If {$A[j] < A[i]$} {
        $minPos \leftarrow j$
      }
    }
    $temp \leftarrow A[i]$\;
    $A[i] \leftarrow A[minPos]$\;
    $A[j] \leftarrow temp$\;
  }
\end{function}

(this function in C is located in progs )

Algorithm maintains that every $A[1:i]$ is sorted and any element in  $A[i + 1:n]$ is greater
or equal to every element of $A[1:i]$. It needs to run only for $n - 1$ times, because the last
element $n$ will be greater or equal then any element in $A[1:n - 1]$ and therefore will
be places in its rightful place.

Worst-case running time of this algorithm is $\Theta(n^2)$, because we compute that
coefficient for $c_4$ is $\sum_{i = 1}^n i = \Theta(n^2)$, while all the other coeffitients
are linear.

Given coefficient for $c_4$ is not changed depending on the input, we follow that
the best running time and worst running time are the same

\subsection{}

\textit{Consider linear search again. How many elements of the input array need to be checked on
  the average, assuming that the element being searched for is equaly likely to be any element
  in the array? How about the worst case? Using $\Theta$-notation, give the average-case and
  worst-case running times of linear search. Justify your answers.}

The low-hanging answer here is that we need to check the first half of the array in order to
find the needed input. It is wrong though.

Assuming that $A[i] = x$ with probability $p$, we follow that we need to check first
$1/p$ elements in order to get the desired element (I can be wrong here, I'm not good with
probabilities).

Assuming that I'm right about it, we follow that $pn \in \Theta(n)$ for the average case.

The worst case is if $A$ does not contain $x$, in which case we've got the
running time of $n$. It is also in $\Theta(n)$.

\subsection{}

\textit{How can you modify any sorting algorithm to have a good best-case running time?}

We can throw a check for the case if $A$ is sorted from the start in the start of the
sorting algorithm. Thus we can follow, that the best case for any such algorithm is
when we get the sorted input, and we get linear best-case running time.

\section{Designing algorithms}

\subsection{}

\textit{Using Figure 2.4 as a model, illustrate the operation of merge sort on an array
  initially containing the sequence }
$$[3, 41, 52, 26, 38, 57, 9, 49]$$
$$[3, 41, 52, 26] [ 38, 57, 9, 49]$$
$$[3, 41] [52, 26] [38, 57] [9, 49]$$
$$[3] [41] [52] [26] [38] [57] [9] [49]$$
$$[3, 41] [26, 52] [38, 57] [9, 49]$$
$$[3, 26, 41,  52] [9, 38, 49, 57]$$
$$[3, 9, 26, 38, 41, 49, 52, 57]$$

\subsection{}

\textit{The test in line 1 of the Merge-Sort procedure reads "if $p \geq r$" rather than
  "if $p \neq r$". If Merge-Sort is called with $p > r$, then the subarray $A[p:r]$ is
  empty. Argue that as long as the initial call of Merge-Sort(A, 1, n) has $n \geq 1$, the
  test "if $p \neq r$" suffices to ensure that no recursive call has $p > r$.}

For a general case when $p \leq r$ we've got that
$$p = 2p/p = (p + p)/2 \leq \lfloor(p + r)/2\rfloor = q$$
thus $p \leq q$ and call on line 4 will be done with the condition that $p \leq q$.

Focusing our attension on $r$, we get that by condition on lines 1 and 2 we follow that
the function returns if $p = r$. Thus we follow that $p \neq r$, and by our assumption
we get that by the line 3 we get if $p < r$. Given that both  $p$ and $r$ are integers,
we follow that
$$(p + r)/2 \leq (p + p + 1)/2  = p + 1/2$$
thus
$$\lfloor(p + r)/2\rfloor \leq  \lfloor p + 1/2 \rfloor = p$$
Therefore we follow that
$$q \leq r - 1$$
$$q + 1\leq r$$
thus we can follow that the call to the Merge-Sort on line 5 happens also with the condition
that $p \leq r$.

Thus we can state that given that $p \leq r$ we can follow that all the other calls
to Merge-Sort, that happen inside of it  will have the same restriction.
Therefore we follow that if initical call to Merge-Sort happens with $n \geq 1$, then the test
"if $p \geq r$" is equivalent to the test "if $p \neq r$".

\subsection{}

\textit{State a loop invariant for the while loop of lines 12-18 of the Merge procedure. Show
  how to use it, along with the while loops 20-23 and 24-27, to prove that the Merge procedure
  is correct.}

At the start of each iteration of the while loop of lines 12-18, the subarray $A[1:k]$ consists
of th elements, originally in $L[1:i]$ or $R[1:j]$ in sorted order and any element
in $A[1:k]$ is less or equal to any element of $L[i + 1, n_L]$ and $R[i + 1, n_R]$.

Base case is trivial, sinse none of the subarrays contain any elements.

For maintenance we've got that we take the lowest element of $L[i:n_L]$ or
$R[j:n_R]$ (which happens to be the lowest  of the  first elements of those subarrays, given that
both of those arrays are sorted), and placing them at $A[k]$. Because of
our restrictions on $L[i + 1, n_L]$ and $R[ + 1, n_R]$ we follow that $A[k]$ will be less or
equal then any element of $L[i + 1, n_L]$ and $R[j + 1, n_R]$. And given that it
is originally from $L[i + 1, n_L]$ or $R[i + 1, n_R]$, we follow that $A[k]$ is
greater or equal then any of $A[1:k - 1]$. Thus we can follow that $A[1:k]$ is sorted.
Thus at the end of the loop we've got our loop invariant.

Termination happens whenever we run out of elements in $L$ or $R$, at which point we still have
our invariant.

We follow then that after our loop, one of the arrays $L$ or $R$ will be empty, and
will contain elements, that are greater or equal then $A[1:k]$. Thus we just append
remaining elements of either of the arrays to $A$.


Given that $L$ and $R$ are two subarrays of $A[k]$, whose disjoint union is $A$, we follow that
merge indeed merges two subarrays of $A[k]$ into one sorted array, as desired.

\subsection{}

\textit{Use mathematical induction to show that when $n \geq 2$ is an exact power of 2, the
  solution of the recurrence }
$$T(n) =
\begin{cases}
  2 \text{ if }n = 2 \\
  2T(n/2) + n \text{ if } n > 2\\
\end{cases}
$$
\textit{is $T(n) = n \lg(n)$.}

Our domain here is $(n_k) = 2^k$

For base case $k = 1 \to n = 2$ we've got
$$T(2) = 2 = 2 * 1 = 2 \lg(2)$$

Our hypothesis is that for $k - 1 < n$ and $n_{k - 1}$ we've got that
$T(n_{k - 1}) = n_{k - 1} \lg(n_{k - 1})$

Thus we follow that if $n$ is an exact power of $2$ and $n \geq 2 > 0$ we've got that
$n/2 < n$. Thus we follow that
$$T(n) = 2T(n/2) + n = 2 n/2 \lg(n/2) = n \lg(n/2) + n = n(\lg(n/2) + 1) =$$
$$= n(\lg(n * 2^{-1}) + 1) =
n(\lg(n) - 1 + 1) = n \lg(n)$$

Therefore for $x \in (n_k)$ we've got that $T(x) = x \lg(x)$, as desired.

\subsection{}

\textit{You can also think of insertion sort as a resursive algorithm. In order to sort
  $A[1:n]$, recursively sort the subarray $A[1:n - 1]$ and then insert $A[n]$ into the
  sorted subarray $A[1: n - 1]$. Write a pseudocode for this recursive version of insertion
  sort. Give a recurrence for its worst-case running time.}

\begin{function}
  \caption{Recursive-insertion-Sort (A, n)}
  \tcc{Ensure termination and sane inputs}
  \If {$n \leq 1$}{
    \Return{}\;
  }
  \tcc{Sort everything in A[1:n - 1]}
  Recursive-insertion-Sort (A, n - 1)\;
  $i \leftarrow n - 1$ \;
  \While{$A[i] > A[n]$ and $i \geq 1$}
  {
    $i \leftarrow i - 1$
  }
  \tcc{Swapping $A[n]$ and $A[i]$}
  $temp \leftarrow A[i]$ \;
  $A[i] \leftarrow A[n]$ \;
  $A[n] \leftarrow temp$ \;
\end{function}

For the worst-case we've got
$$T(n) = T(n - 1) + n$$
which reduces to
$$T(n) = n(n + 1)/2$$


\subsection{}

\textit{Referring back to the searching problem (see Exercise 2.1-4), observe that if the
  subarray being searched is already sorted, the searching algorithm can check the
  midpoint of the subarray against v and eliminate half of the subarray from further
  consideration. The binary search algorithm repeats this procedure, halving the
  size of the remaining portion of the subarray each time. Write pseudocode, either
  iterative or recursive, for binary search. Argue that the worst-case running time of
  binary search is $\Theta(\lg(n))$}

\begin{function}
  \caption{Binary-Search (A, x, s, f)}
  \If{$s = f$} {
    \Return{NIL}\;
  }
  $q \leftarrow \lfloor (s + f)/2\rfloor$\;
  \eIf{$A[q] = x$} {
    \Return{q};
  }{
    \eIf{$A[q] < x$}
    {
      \Return{Binary-Search (A, x, q + 1, f)} \;
    }
    {
      \Return{Binary-Search (A, x, s, q - 1)} \;
    }
  }
\end{function}

For this one we conclude that its worst-case is $\Theta(\lg n)$ by the same logic as in merge,
but we go through only one branch.

\subsection{}

\textit{The while loop of lines 5-7 of the Insertion-Sort procedure in Section 2.1
  uses a linear search to scan (backward) through the sorted subarray $A[1:j - 1]$.
  What if insertion sort used a binary search (see Exercise 2.3-6) instead of a linear
  search? Would that improve the overall worst-case running time of insertion sort
  to $\Theta(n \lg(n))$?}

If we use binary search instead of the linear search, then we'll undoubtably  have a
better running time on a real machine, given that the the binary search uses less instructions
then the linear search.

But in order to put the required value in the needed place, we'll still have to go
through the entire array in a worst-case scenario ( array sorted in reverse order). Thus
it can be argued that the overall asympomatic approximation will be the same.

\subsection{}

\textit{Describe an algorithm that, given a set $S$ of $n$ integers and another integer $x$,
determines whether $S$ contains two elements that sum exactly to $x$. Your algorithm
should take $\Theta (n \lg n)$ time in the worst case.}

The obvious case with just going through the whole array will not do, as it will give us
worst-case scenario of $\Theta(n^2)$.

So the main strategy here will be to sort the whole array and then make a linear search for
the desired values.

Thus we'll get something that looks like Sum-of-ints
\begin{function}
  \caption{Sum-of-ints (A, n, x)}
  Merge-Sort(A) \;
  $i \leftarrow 1$\;
  $j \leftarrow n$\;
  \While{$i \neq j$}{
    \eIf {$A[i] + A[j] > x$}{
      $j \leftarrow j - 1$ \;
    }
    {
      \eIf{$A[i] + A[j] > x$} {
        $i \leftarrow i + 1$ \;
      }
      {
        \Return{$[A[i], A[j]]$}\;
      }
    }
  }
  
\end{function}

The last while iterates once through the array, therefore we can (non-rigorously) follow that
everything except for the search runs at $\Theta(n)$. Thus the whole thing goes
through the array at time
$$\Theta(n) + \Theta(n \lg(n)) = \Theta(n \lg(n))$$
(or at the very least I think so; once again we've
got non-rigorous approach here, but I'm sure that in subsequent chapters we'll have some
more sophisticated tools at our hands to prove me right).


\section{Problems}

\subsection{Insertion sort on small arrays in merge sort}

\textit{(a) Show that insertion sort can sort the $n/k$ sublists, each of length $k$,
  in $\Theta(nk)$ worst-case time}

We know that the insertion sort sorts an array with worst-case performance of
$\Theta(n^2)$, and therefore its worst case performance for the
array of length $k$ is $\Theta(k^2)$. Thus we can follow that it sorts
$n/k$ sublists of the original array at time
$$n/k * \Theta(k^2) = n/k * ck^2 = cnk = \Theta(nk)$$

\textit{(b) Show how to merge the sublists in $\Theta(n \lg(n/k))$ worst-case
  time}

We can probably get it by applying standart merge in pairs, and then recursively. More
explicitely we first merge first and the second, third and fourth, and so on, getting in the
end roughly $n/2k$ sorted subarrays. Then we can relabel resulting arrays, so that
the result of the merge on first and second subarray becomes first array, result of
merging third and fourth becomes second array and so forth.

By doing that we iterate through the whole array at every inetration, and we go through roughly
$\lg(n/k)$ iterations, because every time we reduce the number of subarrays in half.

\textit{(c) Given taht the modified algorithm runs in $\Theta(nk + n\lg(n/k))$ worst-case time,
  what is the largest value of $k$ as a function of $n$ for which the modified algorithm has the
  same running time as standart merge sort, in terms of $\Theta$-notation?}

With our rough definition of $\Theta$-notation, we get that the approximate times of the
algorithms are roughly proportional to the times inside $\Theta$. Thus there must exist
$c_1, c_2$ such that

$$c_1(nk + n\lg(n/k)) = c_2(n \lg(n))$$
$$c_1n(k + \lg(n/k)) = c_2 n\lg(n)$$
$$c_1(k + \lg(n/k)) = c_2\lg(n)$$
$$c_1(k + \lg(n) - \lg(k)) = c_2\lg(n)$$
$$k - \lg(k) = (c_1 - c_2)\lg(n)$$
$$\lg(2^k) - \lg(k) = (c_1 - c_2)\lg(n)$$
$$\lg(2^k/k) = (c_1 - c_2)\lg(n)$$
$$2^k/k = 2^{(c_1 - c_2)}n$$

I'm sure that I've made a mistake here somewhere, but it's the best that I can do.

\textit{(d) How should you choose $k$ in practice? }

By trial and error, I suppose. I'd stick to one sort in general and not overcomplicate the things,
but if we try to overcomplicate things, then we can go through the cases when the linear
sort outperform the merge, get somewhat concrete number and divide the first sort based on this
number.

\subsection{Correctness of bubblesort}

\textit{(a) Let $A'$ denote the array $A$ after $Bubblesort(A, n)$ is executed. To prove that
  Bubblesort is correct, you need to prove that it terminates and that
  $$A'[1] \leq A'[2] \leq ... \leq A'[n]$$
  In order to show that Bubblesort actually sorts, what else do you need to prove? }

That $A'$ consists of elements, that were originally in $A$.

\textit{(b) State precicely a loop invariant for the for loop in lines 2-4, and prove that
  this loop invariant holds.}

$A[j]$ is the smallest element of $A[j:n]$. Elements of $A[i + 1: n]$ remain to be the same,
but their order might change.

\textbf{Initialization: }
$A[j:n]$ consists of one element, therefore $A[j]$ is the smallest element of it. No
manipulations to $A[i + 1: n]$ were applied, therefore the elements of it are the same.

\textbf{Maintenence: }
If $A[j - 1]$ is greater then $A[j]$, then we swap their places. Thus we follow that
before the next iteration $A[j]$ will be the smallest element of $A[j:n]$. The only performed
operation is the possible swap, therefore we can follow that elements remain to be the same.

\textbf{Termination: }
Loop terminates, because at each iteration we reduce $j$ by 1. Nothing happens to the loop
invariant, therefore it still holds.

\textit{(c) Using the termination condition of the loop invariant proved in part (b), state
  a loop invariant for the for loop in lines 1-4 that allows you to prove inequality
  (2.5). Your proof should use the structure of the loop-invariant proof presented
  in this chapter.}


$A[1:i - 1]$ are the smallest values of $A[1:n]$, but in sorted order.
Values of $A[1:n]$ remain the same, but their place may change.

\textbf{Initialization: }
$A[1:i - 1]$ is empty, therefore first part holds. No manipulations were performed on the
array, therefore the second condition also holds.

\textbf{Maintenence: }
As we've discussed earlier, inner for makes it so that we push the smallest element of $A[i:n]$
to $A[i]$. Given that $A[1: i-1]$ has the smallest values, we follow that the smallest value
of $A[i:n]$ is greater or equal to any value in $A[1:i-1]$. THus we follow that $A[1:i - 1]$
will be sorted before the next iteration. Thus the first part holds.

Conditions, that are imposed in loop invariant of inner for guarantees us the second condition
of the loop invariant.

\textbf{Termination: }
We increment $i$ at every iteration and $n$ remains unchanged, therefore we follow that
loop terminates. All of the requirements of loop invariant are implied by maintenence clause.

\textit{(d) What is the worst-case running time of Bubblesort? How does it compare with
  the running time of Insertion-Sort?}

Worst-case is once again the reversely sorted array. In that case we need to
execuse line 4
$$\Theta(\sum_{j = 1}^n{j}) = \Theta(n^2)$$
times. In the best case we've got the same case, but now with the line 3.

This sort has the same asymptotic time as the Insertion sort, although the practice shows, that it
it absolutely horrible when it comes to the machine test.

\subsection{Correctness of Horner's rule}

\textit{(a) In terms of $\Theta$-notation, what is the running time of this procedure? }

Linear with respect to $n$ ($\Theta(n)$).

\textit{(b) Write pseudocode to implement the naive polynomial-evaluation algorithm that
  computes each term of the polynomial from scratch. What is the running time
  of this algorithm? How does it compare with Horner?}

\begin{function}
  \caption{Naive-poly-eval (A, n, x)}
  $p = 0$\;
  \For {$i = 0 \to n$} {
    $pw \leftarrow 1$ \;
    \For{$j = 0 \to i$} {
      $pw = pw * x$\;
    }
    $p = A[i] * pw + p$\;
  }
  \Return{p}\;  
\end{function}

Worst case running time of this algorithm is a triangular number (beccause of the inner for),
therefore it is $\Theta(n^2)$.

With a Horner it comperes badly.

\textit{(c) Consider the following look invariant for the procedure Horner: }

\textit{At the start of each iteration of the foor loop of lines 2-3,}
$$p = \sum_{k = 0}^{n - (i + 1)}{A[k + i + i] * x*k}$$

\textit{Interpret a summation with no terms as equating 0. Use this loop invariant to
show taht, at terminatiom, $p = \sum{A[k] * x*k}$.}

\textbf{Initialization: }
Since $p = 0$, we can follow that the loop invariant holds for the case before the initialization.

\textbf{Maintenence: }
Following the Horner rule definition, we can see taht after completing another
loop cycle, we get to the next parenthesis in the Horner rule's parenthesization. Thus
we've got the desired result at the output.

\textbf{Termination: }
Algorihm terminates, since we don't modify $i$ in the loop itself. Thus, according
to our loop invariant, this procedure indeed returns the $\sum{A[k] * x*k}$, as desired.

(Not sure that this is right, but got no intentions to do a proper one.)

\subsection{}

\textit{(a) List the five inversions of the array $\{2. 3, 8, 6, 1\}$}

$$(6, 1), (8, 1), (3, 1), (2, 1), (8, 6)$$

\textit{(b) What array with elements from the set $\{1, 2, ... n\}$ has the most
  inversions? How many does it have?}

I think that the reversel sorted one gets the cake. It has a triangular number of inversions
(obvious, when counting from the end).

\textit{(c) What is the relationship between the running time of the insertion sort and the
  number of inversions in the input array? Justify your answer.}

It's proportional to the number of inversions, since it essentially corrects them one by one
in decreasing order of the second number of the pair.

\textit{(d) Give an algorithm that determines the number of inversions in any permutation
  on n elements in $\Theta(n \lg(n))$ worst-case time.}

TODO on a later date

\chapter{Characterizing Running Times}

\section{O-notation, $\Omega$-notation, and $\Theta$-notation}

\subsection{}

\textit{Modify the lower-bound argument for insertion sort to handle input sizes that are
  not necessarily a multiple of 3}

We need to remove at most 2 inputs in order to make it so that the input size is a multiple of 3,
for which our arument holds. Now we can follow that
at least$(n - 1)/3$ elements need to move $(n - 1)/3$ positions, therefore making
$$((n - 1)/3)^2 = (n^2 - 2n + 1)/9 \in \Theta(n^2)$$
iterations

\subsection{}

\textit{Using reasoning similar to what we used for insertion sort, analyze the running time of
  the selection sort algorithm}

In order to make everything happen in selection sort we also solving inversions one by one,
therefore making the trip in the middle $n/3$ times, thus making the whole algorithm
run in $\Theta(n^2)$.

\subsection{}

\textit{Suppose that $\alpha$ is a fraction in the range $0 < \alpha < 1$. Show how to
  generalize the lower-bound argument for insertion sort to consider an input in which the
  $\alpha n$ largest values start in the first $\alpha n$ positions. What additional restriction
  do you need to put on $\alpha$? What value of $\alpha$ maximizes the number of times the
  $\alpha n$ largest values must pass through each of the middle $(1 - 2\alpha)n$ array positions?}

Suppose that we start with $\alpha n$ positions in start, then we need to pass through
$(1 - 2\alpha)n$ positions $\alpha n$ times, effectively making that our algorithm runs in
at least
$$(1 - 2\alpha)n \alpha n = an^2 + 2 a^2 n^2$$
time. Additional restriction on $\alpha$ is that $\alpha < 1/2$, because we need to have a middle
part to begin with.

\section{Asymptotic notation: formal definitions}

\subsection{}

\textit{Let $f(n)$ and $g(n)$ be assymptotically nonnegtive functions. Using the basic
  definitions of $\Theta$-notation, prove that $\max\{f(n), g(n)\} = \Theta(f(n) + g(n))$}

Suppose that $n \geq n_0$. Then pick $c_1 = 1/2$ and $c_2 = 1$. Then we can follow that
if $f(n) \geq g(n)$, then
$$c_1(f(n) + g(n)) = c_1 f(n) + c_1 g(n) = 1/2 f(n) + 1/2 g(n) \leq 1/2 f(n) + 1/2 f(n) =
f(n) \leq f(n)$$
and
$$c_2(f(n) + g(n)) = c_2 f(n) + c_2g(n) \geq c_2 f(n) + c_2f(n) = 2f(n) \geq f(n)$$
if $g(n) \geq f(n)$, then the same thing applies, but with renamed functions.
Thus we can follow that 
$$ c_1(f(n) + g(n)) \leq \max \{f(n), g(n)\} \leq c_2(f(n) + g(n))$$
from which we follow that
$$\max \{f(n), g(n)\} \in \Theta(f(n) + g(n))$$
as desired.

\subsection{}

\textit{Explain why the statement "The running time of algorithm $A$ is at least $O(n^2)$" is
  meaningless.}

Because $O$ is an upper bound, therefore it can't be more then an upper bound.

\subsection{}

\textit{Is $2^{n + 1} = O(2^n)$? Is $2^{2n} = O(2^n)$?}

$\frac{2^{n + 1}}{2^n} = 2$, thus we follow that $2^{n + 1} \in \Theta(2^n)$.
$\frac{2^{2n}}{2^n} = 2^n$, which is unbounded, thus $2^{2n} \in \omega(2^n)$ and therefore
$2^{2n} \notin O(2^n)$. (Although trichotomy does not hold in general, we can follow that
if one is true, then the rest is false.)

\subsection{}

\textit{Prove 3.1}

3.1 states that for any two functions $f(n)$ and $g(n)$ we have $f(n) = \Theta(g(n))$ if and only
if $f(n) = O(g(n))$ and $f(n) = \Omega(g(n))$.

We've got forward direction directly from definitions. We're kind of have backward also from
defitnitions, if we firsly let
$$n_0 = \max\{n_{0, \Omega},n_{0, O}\}$$

\subsection{}

\textit{Prove that the running time of an algorithm is $\Theta(g(n))$ if and only if its
  worst-case running time is $O(g(n))$ and its best-case running time is $\Omega(g(n))$}

Forward direction follows directly from 3.1 (best-case and worst-case are both running times,
therefore 3.1 applies).

For the backward direction let $q_1(n)$ be best-case running time and $q_2(n)$ be the worst-case.
Then by the fact that $q_1 \in O(g(n))$ it follows that there exist $c_1$ and $n_1$ such that
$$q(n) \leq c_1 g(n)$$
for $n > n_0$
By the fact that $q_2(n) \in \Omega(g(n))$ we follow that there exist $c_2$ and $n_2$ such that
for $n > n_2$
$$c_2 g(n) \leq q_2(n)$$
Suppose that $f(n)$ is a running time of an algorithm. Then we follow that for every $n$
$$q_2(n) \leq f(n) \leq q_1(n)$$
thus for $n \geq \max\{n_1, n_2\}$ we've got that
$$c_2 g(n) \leq q_2(n) \leq f(n) \leq q_1(n) \leq c_1 g(n)$$
$$c_2 g(n) \leq f(n) \leq c_1 g(n)$$
thus we follow that $f(n) \in \Theta(g(n))$, therefore running time of an algorithm is
$\Theta(g(n))$. (I ommited zeroes here, but they are assumed)

\textit{Prove that $o(g(n)) \cap \omega(g(n))$ is the empty set}

Suppose that it isn't. Then we follow that there exists $f(n)$ such that
$$f(n) \in o(g(n))$$
$$f(n) \in \omega(g(n))$$

Then let $c > 0$ and let $x = \max\{n_o + n_\omega\}$, where $n_o$ and $n_\omega$ are numbers,
where $o$ and $\omega$ start kicking in respectively. Then we follow that
$$f(x) > g(x)$$
and
$$f(x) < g(x)$$
which is impossible, thus giving us a contradiction. Therefore we follow that
$$o(g(n)) \cap \omega(g(n)) = \emptyset$$
as desired.

\subsection{}

Gonna skip this one, as it's tedious, but pretty self-explanatory.


\section{Standard notations and common functions}

\subsection*{3.3-1}

\textit{Show that if $f(n)$ and $g(n)$ are monotonically increasing functions, then so are
  the functions $f(n) + g(n)$, $f(g(n))$, and if $f(n)$ and $g(n)$ are in addition nonnegative,
  then $f(n) g(n)$ is monotonically increasing.
}

Suppose that $x_1 \leq x_2$. Then $f(x_1) \leq f(x_2)$ and $g(x_1) \leq g(x_2)$. Therefore
$$f(x_1) + g(x_1) \leq f(x_2) + g(x_1) \leq f(x_2) + g(x_2)$$
Because $g(x_1) \leq g(x_2)$ we follow that
$$f(g(x_1)) \leq f(g(x_2))$$
and if they are nonnegative then
$$f(x_1) g(x_1) \leq f(x_2) g(x_1) \leq f(x_1) g(x_2)$$
as desired.

\subsection*{3.3-2}
\textit{Prove that $\lfloor an \rfloor + \lceil (1 - a)n \rceil = n$ for any integer $n$ and
  any real numner $a$ in range $0 < a < 1$}
$$\lfloor an \rfloor + \lceil (1 - a)n \rceil =
\lfloor an \rfloor + \lceil n - an \rceil =
\lfloor an \rfloor + n +  \lceil -   an \rceil =
\lfloor an \rfloor + n -  \lfloor an \rfloor = n 
$$
as desired.

\subsection*{3.3-3}

\textit{Use equation (3.14) or other means to show that $(n + o(n))^k = \Theta(n^k)$ for
  any real constant $k$. Conclude that $\lceil n \rceil^k = \Theta(n^k)$ and
  $\lfloor n \rfloor^k = \Theta(n^k)$.}

% $$1 + n + o(n) \leq e^{n + o(n)}$$
% $$\lg(1 + n + o(n)) \leq n + o(n)$$
% $$\lg(1 + n + o(n)) \leq n + o(n)$$

% $$c_1  n^k  \leq (n + o(n))^k \leq c_2 n^k$$
% $$\lg(c_1  n^k)  \leq \lg((n + o(n))^k) \leq \lg(c_2 n^k) $$
% $$\lg(c_1) +  \lg(n^k)  \leq k \lg(n + o(n)) \leq \lg(c_2) + \lg( n^k) $$
% $$\lg(c_1) +  k \lg(n)  \leq k \lg(n + o(n)) \leq \lg(c_2) + k \lg( n) $$
% $$\lg(c_1) \leq k \lg(n + o(n)) - k \lg(n) \leq \lg(c_2) $$
% $$\lg(c_1) \leq k (\lg(n + o(n)) -  \lg(n)) \leq \lg(c_2) $$
% $$\lg(c_1) \leq k (\lg(\frac{n + o(n)}{n}))  \leq \lg(c_2) $$
% $$\lg(c_1) \leq k (\lg(\frac{n}{n} + \frac{o(n)}{n}))  \leq \lg(c_2) $$


We know that for $f \in o(n)$, it is true that 
$$\lim_{n \to \infty}{\frac{f(x)}{n}} = 0$$
Since this functions has a limit, we follow that it is bounded. Thus there exists $M > 0$
such that
$$|\frac{f(x)}{n}| < M$$
$$-M < \frac{f(x)}{n} < M$$
since we're talking about non-negative functions, we can follow that
$$0 \leq \frac{f(x)}{n} \leq M$$
thus
$$0 \leq \frac{f(x)}{n} \leq M$$
$$1 \leq 1 + \frac{f(x)}{n} \leq 1 +  M$$
$$\lg(1) \leq \lg(1 + \frac{f(x)}{n}) \leq \lg(1 +  M)$$
$$\lg(1) \leq \lg(\frac{n}{n} + \frac{f(x)}{n}) \leq \lg(1 +  M)$$
$$\lg(1) \leq \lg(\frac{n + f(x)}{n}) \leq \lg(1 +  M)$$
if $k > 0$ (if $k \leq 0$, then we either have a trivial case, or the logic is the same)
$$k \lg(1) \leq k \lg(\frac{n + f(x)}{n}) \leq k \lg(1 +  M)$$
$$k \lg(1) \leq k (\lg(n + f(x)) - \lg(n)) \leq k \lg(1 +  M)$$
$$k \lg(1)  + k \lg(n) \leq k (\lg(n + f(x))  \leq k \lg(1 +  M) + k \lg(n)$$
let $c_1 = 1^{1/k}$ and $c_2 = (1 +  M)^{1/k}$
$$k \lg(c_1^{1/k})   + k \lg(n) \leq k (\lg(n + f(x))  \leq k \lg(c_2^{1/k}) + k \lg(n)$$
$$k (\lg(c_1^{1/k})   +  \lg(n)) \leq k (\lg(n + f(x))  \leq k (\lg(c_2^{1/k}) + \lg(n))$$
$$k (\lg(c_1^{1/k}n)) \leq k (\lg(n + f(x))  \leq k (\lg(c_2^{1/k}n))$$
$$\lg(c_1n^k) \leq \lg((n + f(x))^k)  \leq \lg(c_2n^k)$$
$$c_1n^k \leq (n + f(x))^k  \leq c_2n^k$$
thus we follow that
$$f \in o(n) \to (n + f(x))^k \in \Theta(n^k)$$

We can follow that
$$ \pm 1 \in o(n)$$
therefore 
$$\lfloor n \rfloor^k  = (n + o(n))^k$$
$$\lceil n \rceil^k  = (n + o(n))^k$$
therefore
$$\lfloor n \rfloor^k = \Theta(n^k)$$
$$\lceil n \rceil^k  = \Theta(n^k)$$
as desired.

\subsection*{3.3-4}

\textit{Prove the following: }

\textit{(a) Equation (3.21)}

$$a^{\log_b{c}} = c^{\log_b{a}}$$
$$\log_a{a^{\log_b{c}}} = \log_a{c^{\log_b{a}}}$$
$$\log_b{c} = \log_b{a} \log_a{c}$$
$$\log_b{c} = \log_b{a} \frac{\log_b{c}}{\log_b{a}}$$
$$\log_b{c} = \log_b{c}$$
$$c = c$$
as desired.

\textit{(b) Equations (3.26 - 3.28)}


$$\frac{n!}{n^n} = \prod_{i = 1}^n{\frac{i}{n}} = \frac{1}{n} \prod_{i = 2}^n{\frac{i}{n}}$$
Since the part under $\prod$ is a product of numbers $0 \leq x \leq 1$, we follow that
$$\frac{1}{n} \prod_{i = 2}^n{\frac{i}{n}} \leq \frac{1}{n}$$
and therefore 
$$\lim_{n \to \infty}{\frac{n!}{n^n}} = 0$$
thus
$$n! = o(n^n)$$

$$\frac{n!}{2^n} = \prod_{i = 1}^{n}{\frac{i}{2}} =
\frac{1}{2} \frac{n}{2} \prod_{i = 2}^{n - 1}{\frac{i}{2}} =
\frac{n}{4} \prod_{i = 2}^{n - 1}{\frac{i}{2}} = 
$$
part under $\prod$ is greater or equal to $1$, therefore
$$\frac{n}{4} \prod_{i = 2}^{n - 1}{\frac{i}{2}} \geq \frac{n}{4}$$
which converges to infinity. Thus
$$\lim_{n \to \infty}{\frac{n!}{2^n}} = \infty$$
therefore
$$n! = \omega(2^n)$$

TODO


\subsection*{3.3-8}

GOTO linear algebra, 5.3.14

\section{Problems}

\subsection{}

Everything follows from this crudely-derived limit
$$\lim_{n \to \infty}{\frac{\sum_{j = 1}^d {a_j n^j}}{n^k}} = a_d n^{d - k} $$
where $d, k \in N \cup \{0\}$.

\subsection*{3.4.5}

\textit{(a)}

We follow that
$$\Theta(\Theta(f(n)))$$
is a dumpster fire of an expression, because it is nothing more that an artifact of
our abusal of notation.

\textit{(b) }

Let $f \in \Theta(h)$ and let $g \in O(h)$. We follow that there exists constantst
$c_1, ...$ such that 
$$ c_1 h(n) \leq f(n) \leq c_2 h(n)$$
$$ g(n) \leq c_3 h(n)$$
Since $g$ is positive, we follow that
$$0 \leq  g(n) \leq c_3 h(n)$$
and thus
$$ c_1 h(n) + 0 \leq g(n) + f(n) \leq (c_2 + c_3) h(n)$$
tthus we follow that $f + g \in \Theta(h)$, which can be translated to the given expression.

\textit{(c)}

Suppose that $f \in \Theta(h_1)$ and $g \in \Theta(h_2)$.
We follow that there are $c_1, c_2, c_1', c_2'$ such that for $n \geq n_0$ we've got 
$$c_1 h_1(n) \leq f(n) \leq c_2 h_1(n)$$
$$c_1' h_2(n) \leq g(n) \leq c_2' h_2(n)$$
thus we follow that
$$c_1' h_2(n) + c_1 h_1(n) \leq f(n) + g(n) \leq c_2 h_1(n) + c_2' h_2(n)$$
Let $q_1 = \min\set{c_1', c_1}, q_2 = \max\set{c_2', c_2}$. We then follow that
$$ c_1' h_2(n) + c_1 h_1(n) \geq q_1 h_2(n) + q_1 h_1(n) = q_1(h_2(n) + h_1(n))$$
simular result holds for the upper bound as well. Thus we can conclude that
$$f \in \Theta(h_1) \land g \in \Theta(h_2) \ra f + g \in \Theta(h_1 + h_2)$$

TODO: finish those exercises (maybe)


\chapter{Divide-and-Conquer}

\section{Multiplying square matrices}

\subsection*{4.1-1}

\textit{Generalize Matrix-Multiply-Recursive to multiply $n \times n$ matrices for which
  $n$ is not necessarily an exact power of $2$. Give a recurrence describing its running
  time. Argue that it runs in $\Theta(n^3)$ time in the worst case.}

Given that the product of two block-diagonal matrices is the block-diagonal matrix with the
consecutive products of the blocks on the diagonal, we can follow that we can
pad a given matrix with a row and a column of zeroes, in case given matrix has an odd number
of rows/columns. 

Then we follow that the running time of produced algorithm at maximum is
$$T'(n) = 8 (T'((n + 1) / 2)) + \Theta(1)$$
We follow that for powers of $2$ we've got that $T(n) = T'(n)$, which kinda implies that
$T' \in \Theta(n^3)$.

\textit{Skipping the rest for now}

\section{Strassen's algoruthm for matrix multiplication}

\section{The substitution method for solving recurrences}

\subsection{}

\textit{Use the substitution method to show that each of the following recurrences defined
  on the reals has the asymptotic solution specified.}

\textit{(a) $T(n) = T(n - 1) + n$ has solution $T(n) = O(n^2)$.}

We can do this thing without recurrence, and show that a function $f(x) = \frac{(n + 1)n}{2} + T(0)$
satisfies given recurrence, but let's be a bit more dilligent.

We follow that
$$T(n) \leq c (n - 1)^2 + n = cn^2 - 2cn + 1 + n \leq cn^2$$
provided that $c \geq 1$ and $n \geq 1$. 

\end{document}

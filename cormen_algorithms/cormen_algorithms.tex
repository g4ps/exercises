\documentclass[11pt,oneside,titlepage]{book}
\title{My algorithms exercises}
\usepackage{amsmath,amssymb}
\usepackage{geometry}
\usepackage[linesnumbered, ruled]{algorithm2e}
\usepackage{hyperref}
\author{Evgeny Markin}
\date{2023}

\begin{document}
\maketitle
\tableofcontents

\chapter*{Preface}
Exercises for Introduction to Algorithms by Cormen et al., 4th ed. It has
exercises, that should be written down, moslty in math and whatnot.

Some of the exercises require that you code something (sometimes it's not
explicitely required, but that would be nice to code it anyways), and this
code is presented in the progs folder. Everything is written in C, because I'm most
familiar with it. 

Pseudocode is written by using package \textbf{algorithm2e}, which does not really correspond
to the one, that is used in the book, but it still does the job.

Same rules as usual apply, if you want to use this book for any reason -- go right ahead, it's
free, just be aware that it is full of mistakes.

\part{Foundations}

\chapter{The Role of Algorithms in Computing}

\section{Algorithms}

\subsection{}

\textit{Describe your own real-world example that required sorting. Describe one
  that required finding the shortest distance between two points.}

I've needed both when I was creating 8-puzzle program

\subsection{}

\textit{Other than speed, what other measures of efficiency migh you need to consideer
  in a real-world setting?}

Memory and parallellability.

\subsection{}

\textit{Select a data structure that you've seen, and discuss its strengths and limitations.}

Linked lists. They are perfect in everything, apart from sorting; but even then you can define
any data structure through linked lists, which makes them just perfect (especially
omnidirectional ones).

\subsection{}

\textit{Suggest a real-world problem in which only the best solution will do. Then come up
  with one in which "approximately" the best solution is good enough.}

Sorting has to be perfect, otherwise it's borderline useless. Estimated time to
complete the task can tolerate imperfections.

\subsection{}

\textit{Describe a real-world problem in which sometimes the entire input is available
  before you need to solve the problem, but other times the input is not
  entirely available in advance and arrives over time}

Traffic on maps does this thing. Sometimes you have all the input, sometimes it changes.


\section{Algorithms as a technology}

\subsection{}

\textit{Give an example of an application that requires algorithmic content at the
  application level, and discuss the function of the algorithms involved.}

Path finding on maps will do. It requires to triverse graphs and whatnot.

\subsection{}

\textit{Suppose that for inputs of size $n$ on a particular computer, insertion sort
  runs in $8n^2$ steps and merge sort runs in $64n \lg n$ steps. For which values of $n$
  does insertion sort beat merge sort?}

For
$$8 n^2 < 64 n \lg n$$
$$n <8 \lg n$$
$$\frac{n}{lg n} < 8$$
$$n \approx 44$$
cases.


\subsection{}

\textit{What is the smallest value of $n$ such that an algorithm whose running time is $100n^2$
  runs faster than an algorithm whose running time is $2^n$ on the same machine?}

Calculator says 15

\chapter{Getting Started}


\section{Inserition sort}

\subsection{}

\textit{Using Figure 2.2 as a model, illustrate the operation of Insertion-Sort on an
  array initially containing the sequence $[31, 41, 59, 26, 41, 58]$}
$$[31, 41, 59, 26, 41, 58]$$
$$[26, 31, 41, 59, 41, 58]$$
$$[26, 31, 41, 41, 59, 58]$$
$$[26, 31, 41, 41, 58, 59]$$

\subsection{}

\textit{State loop invariant for the Sum-Array procedure.}

At the start of each iteration of the for loop, the $sum$ variable contains the sum of elements
in $A[1: i]$.

\textbf{Initialization: }

Firstly, we've got 0 as the sum. Given that we've summed 0 elements so far, we can conclude
that this is indeed a correct value to set it.

\textbf{Maintenance: }

For each iteration of $i$ we've got that we add a i'th element from the array to our sum and
incrementing $i$. Thus before iterating through $i$ we had a sum of all of the elements
before $i$, and after iterating through it we create a sum of elements before $i$ and
the $i$'th element as well. Thus the sum after iterating through $i$ is correct.

\textbf{Termination: }

Given that the array is finite, we follow that because we are incrementing $i$ at each
iteration the algoritm will terminate. Because we increment through elements, we follow that
we've added every element of the array to the sum at the point of termination.

\subsection{}

\textit{Rewrite the Insertion-Sort procedure to sort into monotonically decreasing instead
  of monotonically increasing order.}

Done it in the progs section; long story short: reverse the ordering function in the inner loop,
replace  $A[j] > key$ with $A[j] < key$.

\subsection{}

\textit{Consider the seqrching problem}

\textit{Input: A sequence of $n$ numbers $[a_1, ... a_n]$ stored in array $A[1:n]$ and a
  value x.}

\textit{Output: An index $i$ such that $x$ equals $A[i]$ or the special value $NIL$ if $x$
  does not appear in $A$.}

\textit{Write pseudocode for linear search, which scans through the array from beginning to
  end, looking for $x$. Using a loop invariant, prove that your algorythm is correct.
  Make sure that your loop invariant fufills the three necessary properties.}

\begin{function}
  \caption{Linear-search (A, x)}
  \For{($i = 1 \to n$)}{
    \If{$A[i] = x$} {
      \Return {i} \;
    }
  }
  \Return{NIL}\;

\end{function}

At each iteration of the for loop, we've got that elements $A[1:i]$ do not contain $x$.

\textbf{Initialization: }
Null case is when we haven't gone through any of the elements yet;
because we haven't searched anything, we
can follow that we haven't found anything, thus the base case is correct

\textbf{Maintenance: }
Suppose that $i = j + 1$. Then we follow that $A[1:j]$ does not contain our element by our
induction hypothesis (not sure that we can use this kind of language here, but it's
my book and I can do whatever I want). Then $i$'th element is cheched for the necessary
equality and returned in case of the equality; otherwise we increment $i$ and make it so
$A[1: j + 1]$ is the array of processed values. Thus before and after the loop iteration
we have a correct solution.

\textbf{Termination: }
We terminate either after going through every element, or some time before it.

Example of this thing in C is presented in progs directory


\subsection{}

\textit{Consider the problem of adding two $n$-bit binary integers $a$ and $b$, stored in
  two $n$-element arrays $A[0: n - 1]$ and $B[0: n - 1]$, where each element is either 0 or 1,
  $a = \sum_{n = 0}^{n - 1}A[i] * 2^i$ and $b = \sum_{n = 0}^{n - 1}B[i] * 2^i$. The
  sum $c = a + b$ of the two integers should be stored in binary form in an $(n + 1)$-element array
  $C[0:n]$, where   $c = \sum_{n = 0}^{n - 1}C[i] * 2^i$. Write a procedure Add-Binary-Integers
  that takes an input arrays $A$ and $B$, along with the length $n$, and returns array $C$ holding
  the sum.}


\begin{function}
  \caption{Add-Binary-Integers (A, B, n)}
  define C[0: n + 1] and fill it with zeroes\;
  \tcc{carry stores overflow from the previous iteration} 
  $carry \leftarrow 0$\;
  \For{($i = 0 \to n + 1$)}{
    \tcc{Initializing temporary variable with carry bit; we need to sum carry, A[i], and B[i],
    so we can just initialize temp with carry}
  $r \leftarrow carry$ \;
  \tcc{if it is not the last bit, where A nor B are not defined; We can just define it to be zero
  with the same result}
    \If{$i \neq n + 1$} {
      $r \leftarrow A[i] + B[i] + r$ \;
    }
    \tcc{If we've got an overflow as the result, set carry bit and result of summation
      appropriately  }
    \eIf{$r > 1$}{
      $carry \leftarrow 1$ \;
      $r \leftarrow r \% 2$\;
    }{
      \tcc{otherwise zero the carry bit}
      $carry \leftarrow 0$\;
    }
    \tcc{lastly, put the result of the partial summation into the resulting array}
    $C[i] \leftarrow r$\;
  }
  \Return{C}\;

\end{function}

\section{Analyzing algorithms}

\subsection{}

\textit{Express the function $n^3/1000 + 100n^2 - 100n + 3$ in terms of $\Theta$-notation}

$$n^3/1000 + 100n^2 - 100n + 3 \in \Theta(n^3)$$

\subsection{}

\textit{Consider sorting $n$ numbers strored in array $A[1: n]$ by first finding the smallest
  element of $A[1:n]$ and exchanging it with the element in $A[1]$. Continue in this manner for
  the first $n - 1$ elements of $A$. Write a pseudocode for this algorithm, which is known as
  selection sort. What loop invarian does this algorithm maintain? Why does it need to run
  for only the first $n - 1$ elements, rather than for all $n$ elements? Give the worst-case
  running time fo selection sort in $\Theta$-notation. Is the best-case running time
  any better?}


\begin{function}
  \caption{Selection-Sort (A, n)}
  \For{$i = 1 \to n - 1$} {
    $minPos \leftarrow i$\;
    \For {$j = i \to n$} {
      \If {$A[j] < A[i]$} {
        $minPos \leftarrow j$
      }
    }
    $temp \leftarrow A[i]$\;
    $A[i] \leftarrow A[minPos]$\;
    $A[j] \leftarrow temp$\;
  }
\end{function}

(this function in C is located in progs )

Algorithm maintains that every $A[1:i]$ is sorted and any element in  $A[i + 1:n]$ is greater
or equal to every element of $A[1:i]$. It needs to run only for $n - 1$ times, because the last
element $n$ will be greater or equal then any element in $A[1:n - 1]$ and therefore will
be places in its rightful place.

Worst-case running time of this algorithm is $\Theta(n^2)$, because we compute that
coefficient for $c_4$ is $\sum_{i = 1}^n i = \Theta(n^2)$, while all the other coeffitients
are linear.

Given coefficient for $c_4$ is not changed depending on the input, we follow that
the best running time and worst running time are the same

\subsection{}

\textit{Consider linear search again. How many elements of the input array need to be checked on
  the average, assuming that the element being searched for is equaly likely to be any element
  in the array? How about the worst case? Using $\Theta$-notation, give the average-case and
  worst-case running times of linear search. Justify your answers.}

The low-hanging answer here is that we need to check the first half of the array in order to
find the needed input. It is wrong though.

Assuming that $A[i] = x$ with probability $p$, we follow that we need to check first
$1/p$ elements in order to get the desired element (I can be wrong here, I'm not good with
probabilities).

Assuming that I'm right about it, we follow that $pn \in \Theta(n)$ for the average case.

The worst case is if $A$ does not contain $x$, in which case we've got the
running time of $n$. It is also in $\Theta(n)$.

\subsection{}

\textit{How can you modify any sorting algorithm to have a good best-case running time?}

We can throw a check for the case if $A$ is sorted from the start in the start of the
sorting algorithm. Thus we can follow, that the best case for any such algorithm is
when we get the sorted input, and we get linear best-case running time.

\section{Designing algorithms}

\subsection{}

\textit{Using Figure 2.4 as a model, illustrate the operation of merge sort on an array
  initially containing the sequence }
$$[3, 41, 52, 26, 38, 57, 9, 49]$$
$$[3, 41, 52, 26] [ 38, 57, 9, 49]$$
$$[3, 41] [52, 26] [38, 57] [9, 49]$$
$$[3] [41] [52] [26] [38] [57] [9] [49]$$
$$[3, 41] [26, 52] [38, 57] [9, 49]$$
$$[3, 26, 41,  52] [9, 38, 49, 57]$$
$$[3, 9, 26, 38, 41, 49, 52, 57]$$

\subsection{}

\textit{The test in line 1 of the Merge-Sort procedure reads "if $p \geq r$" rather than
  "if $p \neq r$". If Merge-Sort is called with $p > r$, then the subarray $A[p:r]$ is
  empty. Argue that as long as the initial call of Merge-Sort(A, 1, n) has $n \geq 1$, the
  test "if $p \neq r$" suffices to ensure that no recursive call has $p > r$.}

For a general case when $p \leq r$ we've got that
$$p = 2p/p = (p + p)/2 \leq \lfloor(p + r)/2\rfloor = q$$
thus $p \leq q$ and call on line 4 will be done with the condition that $p \leq q$.

Focusing our attension on $r$, we get that by condition on lines 1 and 2 we follow that
the function returns if $p = r$. Thus we follow that $p \neq r$, and by our assumption
we get that by the line 3 we get if $p < r$. Given that both  $p$ and $r$ are integers,
we follow that
$$(p + r)/2 \leq (p + p + 1)/2  = p + 1/2$$
thus
$$\lfloor(p + r)/2\rfloor \leq  \lfloor p + 1/2 \rfloor = p$$
Therefore we follow that
$$q \leq r - 1$$
$$q + 1\leq r$$
thus we can follow that the call to the Merge-Sort on line 5 happens also with the condition
that $p \leq r$.

Thus we can state that given that $p \leq r$ we can follow that all the other calls
to Merge-Sort, that happen inside of it  will have the same restriction.
Therefore we follow that if initical call to Merge-Sort happens with $n \geq 1$, then the test
"if $p \geq r$" is equivalent to the test "if $p \neq r$".

\subsection{}

\textit{State a loop invariant for the while loop of lines 12-18 of the Merge procedure. Show
  how to use it, along with the while loops 20-23 and 24-27, to prove that the Merge procedure
  is correct.}

At the start of each iteration of the while loop of lines 12-18, the subarray $A[1:k]$ consists
of th elements, originally in $L[1:i]$ or $R[1:j]$ in sorted order and any element
in $A[1:k]$ is less or equal to any element of $L[i + 1, n_L]$ and $R[i + 1, n_R]$.

Base case is trivial, sinse none of the subarrays contain any elements.

For maintenance we've got that we take the lowest element of $L[i:n_L]$ or
$R[j:n_R]$ (which happens to be the lowest  of the  first elements of those subarrays, given that
both of those arrays are sorted), and placing them at $A[k]$. Because of
our restrictions on $L[i + 1, n_L]$ and $R[ + 1, n_R]$ we follow that $A[k]$ will be less or
equal then any element of $L[i + 1, n_L]$ and $R[j + 1, n_R]$. And given that it
is originally from $L[i + 1, n_L]$ or $R[i + 1, n_R]$, we follow that $A[k]$ is
greater or equal then any of $A[1:k - 1]$. Thus we can follow that $A[1:k]$ is sorted.
Thus at the end of the loop we've got our loop invariant.

Termination happens whenever we run out of elements in $L$ or $R$, at which point we still have
our invariant.

We follow then that after our loop, one of the arrays $L$ or $R$ will be empty, and
will contain elements, that are greater or equal then $A[1:k]$. Thus we just append
remaining elements of either of the arrays to $A$.


Given that $L$ and $R$ are two subarrays of $A[k]$, whose disjoint union is $A$, we follow that
merge indeed merges two subarrays of $A[k]$ into one sorted array, as desired.

\subsection{}

\textit{Use mathematical induction to show that when $n \geq 2$ is an exact power of 2, the
  solution of the recurrence }
$$T(n) =
\begin{cases}
  2 \text{ if }n = 2 \\
  2T(n/2) + n \text{ if } n > 2\\
\end{cases}
$$
\textit{is $T(n) = n \lg(n)$.}

Our domain here is $(n_k) = 2^k$

For base case $k = 1 \to n = 2$ we've got
$$T(2) = 2 = 2 * 1 = 2 \lg(2)$$

Our hypothesis is that for $k - 1 < n$ and $n_{k - 1}$ we've got that
$T(n_{k - 1}) = n_{k - 1} \lg(n_{k - 1})$

Thus we follow that if $n$ is an exact power of $2$ and $n \geq 2 > 0$ we've got that
$n/2 < n$. Thus we follow that
$$T(n) = 2T(n/2) + n = 2 n/2 \lg(n/2) = n \lg(n/2) + n = n(\lg(n/2) + 1) =$$
$$= n(\lg(n * 2^{-1}) + 1) =
n(\lg(n) - 1 + 1) = n \lg(n)$$

Therefore for $x \in (n_k)$ we've got that $T(x) = x \lg(x)$, as desired.

\subsection{}

\textit{You can also think of insertion sort as a resursive algorithm. In order to sort
  $A[1:n]$, recursively sort the subarray $A[1:n - 1]$ and then insert $A[n]$ into the
  sorted subarray $A[1: n - 1]$. Write a pseudocode for this recursive version of insertion
  sort. Give a recurrence for its worst-case running time.}

\begin{function}
  \caption{Recursive-insertion-Sort (A, n)}
  \tcc{Ensure termination and sane inputs}
  \If {$n \leq 1$}{
    \Return{}\;
  }
  \tcc{Sort everything in A[1:n - 1]}
  Recursive-insertion-Sort (A, n - 1)\;
  $i \leftarrow n - 1$ \;
  \While{$A[i] > A[n]$ and $i \geq 1$}
  {
    $i \leftarrow i - 1$
  }
  \tcc{Swapping $A[n]$ and $A[i]$}
  $temp \leftarrow A[i]$ \;
  $A[i] \leftarrow A[n]$ \;
  $A[n] \leftarrow temp$ \;
\end{function}

For the worst-case we've got
$$T(n) = T(n - 1) + n$$
which reduces to
$$T(n) = n(n + 1)/2$$


\subsection{}

\textit{Referring back to the searching problem (see Exercise 2.1-4), observe that if the
  subarray being searched is already sorted, the searching algorithm can check the
  midpoint of the subarray against v and eliminate half of the subarray from further
  consideration. The binary search algorithm repeats this procedure, halving the
  size of the remaining portion of the subarray each time. Write pseudocode, either
  iterative or recursive, for binary search. Argue that the worst-case running time of
  binary search is $\Theta(\lg(n))$}

\begin{function}
  \caption{Binary-Sort (A, x, s, f)}
  \If{$s = f$} {
    \Return{NIL}\;
  }
  $q \leftarrow \lfloor (s + f)/2\rfloor$\;
  \eIf{$A[q] = x$} {
    \Return{q};
  }{
    \eIf{$A[q] < x$}
    {
      \Return{Binary-Sort (A, x, q + 1, f)} \;
    }
    {
      \Return{Binary-Sort (A, x, s, q - 1)} \;
    }
  }
\end{function}

For this one we conclude that its worst-case is $\Theta(\lg n)$ by the same logic as in merge,
but we go through only one branch.

\subsection{}

\textit{The while loop of lines 537 of the Insertion-Sort procedure in Section 2.1
  uses a linear search to scan (backward) through the sorted subarray $A[1:j - 1]$.
  What if insertion sort used a binary search (see Exercise 2.3-6) instead of a linear
  search? Would that improve the overall worst-case running time of insertion sort
  to $\Theta(n \lg(n))$?}





\part{Appendix: Mathematical Background}
\chapter{Summations}

\section*{1-1}
\textit{Prove that $\sum_{k  = 1}^n{O(f_k(i))} = O(\sum_{k  = 1}^n{f_k(i)})$}

Short answer:
$$\sum cg(x) = c\sum{g(x)}$$

Long answer:

Suppose that $g \in O(f_k(i))$. It follows that there exists $n_i$ and $c_i$
such that $0 \leq g(n) \leq cf_i(n)$. Thus we can pick
$n = \max\{n_0, n_1, ...\}$ and $c = \max\{c_0, c_1, ... \}$. We know that
both $n$ and $c$ will work all of functions $f_k$. Therefore by
linearity of summations
$$\sum_{k  = 1}^n{O(f_k(i))}
= \sum_{k  = 1}^n{cf_k(i)} =
= c\sum_{k  = 1}^n{f_k(i)} = 
= O(\sum_{k  = 1}^n{f_k(i)})$$
(notation is a little abused and there is nothing is rigorously
proven, but it'll do).

\section*{1-2}
\textit{Find a simple formula for $\sum_{k = 1}^n{(2k - 1)}$.}

$$\sum_{k = 1}^n{(2k - 1)} =
\sum_{k = 1}^n{(2k)} - \sum_{k = 1}^n{(1)} =
2\sum_{k = 1}^n{(k)} - n =
2\frac{n(n + 1)}{2} - n =
n(n + 1) - n =
n^2
$$


\section*{1-3}
\textit{Interpret the decimal number $111,111,111$ in light of equation A.6}

$$111,111,111 = \sum_{k = 0}^{9}{10^k} = \frac{10^{10} - 1}{10 - 1}$$

\section*{1-4}
\textit{Evaluate the infinite series $1 - \frac 1 2 + \frac 1 4 - \frac 1 8
  + \frac{1}{16} - ...$}

The series converges absolutely to 2, so we are free to do anything with it.

$$1 - \frac 1 2 + \frac 1 4 - \frac 1 8
+ \frac{1}{16} - ... = 
\sum_{k = 0}^{\infty}{\frac{1}{2}^{2k}}
- \sum_{k = 0}^{\infty}{\frac{1}{2}^{1 + 2k}} =
\sum_{k = 0}^{\infty}{\frac{1}{2}^{2k}}
- \frac{1}{2}\sum_{k = 0}^{\infty}{\frac{1}{2}^{2k}} =
\left(1 - \frac{1}{2}\right)\sum_{k = 0}^{\infty}{\frac{1}{2}^{2k}} = 
$$
$$
= \left(1 - \frac{1}{2}\right)\sum_{k = 0}^{\infty}{\frac{1}{4}^{k}}
= \left(1 - \frac{1}{2}\right)\frac{1}{1 - \frac{1}{4}}
= \frac 1 2 *  \frac 4 3 = \frac 2 3
$$

\section*{1-5}
\textit{Let $c \geq 0$ be a constant. Show that
  $\sum_{k = 1}^{n}{k^c} = \Theta(n^{c + 1})$}

$$
\sum_{k = 1}^{n}{k^c} = \sum_{k = 1}^{n - 1}{k^c} + n^c =
n^c\sum_{k = 1}^{n}{\frac{k^c}{n^c}} =
$$

Let $f(n) = n^c$.
It can be seen from the graph that
$$\int_{0}^{n}{f(x)dx} \leq \sum_{k = 1}^{n}{k^c} \leq
\int_{0}^{n}{f(x + 1)dx}$$

Thus 
$$\int_0^n{f(x)dx} = \int_0^n{x^c} = \frac{n^{c + 1}}{c + 1} \in$$
$$\int_0^n{f(x + 1)dx} = \int_0^n{(x + 1)^c} = \frac{(n + 1)^{c + 1}}{c + 1}$$

Thus we can state that $\sum_{k = 1}^{n}{k^c} = \Theta(n^{c + 1})$
(I'm not good enough yet to show that $\frac{(n + 1)^{c + 1}}{c + 1} \in
\Theta(n^{c + 1})$, but I'm pretty sure that it's true TODO).



\section*{1-6}
\textit{Show that $\sum_{k=0}^{\infty}{k^2 x^k} = x(1 + x)/(1 - x)^3$ for
  $|x| < 1$}

We know that for $|x| < 1$
$$\sum_{k = 0}^{\infty}{kx^k} = \frac{x}{(1 - x)^2}$$
thus if we differentiate both sides we get
$$\sum_{k = 0}^{\infty}{k^2x^{k - 1}} = \frac{2x}{(1 - x)^3}
+ \frac{1}{(1 - x)^2} $$
and then if we multiply all of it by $x$ we'll get
$$\sum_{k = 0}^{\infty}{k^2x^k} = \frac{2x^2}{(1 - x)^3}
+ \frac{x}{(1 - x)^2} $$
thus if we factor all of this jazz we'll get
$$\sum_{k = 0}^{\infty}{k^2x^k} = - \frac{x(x + 1)}{(x - 1)^3}$$
and if we tuck this minus into denominator we'll get (which we can do because
the power is odd)
$$\sum_{k = 0}^{\infty}{k^2x^k} = \frac{x(x + 1)}{(1 - x)^3}$$
as desired.

\section*{1-7}
\textit{Prove that $\sum_{k = 1}^n{\sqrt{k \lg k}} = \Theta(n^{3/2}\lg^{1/2}n)$}

$$\int \sqrt{k \lg k} = $$

\section*{1-9}
\textit{Show that}
$$\sum_{k = 0}^{\infty}{(k - 1)/2^k} = 0$$

$$\sum_{k = 0}^{\infty}{(k - 1)/2^k} = \sum_{k = 0}^{\infty}{k/2^k} - \sum_{k = 0}^{\infty}{1/2^k} =
\sum_{k = 0}^{\infty}{k/2^k} - 2 = 0$$

$$\sum_{k = 0}^{\infty}{k/2^k} - 2 = 0$$
$$\sum_{k = 0}^{\infty}{k/2^k} = 2$$
$$\sum_{k = 0}^{\infty}{k/2^k - 2} = 0$$
$$\sum_{k = 0}^{\infty}{\frac{k - 2^{k + 1}}{2^k}} = 0$$


\chapter{Sets, Etc.}

\section*{1-1}
\textit{Draw Venn diagrams that illustrate the first of the distributive laws
(B.1)}

TODO, add picture here

\section*{1-2}
\textit{Prove the generalization of DeMorgan's laws to any finite collection
  of sets}

\textit{Copy from real analysis exercises}

Suppose that $x \in \left(\cup_{\lambda \in \Lambda} E_\lambda \right)^c$. It
follows, that $x$ is not in the union of given sets. Therefore there is no
set $E_n$ such that $x \in E_n$ (because if there would be such a set, then $x$
wouldn't be in $\left(\cup_{\lambda \in \Lambda} E_\lambda \right)^c$).
Therefore $x \in \cap_{\lambda \in \Lambda} E_\lambda^c$. Therefore 
$$\left(\cup_{\lambda \in \Lambda} E_\lambda \right)^c \subseteq
\cap_{\lambda \in \Lambda} E_\lambda^c$$

The proof of reverse inclusion is the same as with the forward, but in reverse
order.

$x \in \left(\cap_{\lambda \in \Lambda} E_\lambda \right)^c$ implies that
$x$ is not in every $E_n$. Therefore there exists $x \in E_n^c$ for some $E_n$.
therefore it is in $\cup_{\lambda \in \Lambda} E_\lambda^c$. The proof of
reverse inclusion uses the same argument, but in other direction.

\section*{1-3}
TODO

\section*{1-4}
\textit{Show that the set of odd natural numbers is countable.}

Let us set a function $f: A \to N$, where $A$ denotes the set of
odd natural numbers
$$f(n) = (n + 1) / 2$$
for this function  we've got
$$f^{-1}(n) = 2n - 1$$

Both funcitons are injective and therefore $f$ is bijective. Therefore
we've got a bijective function betweeen $A$ and $N$, therefore
$A \sim N$, therefore it is conuntable, as desired.

\section*{1-5}
\textit{Show that for any finite set $S$, the power set $2^S$ has
  $2^{|S|}$ elements (that is, there are $2^{|S|}$ distinct subsets of $S$).}

\textit{Another copy from real analysis}

This proof is dumb, but intuitive:

Every subset is corresponding to a number in binary system: 0 for excluded,
1 for included. Therefore there exist $2^n$ possible combinations.

For a more concrete proof let's resort to induction.

Base case(s): subsets of $\emptyset$ are $\emptyset$ itseft
($2^0 = 1$ in total). Subsets of
set with one element are $\emptyset$ and set itself ($2^1 = 1$ in total).

Proposition is that set with n elements has $2^n$ subsets.

Inductive step is that for set with $n + 1$ elements can either have or hot
have the $n + 1$'th element. Therefore there exist $2^n + 2^n = 2 * 2^n =
2^{n + 1}$ subsets, as desired.

\section*{1-6}
\textit{Give an inductive definition for an $n$-tuple by extending the
  set-theoretic definition for an ordered pair.}

The tuple is actually just a re-writing of particular set
$$(a_1, a_2, ..., a_n) = \{\{a_1\}, \{a_1, a_2\}, \{a_1, a_2, a_3\} ...
\{a_1, a_2, a_3, ..., a_n\}\}$$



\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:

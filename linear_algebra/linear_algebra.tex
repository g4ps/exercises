\documentclass[11pt,oneside,titlepage]{book}
\title{My linear algebra exercises}
\usepackage{amsmath, amssymb}
\usepackage{geometry}
\usepackage{hyperref}
\author{Evgeny Markin}
\date{2023}

\DeclareMathOperator \map {\mathcal {L}}
\DeclareMathOperator \ns {null}
\DeclareMathOperator \range {range}
\DeclareMathOperator \inv {^{-1}}
\DeclareMathOperator \Span {span}


\begin{document}
\maketitle
\tableofcontents

\chapter*{Preface}

Exercises are from "Linear algebra done right" by Sheldon Axler, 3rd ed.
I've already read this book before and completed some exercises from it.
Right now I want to brush up the material once again, put all the
proofs on a more durable material than paper and to prepare myself to
what's gonna happen afterwards.

\chapter*{Glossary}

FTLM - Fundamental Theorem of Linear Maps 

\chapter{Vector Spaces}
\section{$R^n$ and $C^n$}

\subsection{}
\textit{Suppose $a$ and $b$ are real numbers, not both $0$. Find real nuber
  $c$ and $d$ such that }
$$1/(a + bi) = c + di$$

$$\frac{1}{a + bi} = c + di$$
$$\frac{1}{a + bi} - c - di = 0$$
$$\frac{a - bi}{(a + bi)(a - bi)} = c + di$$
$$\frac{a - bi}{(a^2 + b^2)} = c + di$$
$$\frac{a}{a^2 + b^2} - \frac{b}{a^2 + b^2}i = c + di$$
Thus $c = \frac{a}{a^2 + b^2}$ and $d = -\frac{b}{a^2 + b^2}$

\subsection{}
\textit{Show that }
$$\frac{-1  + \sqrt{3}i}{2}$$
\textit{is a cube root of $1$ (meaning that its cube equals 1)}
$$(\frac{-1  + \sqrt{3}i}{2})^3 =
\frac{(-1  + \sqrt{3}i)^3}{8} =
\frac{(-1  + \sqrt{3}i)(-1  + \sqrt{3}i)^2}{8} =
\frac{(-1  + \sqrt{3}i)(1  - 2\sqrt{3}i - 3)}{8} =
$$
$$
=\frac{(-1  + \sqrt{3}i)(-2  - 2\sqrt{3}i)}{8} =
\frac{2 + 2\sqrt{3}i - 2\sqrt{3}i + 6}{8} =
\frac{8}{8} = 1
$$
as desired.

\subsection{}
\textit{Find two distinct square roots of $i$}

Square root of $i$, I assume, is a number, whose square is equal to $i$.
Suppose that $(a + bi)^2 = i$. It follows that
$$(a + bi)^2 = a^2 + 2abi - b^2$$
So if we set $$a = b = 1/\sqrt{2}$$ this equation holds. Also it holds for
$$a = b = -1/\sqrt{2}$$
maxima seems to agree with me on this one

\subsection{}
\textit{Show that $\alpha + \beta = \beta + \alpha$ for all
  $\alpha, \beta \in \textbf{C}$}

Let $\alpha = a_1 + b_1 i$ and $\beta = a_2 + b_2 i$. It follows
$$\alpha + \beta = a_1 + b_1 i + a_2 + b_2 i = a_2 + b_2 i + a_1 + b_1 i =
\beta  + \alpha$$
as desired.

\subsection{}
\textit{Show that $(\alpha + \beta) + \lambda = \alpha + (\beta + \lambda)$
  for all  $\alpha, \beta, \lambda \in \textbf{C}$}

Let $\alpha = a_1 + b_1 i$ , $\beta = a_2 + b_2 i$, $\lambda = a_3 + b_3 i$.
It follows that 
$$\alpha + (\beta + \lambda)  = a_1 + b_1 i + (a_2 + b_2 i + a_3 + b_3 i) =
(a_1 + b_1 i + a_2 + b_2 i) + a_3 + b_3 i = 
(\alpha + \beta) + \lambda$$

\subsection{}
\textit{Show that $(\alpha \beta) \lambda = \alpha( \beta \lambda)$}

$$\alpha + (\beta + \lambda)  = (a_1 + b_1 i) ((a_2 + b_2 i) + (a_3 + b_3 i)) =
((a_1 + b_1 i)(a_2 + b_2 i)) + (a_3 + b_3 i) = 
(\alpha \beta) \lambda$$

\subsection{}
\textit{Show that for every $\alpha \in \textbf{C}$ there exists a unique
  $\beta \in \textbf{C}$ such that $\alpha + \beta = 0$}

Suppose that there exist two different $\beta_1 \neq \beta_2$ such that
$\alpha + \beta_1 = 0$ and $\alpha + \beta_2 = 0$. It follows that
$$ \beta_1 = \beta_1 + 0 =  \beta_1 + \alpha + \beta_2 = \alpha + \beta_1  + \beta_2 = 0  + \beta_2 = \beta_2$$
which is a contradiction. Therefore there exists only one unique $\beta$.

\subsection{}
\textit{Show that for every $\alpha \in \textbf{C}$ with $\alpha \neq 0$
  there exists a unique $\beta \in \textbf{C}$ such that $\alpha \beta = 1$}

Suppose that it is not true and there exist two different
$\beta_1 \neq \beta_2$ such that
$$\alpha \beta_1 = 1 \textit{ and } \alpha \beta_2 = 1$$
it follows then that 
$$\beta_1 = 1 *  \beta_1 = \alpha \beta_2 \beta_1  =
\alpha \beta_1 \beta_2 = 1 * \beta_2 = \beta_2$$
which is a contradiction. Therefore there exists only one unique $\beta$.

\subsection{}
The rest of the section is the repetition of this kind of stuff.
That is a lot of writing, and not a lot of thinking, so I'll skip it.
I don't ususally like to skip sections, but I have  aa feeling, that I've
completed this thing on paper somewhere, and there is not much reason to
rewrite it here.

\section{Definition of Vector Space}

\subsection{}
\textit{Prove that $-(-v)= v$ for every $v \in V$.}

For $v$ there exists only one $-v$. For $-v$ there exists only one $-(-v))$.

Thus
$$v = v + 0 = v + (-v) + (-(-v)) = 0 + (-(-v)) = -(-v)$$
as desired (idk if it's true, I'm not good at axioms and stuff)

\subsection{}
\textit{Suppose $a \in F, v \in V$, and $av = 0$. Prove that
  $a = 0$ or $v = 0$.}

Suppose that $a \neq 0$, $v \neq 0$ but $av = 0$. It follows that there
exist $1/a$ - multiplicative inverse of $a$. It follows that
$$1/a * av = 1/a * 0$$
$$1v = 0$$
$$v = 0$$
which is a contradiction. Thus either $a = 0$ or $v = 0$.

\subsection{}
\textit{Suppose $v, w \in V$. Explain why there exists a unique $x \in V$
  such that $v + 3x = w$.}

Suppose that there exists $x_1 \neq x_2$ such that
$v + 3x_1 = w$ and $v + 3x_2 = w$. Thus
$$3x_1 = w - v = 3x_2$$
$$x_1 = \frac{1}{3}(w - v) = x_2$$
which is a contradiction.

Same can be stated from the fact that $x$ is a unique additive inverse of
$\frac{1}{3}(v - w)$.

\subsection{}
\textit{The empty set is not a vector space. The empty set fails to satisfy
  only one of the requirements listed in 1.19. Which one?}

Additive indentity. Empty set does not have zero element in it.
BTW $\{0\}$ is a vector space.

\subsection{}
\textit{Show that n the defintition of a vector space (1.19), the additive
  inverse condition can be replaced with the condition that}
$$0v = 0 \textit{ for all } v \in V$$
\textit{Here the 0 on the left side is the number 0, and the 0 on the right
  side is the additive identity of $V$.}

$$0v = 0$$
$$(1 - 1)v = 0$$
$$1v - 1v = 0$$
$$v - v= 0$$
$$v + (- v)= 0$$

\subsection{}
\textit{Let $\infty$ and $-\infty$ denote two distinct object, neither of
  which is in $R$. Define an addition and multiplication on
  $R \cup \{\infty\} \cup \{-\infty\}$ as you could guess from the notation.
  Specifically, the sum and the product of two real numbers is as usual,
  and for $t \in R$ define
}

$$
t\infty =
\begin{cases}
  -\infty \text{ if } t < 0 \\
  0 \text{ if } t = 0 \\
  \infty \text{ if } t > 0 \\
\end{cases}
$$

$$
t(-\infty) =
\begin{cases}
  \infty \text{ if } t < 0 \\
  0 \text{ if } t = 0 \\
  -\infty \text{ if } t > 0 \\
\end{cases}
$$

$$t + \infty = \infty + t = \infty$$
$$t + (-\infty) = (-\infty) + t = (-\infty)$$
$$\infty + \infty = \infty$$
$$(-\infty) + (-\infty) = (-\infty)$$
$$\infty + (-\infty) = 0$$

\textit{Is $R \cup \{\infty\} \cup \{-\infty\}$ a vector space over
  $R$? Explain.}

I don't think that it is.

$$(t + \infty) - \infty = \infty - \infty = 0$$
$$t + (\infty - \infty) = t + 0 = t$$
thus
$$t + (\infty - \infty) \neq (t + \infty) - \infty$$
thus 
$R \cup \{\infty\} \cup \{-\infty\}$ is not associative, therefore it is
not a vector space.

\section{Subspaces}

\subsection{}
\textit{For each of the following subsets of $F^3$, determine whether it is a
  subspace of $F^3$:}

\textit{(a) $\{(x_1, x_2, x_3) \in F^3: x_1 + 2x_2 + 3x_3 = 0\}$}

Yes, it is. $0$ is contained within it.
$$(x_1, x_2, x_3) + (y_1, y_2, y_3) = (x_1 + y_1, x_2 + y_2, x_3 + y_3)$$
therefore
$$x_1 + y_1 + 2(x_2 + y_2) +  3(x_3 + y_3) =
x_1 + 2x_2 + 3x_3 + y_1 + 2y_2 + 3y_3 = 0+ 0 = 0$$
therefore it is closed under addition
$$n(x_1, x_2, x_3) = (nx_1, nx_2, nx_3)$$
$$nx_1 + 2nx_2 + 3nx_3 = n(x_1 + 2x_2 + 3x_3 ) = 0n = 0$$
therefore it is closed under multiplication.

\textit{(b) $\{(x_1, x_2, x_3) \in F^3: x_1 + 2x_2 + 3x_3 = 4\}$}

It's not a subspace, because it does not contain zero.


\textit{(c) $\{(x_1, x_2, x_3) \in F^3: x_1 x_2 x_3 = 0\}$}

It's not a subspace, because
$$(0, 1, 1) + (1, 0, 0) = (1, 1, 1)$$
therefore it's not closed under addition.

\textit{(d) $\{(x_1, x_2, x_3) \in F^3: x_1  = 5x_3\}$}

It's a subspace, proof is the same as in (a), can be seen more clearly when we
rewrite constraint as
$$x_1 = 5x_3 \to x_1 + 0x_2 -5x_3 = 0$$

\subsection{}
\textit{Verify all the assertions in Example 1.35}

\textit{(a) if $b \in F$, then}
$$\{(x_1, x_2, x_3, x_4) \in F^4: x_3 = 5x_4 + b\}$$
\textit{is a subspace of $F^4$ if and only if $b = 0$}

If $b \neq 0$, then $0$ is not an element of this set.

Proving that it's a subspace when $b = 0$ is trivial

\textit{(b) The set of continous real-valued functions on the interval $[0, 1]$
  is a subspace of $R^{[0, 1]}$.}

$(kf) = kf$ by algebraic properties of continous functions.
If $f$ and $g$ are continous, then $(f + g)$ is continous as well by the same
property.
$f(x) = 0$ is continous because it's a constant functions.

By the way, same (probably) applies to a set of uniformly continous functions.

\textit{(c) The set of differentiable real-valued functions on $R$ is a
  subspace of $R^R$.}

Same deal, algebraic proerties imply linearity, adn zero is included.

\textit{(d) The set of differentiable real-valued functions $f$ on the
  interval $(0, 3)$ such that $f'(2) = b$ is a subspace of $R^{(0, 3)}$
  if and only if $b = 0$.}

Same deal as in previous one, $f'(2)$ needs to be equal to zero in order
to include zero. Previous part does not include it, because it does not
have specific restrictions on derivatives being particular values at particular places.

\textit{(e) The set of all sequences of complex numbers with limit $0$ is a
  subspace of $C^{\infty}$.}

Here we can take zero to be $(x_n) = 0$. Linearity is implied by aldebraic
properties of limits of sequences.

\subsection{}
\textit{Show that the set of differentiable real-valued functions $f$ on the
  interval $(-4, 4)$ such that $f'(1) = 3f(2)$ is a subspace of $R^{[-4, 4]}$.}

Zero is included here. Suppose that $f$ and $g$ are functions in given set.
It follows that
$$f'(1) + g'(1) = 3f(2) + 3g(2)$$
$$f'(1) + g'(1) = 3(f(2) + g(2))$$
$$(f + g)'(1) = 3(f + g)(2)$$
thus it's closed under addition.

$$(kf)'(1) = 3(kf)(2)$$
implies
$$kf'(1) = 3kf(2)$$
therefore it's closed under multiplication by scalar.
Therefore we can state that given subset is a vector subspace.

\subsection{}
analogous to previous

\subsection{}
\textit{Is $R^2$ a subspace of the complex vector space $C^2$?}

No, it's not closed under scalar multiplication.

\subsection{}
\textit{(a) Is }
$$\{(a, b, c) \in R^3: a^3 = b^3\}$$
\textit{a subspace if $R^3$?}

Yes. it is. $a^3 = b^3 \to a = b \to a - b = 0$, the rest of proof is trivial.

\textit{(b) Is }
$$\{(a, b, c) \in C^3: a^3 = b^3\}$$
\textit{a subspace if $C^3$?}

I want to say no to this one, example is
$$(1/2 + i\frac{\sqrt{3}}{2}, -1, 0) +
(1/2 - i\frac{\sqrt{3}}{2}, -1, 0) =
(1, -1, 0)$$
thus it's not closed under additon.

\subsection{}
\textit{Give an example of a nonemplty subset $U$ of $R^2$ such that $U$ is
  closed under addition and under additive inverses (meaning $-u \in U$
  whenever $u \in U$), but $U$ is not a subspace of $R^2$}

$Q^2$. On the other thouhgh, $Z$ will do as well.

\subsection{}
\textit{Give an example of a nonempty subset $U$ of $R^2$ such that $U$ is
  closed under scalar multiplication, but $U$ is not a subspace of $R^2$.}

Two lines through origin.

\subsection{}
\textit{A function is called periodic if there exists a positive number
  $p$ such that $f(x) = f(x + p)$ for all $x \in R$. Is the set of
  periodic functions from $R$ to $R$ a subspace of $R^R$? Explain. }

Zero is a periodic function. Set is
certainly closed under scalar multiplication.

Suppose that $f$ and $g$ are both periodic and $f$ has a period of $p1$
and $g$ has a period of $p2$. Thus if $p2/p1 \in I$,
then functions will be constantly out of phase, therefore the set is not
closed under addition. Thus this subset is not a subspace.

\subsection{}
\textit{Suppose $U_1$ and $U_2$ are subspaces of $V$. Prove that the
  intersection $U_1 \cap U_2$ is a subspace of $V$.}

Zero is included in any subspace, therefore zero is included.

Suppose that $u_1, u_2 \in U_1 \cap U_2$. It follows that for $z \in F$
$zu_1 \in U_1$ and $zu_1 \in U_2$ by closure of those two subspaces.
Therefore $zu_1 \in U_1 \cap U_2$ for any scalar, thus the set is
closed under scalar multiplication.

$u_1 + u_2 \in U_1$ and $u_1 + u_2 \in U_2$ by closure under addition for
both subspaces. Thus $u_1 + u_2 \in U_1 \cap U_2$ for any such vectors.
Therefore the set is closed under addition.

Thus the set satisfies all requirements to be a subspace. Therefore it is a
subspace.

\subsection{}
\textit{Prove that the interseection of every collection of subspace of $V$ is
  a subspace of $V$}

Intersection of two subspaces  is a subspace. Therefore by induction
intersection of any finite collection of subspaces is a subspace.

Suppose that $\Lambda$ is an arbitrary collection of subspaces.
Every subspace contains a zero element, therefore
$$0 \in \cap \Lambda$$

Any vector in $\cap \Lambda$ will be closed under scalar multiplication
for every $U \in \Lambda$. Thus, it will be contained in every
$U \in \Lambda$. Therefore it is contained in $\cap \Lambda$.


Any two  vectors in $\cap \Lambda$ will be closed under addition,
for every $U \in \Lambda$. Thus, their sum  will be contained in every
$U \in \Lambda$. Therefore it is contained in $\cap \Lambda$.

Thus $\cap \Lambda$ is a vector space.

\subsection{}
\textit{Prove that the union of two subspaces of $V$ is a subspace of $V$
  if and only if one of the subspaces is contained in the other.}

Suppose that a union of two subspaces $U_1 \cup U_2$
is a subspace of $V$.

Zero is included in eveery subspace, so in case of the union we don't worry
about it.
Scalar multiplication is also trivial, as we are working only with one vector.

Now for the interesting part: addition. Let $u_1, u_2 \in U_1 \cup U_2$.
In case when $u_1, u_2$ are contained only in one subspace we've got
a trivial case. Interesting part comes when $u_1 \in U_1$ and $u_2 \in U_2$.

What we want to prove is that it is impossible to have
$u_1 \in U_1 \setminus U_2$ and $u_2 \in U_2 \setminus U_1$ and we're going to
use contradiction. Suppose that
$u_1 \in U_1 \setminus U_2$, $u_2 \in U_2 \setminus U_1$ and
$u_1 + u_2 \in U_1 \cup U_2$. Thus it must be the case that
$u_1 + u_2 \in U_1$ or $u_1 + u_2 \in U_2$. Suppose that the former is true;
then it follows that $u_1 + u_2 - u_1 = u_2 \in U_1$, which is a contradiction
(same thing happens if we assume the latter). Thus given case is impossible.
Therefore there cannot exist $u_1 \in U_1 \setminus U_2$ and
$u_2 \in U_2 \setminus U_1$. Thus
$U_1 = U_1 \cup U_2$ or $U_2 = U_1 \cup U_2$.

The reverse case is trivial: if we have two subspaces and
one of it is a subset of another, then larger subspace is is subspace.

\subsection{}
\textit{Prove that the union of three subspaces of $V$ is a subspace of
  $V$ if and only if one of the subspaces contains the other two.}

Same thing applies as in previous exercise: zero and multiplication are
trivial.

We are going to proceed with a proof by contradiction, but firstly we want
to state precisely what we want to prove in a first place. We want to state,
that if a union of three subspaces is a subspace, then this union is equal to
one of the subpsaces. So let us start: suppose that the union of three subspaces is not equal to one of the subspaces.

Firstly, we can eliminate the case, when one of the subspaces is a
subset of another subspace, but third isn't, because it will mean that
union of first two subspaces constitues a subspace, and thus we'll default
to result in the previous exercise.

Thus let us assume that none of the subspaces is a subset of another subspace.
Now we've got two cases to sort out: suppose that if we take $u_2 \in U_2$
and $u_3 \in U_3$ we get that
$$u_2 + u_3 \in U_1$$
for every $u_2 \in U_2$ and $u_3 \in U_3$. Then we can follow, by setting
$u_2 = 0$ to the case that
$$\forall u_3 \in U_3 \to u_3 + u_2 \in U_1 \to u_3 + 0 \in U_1 \to
u_3 \in U_1$$
thus $U_3$ is a subset of $U_1$, which raises a contradiction (in our
assumptions that $U_3$ is not a subset of $U_1$  and by extension
for the default 2-subspace case).

The case when $u_2 \in U_2$, $u_3 \in U_3$ and $u_2 + u_3 \notin
U_1 \cup U_2 \cup U_3$ implies that $U_1 \cup U_2 \cup U_3$ is
not a vector space, thus it cannot happen.

The case when $u_2 \in U_2$, $u_3 \in U_3$ and $u_2 + u_3 \notin U_1$
implies that $u_2 + u_3$ is  in $U_2 \cup U_3$. This raises the case that
$U_2$ is a subspace of $U_3$, which is a contradiction.

Thus we can follow that there exists $u_1 \in U_1$ such that it
cannot be represented in terms of vectors from $U_2$ and $U_3$.
Thus we can follow that analogous  vectors   $u_2 \in U_2$ and $u_3 \in U_3$
also exist.

Because we are still assuming that $U_1 \cup U_2 \cup U_3$ we can follow that
$$u_1 + u_2 + u_3 \in U_1 \cup U_2 \cup U_3$$
Thus this sum is bound to be located in one of the $U_1$, $U_2$ or $U_3$.
Let us assume for simplicity of notation that it is located in $U_1$. Then
we can follow that
$$u_1 + u_2 + u_3 - u_1 = u_2 + u_3 \in U_1$$

Suppose that we take $u_2 \in U_2 \setminus (U_3 \cup U_1)$ and
$u_3 \in U_3 \setminus (U_1 \cup U_2)$. It follows that
$u_2 + u_3$ cannot be in either $U_2$ nor in $U_3$ because in this
case we have that
$$u_2 + u_3 - u_2 = u_3 \in U_2$$
which is a contradiction. Thus
$$u_2 + u_3 \in U_1 \setminus (U_2 \cup U_3)$$
let us call it $u_1'$. In the same fashion we can define $u_2'$ and $u_3'$.

Thus $u_1' + u_2' + u_3' \in U_1 \cup U_2 \cup U_3$. Thus it needs to
be in one of $U_1$, $U_2$ or $U_3$. Suppose that it is included in
$U_1$. Then we can follow that
$$u_1' + u_2' + u_3' \in U_1$$
$$u_2' + u_3' \in U_1$$
$$u_1 + u_3 + u_1 + u_2 \in U_1$$
$$2u_1 + u_3  + u_2 \in U_1$$
$$u_3  + u_2 \in U_1$$

TODO

\subsection{}
\textit{Verify the assertion in Example 1.38}

1.38 states that

\textit{Suppose that $U = \{(x, x, y, y) \in F^4: x, y \in F\}$ and
  $W = \{(x, x, x, y) \in F^4: x, y \in F\}$. Then }
$$U + W = \{(x, x, y, z) \in F^4: x, y, z \in F\}$$
\textit{as you should verify}

Let $u \in U$ and $w \in W$. It follows that
$$u = (x_1, x_1, x_1, y_1)$$
$$w = (x_2, x_2, y_2, y_2)$$

Suppose that $q \in U + W$.
It follows that
$$q = (x_1 + x_2, x_1 + x_2, x_1 + y_2, y_1 + y_2)$$
thus we can set $x = x_1 + x_2$, $y = x_1 + y_2$ and $z = y_1 + y_2$
and call it a day.

\subsection{}
\textit{Suppose $U$ is a subspace of $V$. What is $U + U$.}

By properties of vector space, if we take $u_1, u_2 \in U$ then
$$u_1 + u_2 \in U$$
for every $u_1, u_2 \in U$. Thus we can follow that
$$U + U = U$$

\subsection{}
\textit{Is the operation of addition on the subspaces of $V$ commutative? In
  other words, if $U$ and $W$ are subspaces of $V$, is $U + W = W + U$?}

If $q \in U + W$ it follows that there exists $u \in U$ and $w \i W$
such that
$$q = v + w = w + v = q'$$
where $q' \in W + U$. Thus we can follow that $W + U = U + W$.

\subsection{}

\textit{Is the operation of addition on the subspaces of $V$ associative? In
  other words, if $U_1$, $U_2$, $U_3$ are subspaces of $V$, is}
$$(U_1 + U_2) + U_3 = U_1 + (U_2 + U_3)?$$

Yes it is. We can apply the same logic as in the previous exercise and it'll do
the job.

\subsection{}

\textit{Does the operation of addition on the subspaces of $V$ have an additive
  identity? Which subspace have additive inverces?}

Every subspace contains zero, therefore
$$U + 0 = U$$
thus we've got additive identity.

By adding two subspaces together we get a larger subspace, thus we can follow
that the only way to get 0 vector space as the result of addition of two
subspaces is to add
$$0 + 0 = 0$$
thus the only subspace that contains additive inverse is $0$.

\textit{Prove or give counterexample: if $U_1$, $U_2$, $W$ are subspaces of
  $V$, such that }
$$U_1 + W = U_2 + W$$
\textit{then $U_1 = U_2$}

This is wrong: suppose that $U_2$ is a nonzero subspace of $W$ and $U_1 = 0$.
Then it follows that
$$U_1 + W = 0 + W = W = W + U_2$$
and
$$U_1 \neq U_2$$
as desired.

\subsection{}

\textit{Suppose}
$$U = \{(x, x, y, y) \in F^4: x, y \in F\}$$
\textit{Find a subspace $W$ of $F^4$ such that $F^4 = U \bigoplus W$ }

$$W = \{(0, x, y, 0) \in F^4: x, y \in F\}$$


\subsection{}

\textit{Suppose}
$$U = \{(x, y, x + y, x - y, 2x) \in F^5: x, y \in F\}$$
\textit{Find a subspace $W$ of $F^5$ such that $F^5 = U \oplus W$ }

$$W = \{(0, 0, x, y, z) \in F^5: x, y, z \in F\}$$

\subsection{}

\textit{Suppose}
$$U = \{(x, y, x + y, x - y, 2x) \in F^5: x, y \in F\}$$
\textit{Find a thee subspaces $W_1$, $W_2$, $W_3$ of $F^5$
  such that $F^5 = U \oplus W_1 \oplus W_2 \oplus W_3$ }

$$W_1 = \{(0, 0, x, 0, 0) \in F^5: x \in F\}$$
$$W_2 = \{(0, 0, 0, y, 0) \in F^5: y \in F\}$$
$$W_3 = \{(0, 0, 0, 0, z) \in F^5: z \in F\}$$

\subsection{}

\textit{Prove or give a counterexample: if $U_1$, $U_2$, $W$ are subspaces
  of $V$ such that }
$$ V = U_1 \oplus W \text{ and } V = U_2 \oplus W $$
\textit{then $U_1 = U_2$}

This one is false;
$$U_1 = \{(x, x) \in F^2: x \in F\}$$
$$U_2 = \{(x, 0) \in F^2: x \in F\}$$
$$W = \{(0, y) \in F^2: y \in F\}$$

\subsection{}

\textit{A function $f: R \to R$ is called even if }
$$f(-x) = f(x)$$
\textit{for all $x \in R$. A function $f: R \to R$ is called odd if }
$$f(-x) = -f(x)$$
\textit{for all $x \in R$. Let $U_e$ denote the set of real-valued
  even functions on $R$ and let $U_o$ denote the set of real-valued odd
  functions on $R$. Show that }
$$R^R = U_e \oplus U_o$$



Let $f: R \to R$ be arbitrary. It follows that

$$f_e(x) =
\begin{cases}
  2 f(x) - f(-x) \text{ if } x \geq 0 \\
  f(x) \text{ if } x = 0 \\
  2 f(-x) - f(x) \text{ if } x < 0
\end{cases}
$$

Every odd function satisfies $f(0) = 0$.
Therefore for even function we've got to have $f_e(0) = f(0)$


$$
f_e(x) =
\begin{cases}
  a_1 f(x) + b_1 f(-x) \text{ if } x > 0 \\
  a_1 f(-x) + b_1 f(x) \text{ if } x < 0
\end{cases}
$$

$$
f_o(x) =
\begin{cases}
  a_2 f(x) + b_2 f(-x) \text{ if } x > 0 \\
  -a_2 f(-x) - b_2 f(x)  \text{ if } x < 0
\end{cases}
$$

$$
\begin{cases}
  a_1 + a_2 = 1 \\
  b_1 + b_2 = 0 \\
  a_1 - a_2 = 0 \\
  b_1 - b_2 = 1 \\
\end{cases}
$$

$$
\begin{cases}
  a_1  = 0.5 \\
  b_1 = 0.5 \\
\end{cases}
$$

$$
f_e(x) =
\begin{cases}
  1/2 f(x) + 1/2 f(-x) \text{ if } x > 0 \\
  f(x) \text{ if } x = 0 \\
  1/2 f(x) + 1/2 f(-x)  \text{ if } x < 0
\end{cases}
$$

$$
f_o(x) =
\begin{cases}
  1/2 f(x) - 1/2 f(-x) \text{ if } x > 0 \\
  0 \text{ if } x = 0 \\
  -1/2 f(-x) + 1/2 f(x)  \text{ if } x < 0
\end{cases}
$$

Thus
$$f_e(x) = f_e(-x)$$
$$f_o(-x) = -f_o(x)$$
and
$$f_e(x) + f_o(x) = f(x)$$
as desired.

Also, the only function that is odd and even at the same time is $0$,
therefore we've got a direct sum, as desired.


\chapter{Finite-Dimentional Vector Spaces}

\section{Span and Linear Independence}

\subsection{}
\textit{Suppose $v_1, v_2, v_3, v_4$ spans $V$. Prove that the list }
$$v_1 - v_2, v_2 - v_3, v_3 - v_4, v4$$
\textit{also spans $V$.}

Let $v \in V$ be represented as
$$v = a_1 v_1 + a_2 v_2 + a_3 v_3 + a_4 v_4$$

then we can follow that
$$v = a_1 (v_1 - v_2) + (a_2 + a_1) (v_2 - v_3)+ (a_3 + a_2 + a_1) (v_3 - v_4) + (a_1 + a_2 + a_3 + a_4) v_4$$
therefore any $v \in V$ can be represented using given list, therefore
given list spans $V$, as desired.

\subsection{}
\textit{Verify the assertion in Example 2.18}

Suppose that $v \in V$. Then it follows from some exercise in previous
chapter that $a_1 v = 0$ iff $a_1 = 0$ or $v = 0$. Thus if $v \neq 0$ we
can follow that the only way to represent zero is to set $a_1$ to 0. Thus
list is linearly independent.


Suppose that we've got linearly independent list of two vectors. We therefore
can follow that the only way to represent 0 is to set
$a_1 = 0$ and $a_2 = 0$. Thus vectors are not a scalar multiples of each other.
In other directon we've got a trivial case.

For the list
$$v_1 = (1, 0, 0, 0), v_2 = (0, 1, 0, 0), v_3 = (0, 0 1, 0)$$
we've got that
$$v = a_1 v_1 + a_2 v_2 + a_3 v_3 = (a_1, a_2, a_3, 0)$$
therefore the only way to represent zero is to set all of a's into 0.

Same case applies for the last one.

\subsection{}
\textit{Find a number $t$ such that}
$$(3, 1, 4), (2, -3, 5), (5, 9, t)$$
\textit{is not linearly inependent in $R^3$}

The only way that this list is not linearly independent is if 
we can represent last vector as a linear combiination of the other two. Thus
$$
\begin{cases}
  3 a_1 + 2 a_2 = 5
  1 a_1 - 3 a_2 = 9
\end{cases}
$$
$$
\begin{cases}
  3 a_1 + 2 a_2 = 5
  a_1= 9 +  3 a_2
\end{cases}
$$
$$   3 (9 +  3 a_3) + 2 a_2 = 5 $$
$$   27 +  9 a_2 + 2 a_2 = 5 $$
$$ 11 a_2= -22$$
$$ a_2 = -2$$
thus
$$a_1 = 3$$
therefore
$$3 * 4 - 5 * 2 = t$$
$$t = 2$$

\subsection{}
\textit{Verify the assertion in the second bullet point in Example 2.20}

$c = 8$ is the only solution such that third vector is a
scalar multipe of first vector plus scalal multiple of second. Thus we
can follow that the last vector is not in the span of first two, therefore
the list is linearly independent.

\subsection{}
\textit{(a) Show that if we think of $C$ as a vector space over $R$, then the
  list $(1 + i, 1 - i)$ is linearly independent.}

$$(1 + i  + 1 - i)/2 = 1$$
$$(1 + i - 1 + i)/2 = i$$
thus the only way to represent $0$ is to set all of a's to zero

\textit{(b) Show that if we think of $C$ as a vector space over $C$, then
  the list $(1 + i, 1 - i)$ is linearly dependent}

List $(1)$ spans $C$, and its length is less that
the length of given set. THus given set is linearly dependent.


\subsection{}
\textit{Suppose $v_1, v_2, v_3, v_4$ is linearly independent.
  Prove that the list }
$$v_1 - v_2, v_2 - v_3, v_3 - v_4, v4$$
\textit{is also linearly independent.}

As we've shown before, spans of two sets are equal, therefore the only
way to represent $0$ is to put all a's to 0.



\subsection{}
\textit{Prove or give counterexample: If $v_1, v_2, ... v_m$ is a linearly
  independent list of vectors in $V$, then}
$$5v_1 - 4v_2, v_2, v_3, ... v_m$$
\textit{is linearly independent}

Both sets span the same space and have the same length, therefore they are
both linearly independent.

\subsection{}

Trivial, equivalent to previous

\subsection{}
\textit{Prove or give counterexample: If $v_1, ... v_m$ and $w_1, ..., w_m$ are
  linearly independent lists of vectors in $V$, then
  $v_1 + w_1, ..., v_m + w_m$ is linearly independent.}

False: set $w_1 = - v_1$ and get the desired result.

\subsection{}
\textit{Suppose $v_1, ..., v_m$ is linearly independent in $V$ and $w \in V$.
  Prove that if $v_1 + w, v_2 + w, ... v_m + w$ is linearly dependent,
  then $w \in span(v_1, v_2, ... v_m)$.}

Suppose that resulting list is linearly dependent. It follows that there
exists a way to represent 
$$\sum_{n = 1}^m{a_n (v_n + w)} = 0$$
such that not all a's are zeroes. Thus
$$\sum_{n = 1}^m{a_n (v_1 + w)} = \sum_{n = 1}^m{(a_n w + a_n v_n)} =
\sum_{n = 1}^m{a_n w} + \sum_{n = 1}^m{a_n v_n} =
w \sum_{n = 1}^m{a_n} + \sum_{n = 1}^m{a_n v_n} = 0
$$
$$- w \sum_{n = 1}^m{a_n} =  \sum_{n = 1}^m{a_n v_n}$$
$\sum_{n = 1}^m{a_n} \neq 0$, because otherwise left side is zero and
therefore right side is zero, which is not assumed.
$$w  =  \sum_{n = 1}^m{ - \frac{a_n}{\sum_{j = 1}^m{a_j}} v_n}$$
thus $w \in span(v_1, v_2, ... v_m)$, as desired.


\subsection{}
\textit{Suppose $v_1, ..., v_m$ is linearly independent in $V$ and $w \in V$.
  Show that $v_1, ..., v_m, w$ is linearly independent if and only if }
$$w \notin span(v_1, ..., v_m)$$

Because otherwise we've got a bigger linearly independent list, that spans
$V$.

\subsection{}
\textit{Explain why there does not exist a list of six polinomials that is
  linearly independent of $\mathcal{P_4}(F)$.}

Because the list of length 5 spans this space.

\subsection{}
\textit{Explain why no list of four polynomials spans $\mathcal{P_4}(F)$.}

Because the list of length 5 spans this space.

\subsection{}
\textit{Prove that $V$ is infinite-dimentional if and only if there is a
  sequence $v_1, v_2, ... $ of vectors in $V$ such that $v_1, ... v_m$ is
  linearly independent for every possible integer $m$.}

Forward is coming from the fact that we can always add new vectors
to a given linearly independent list of vectors, that are outside of span
of given list.

Because there always exists list that is bigger than
given list and is linearly independent in $V$ we can follow that
no final list of vectors spans $V$, therefore it is infinite-dimentional.


\subsection{}
\textit{Prove that $F^{\infty}$ is infinite-dimentional.}

Infinite list
$$(1, 0, ....), (0, 1, 0, ...), .... $$
is all linearly indepenent, therefore no finite set spans the space.


\subsection{}
\textit{PRove that the real vector space of all continous real-valued
  functions on the interval $[0, 1]$ is infinite-dimentional.}

We can create a countable sequence $(r_1, r_2, ... )$ of rationals in this
space, and correspod each one of them with some number, thus creating a
infinite linearly inedependent list.

\subsection{}

\textit{Suppose $p_0, p_1, ... p_n$ are
  polynomials in $\mathcal{P_m}(F)$ such that
  $p_j(2) = 0$ for each $j$. Prove tat $p_0, p_1, ... p_m$ is not linearly
  independent in $\mathcal{P_m}(F)$.}

Because it has the same length as $1, x, x^2 ... $, but doesn't span the same
space.

\section{Bases}

There are no challenging exercises in this section, just a recap of
the material. Looked them over, brushed up the material, not gonna waste
my time writing them down.

\section{Dimention}

\subsection{}
\textit{Suppose $V$ is finite-dimentional and $U$ is a subspace of $V$ such
  that $\dim U = \dim V$. Prove that $U = V$}

They have the same length of basis, thus basis of $U$ is a basis of $V$.

\subsection{}
\textit{Show that the subspaces of $R^2$ are precisely $\{0\}, R^2$ and all
  lines through the origin}

For 0 dimention we've got null

For dimention 1 we've got scalar multiple of any vector, which are lines
through the origin

For dimention 2 we've got the space itself

\subsection{}
\textit{Show that the subspaces of $R^3$ are precisely $\{0\}, R^3$, all
  lines through the origin, and all planes through the origin}

Same idea as in previos exericise, but list of length 2 defines a plane
through the origin and 3 defined space itself

\subsection{}
\textit{(a) Let $U = \{p \in P_4(F): p(6) = 0$. Find a basis of $U$.}

$$(x - 6), (x - 6)^2, (x - 6)^3, (x - 6)^4$$

\textit{(b) Extend the basis in part (a) to a basis of $P_4(F)$}

$$1, (x - 6), (x - 6)^2, (x - 6)^3, (x - 6)^4$$

\textit{Find a subspace $W$ of $P_4(F)$ such that $P_4(F) = U \oplus W$}

$$\{c: c \in F\}$$

\subsection{}
\textit{(a) Let $U = \{p \in P_4(F): p''(6) = 0$. Find a basis of $U$.}

$$1, (x - 6), (x - 6)^3, (x - 6)^4$$

\textit{(b) Extend the basis in part (a) to a basis of $P_4(F)$}

$$1, (x - 6), (x - 6)^2, (x - 6)^3, (x - 6)^4$$

\textit{Find a subspace $W$ of $P_4(F)$ such that $P_4(F) = U \oplus W$}

$$ (x - 6)^2$$


\subsection{}
\textit{(a) Let $U = \{p \in P_4(F): p(2) = p(5)$. Find a basis of $U$.}

$$1, (x - 2)(x - 5), (x - 2)^2(x - 5), (x - 2)^2(x - 5)^2$$

\textit{(b) Extend the basis in part (a) to a basis of $P_4(F)$}

$$1, x, (x - 2)(x - 5), (x - 2)^2(x - 5), (x - 2)^2(x - 5)^2$$

\textit{Find a subspace $W$ of $P_4(F)$ such that $P_4(F) = U \oplus W$}

$$ x $$

\subsection{}
\textit{(a) Let $U = \{p \in P_4(F): p(2) = p(5) = p(6)$. Find a basis of $U$.}

$$1, (x - 2)(x - 5)(x - 6), (x - 2)^2(x - 5)(x - 6)$$

\textit{(b) Extend the basis in part (a) to a basis of $P_4(F)$}

$$1, x, x^2,  (x - 2)(x - 5)(x - 6), (x - 2)^2(x - 5)(x - 6)$$

\textit{Find a subspace $W$ of $P_4(F)$ such that $P_4(F) = U \oplus W$}

$$x, x^2$$

\subsection{}
\textit{(a) Let $U = \{p \in P_4(F): \int_-1^1{p} = 0 \}$.
  Find a basis of $U$.}

$$x, x^3$$

\textit{(b) Extend the basis in part (a) to a basis of $P_4(F)$}

$$1, x, x^2, x^3, x^4$$

\textit{Find a subspace $W$ of $P_4(F)$ such that $P_4(F) = U \oplus W$}

$$1, x^2, x^4$$

\subsection{}
\textit{Suppose $v_1, ... v_m$ is linearly independent in $V$ and $w \in V$.
  Prove that}

$$\dim span(v_1 + w, ..., v_m + w) \geq m - 1$$

Because $v_1, ... v_m$ is linearly independent we can follow that $w$ is either
in $span(v_1, ..., v_m)$ or not. In the latter case we've got that the
case that we increase the span. In the former we've got by linear independence
of $v_1, ... v_m$ that the maximum decline of degree is $1$. Thus
$$\dim span(v_1 + w, ..., v_m + w) \geq m - 1$$
as desired.

\subsection{}

\textit{Suppose $p_0, p_1, ..., p_m \in P(F)$ are such taht each $p_j$ has
  degree $j$. Prove that $p_0, ... p_m$ is a basis of $P_m(F)$. }

Suppose that $p \in P_m(f)$. Because each $p_n$ has a degree of $n$ we can
follow that there exists only 1 $a_m \in F$ such that 
of $p_m$ such that
$$p - a_m p_m \in P_{m - 1}(F)$$.

Bu applying the same procedure  again repeatedly  we get unique
$a_m, ..., a_0$ such that
$$\sum{a_n p_m} = p$$
for every $p \in P_m(f)$. Thus we can follow that given list spans $P_m(F)$
and by unique representation we get that this list is linearly independent.
Thus we can follow that given list is a basis of $P_m(F)$, as desired.


\subsection{}
\textit{Suppose that $U$ and $W$ are subspaces of $R^8$ such that $\dim U = 3$,
  $\dim W = 5$, and $U + W = R^8$. Prove that $R^8 = U \oplus W$.}

We know that
$$\dim (U_1 + U_2) = \dim U_1 + \dim U_2 - \dim (U_1 \cap U_2)$$

Thus we can follow that in this particular case
$$\dim (R^8) = \dim U + \dim W - \dim (U \cap W)$$
$$8 = 3 + 5 - \dim (U \cap W)$$
$$\dim (U \cap W) = 0$$
thus we can follow that $U \cap W = \{0\}$. Therefore
$$U + W = U \oplus W = R^8$$
as desired.

\subsection{}
\textit{Suppose that $U$ and $W$ are both five-dimentional subspaces of $R^9$.
  Prove that $U \cap W \neq \{0\}$}

Once again we get that
$$\dim R^9 = \dim U + \dim W - \dim (U \cap W)$$
$$9 = 5 + 5- \dim (U \cap W)$$
$$\dim (U \cap W) = 1$$
thus
$$U \cap W \neq 0$$
as desired.

\subsection{}
\textit{Suppose $U$ and $W$ are both 4-dimentional subspaces of $C^6$. Prove
  that there exists two vectors in $U \cap W$ such that neither of these
  vectors is a scalar multiple of the other}

Goto previous exercise for concretee explanation if needed, but  we can 
conclude that
$$\dim U \cap W = 2$$
thus there exists a linearly independent list of length 2 in $U \cap W$ (basis)
so that neither of them is a scalar multiple of another by some exercise in 2.A

\subsection{}
\textit{Suppose $U_1, ... U_m$ are finite-dimentional subspaces of $V$.
  Prove that $U_1 + ... + U_m$ is finite-dimentional and }
$$\dim(\sum U_n) \leq \sum{\dim U_n}$$

We know that 
$$\dim (U_1 + U_2) = \dim U_1 + \dim U_2 - \dim (U_1 \cap U_2)$$
given that $\dim W \geq 0$ for any vector space $W$ we follow that
$$\dim (U_1 + U_2) \leq \dim U_1 + \dim U_2$$
Thus by induction
$$\dim(\sum U_n) \leq  \sum \dim U_n $$
which in presented case get us desired result.

\subsection{}
\textit{Suppose $V$ is finite-dimentional, with $\dim V = n \geq 1$. Prove
  that there exist 1-dimentional subspaces $U_1, ... U_n$ of $V$ such that  
}
$$V = U_1 \oplus ... \oplus U_n$$

For $V$ there exists a basis of length $n$. Thus by setting
$$U_j = \{c v_j: c \in F\}$$
we get desired result.

\subsection{}
\textit{Suppose $U_1, ..., U_m$ are finite-dimentional subspaces of $V$ such
  that $U_1 + .. + U_m$ is a direct sum. Prove that $U_1 + ... + U_m$ is
  finite dimentional and that}
$$\dim \sum U_n = \sum \dim U_n$$

We can just go by induction on the case that
$$\dim (U \oplus W) = \dim U + \dim W + \dim (U \cap W) =
\dim U + \dim W + 0$$
Or we can use the fact, that we can combine all bases
of subspaces together in one
mega-basis for their sum. Both will suffice.

\subsection{}
\textit{You might guess, by analogy with the formula for the number of elements
  in the union of three subsets of a finite set, that if $U_1, U_2, U_3$ are
  subspaces of finite-dimentional vector space, then}
$$\dim (U_1 + U_2 + U_3) =
\dim U_1 + \dim U_2 + \dim U_3 - \dim (U_1 \cap U_2) - \dim (U_1 \cap U_3) - $$
$$
- \dim (U_2 \cap U_3)
+ \dim (U_1 \cap U_2 \cap U_3)$$

We know that 
$$\dim (U_1 + U_2) = \dim U_1 + \dim U_2 - \dim (U_1 \cap U_2)$$
and
$$U_1 + U_2 + U_3 = (U_1 + U_2) + U_3$$
thus
$$\dim (U_1 + U_2 + U_3) = \dim ((U_1 + U_2) + U_3) =
\dim (U_1 + U_2) + \dim U_3 - \dim ((U_1 + U_2) \cap U_3) =$$
$$ =
\dim U_1 + \dim U_2 - \dim U_1 \cap U_2 + \dim U_3 -
\dim ((U_1 + U_2) \cap U_3) = $$
here we get a little problem because we don't know how to reduce
$(U_1 + U_2) \cap U_3$ to some managable pieces. After this discovery
one might even glance over
the equation once again in order to try to disprove the theorem.
And indeed we've found a counterexample: suppose that $U_1, U_2, U_3$ are
lines through the origin in $R^3$ such that they are located on the same
plane. Then it follows that left-hand side becomes 2, and the right side is
equal to 3. Thus we've got a contradiction (which is a shame, because
the formula looks nice :( ).

\chapter{Linear maps}

\section{The Vector Space of Linear Maps}

\subsection{}
\textit{Suppose $b, c \in R$. Define $T: R^3 \to R^2$ by }
$$T(x, y, z) = (2x - 4y + 3z + b, 6x + cxyz)$$
\textit{Show that $T$ is linear if and only of $b = c = 0$.}

Suppose that $T$ is linear. Then it follows that
$$T(0) = 0 = (0 + b, 0)$$
thus we can follow that $b = 0$.

Also,
$$T((1, 1, 1) + (2, 2, 2)) = (6 - 12 + 9, 18 + 27c) = (3, 18 + 27c) =
$$
$$
= T((1, 1, 1)) + T(2, 2, 2) = (2 - 4 + 3, 6 + c) + (4 - 8 + 6, 12 + 8c) =
(1, 6 + c) + (2, 12 + 8c) = (3, 18 + 9c)$$

Thus
$$27c = 9c$$
$$3c = c$$
$$c = 0$$
as desired.

Reverse implication is trivial, thus we get the desired result.

\subsection{}
\textit{Suppose $b, c \in R$. Define $T: \mathcal(P)(R) \to R^2$ by}
$$Tp = \left(3p(4) + 5p'(6) + bp(1)p(2), \int_{-1}^2{x^3 p(x) dx} + c \sin{p(0)}\right)$$
\textit{Show that $T$ is linear if and only if $b = c = 0$.}

Suppose that $T$ is linear. Then it follows that if $p(0) = \pi/2$, then latter
part of resulting vector has additive property only when $c = 0$. For the former
we've got result that
$$\lambda^2 b = b$$
for all $\lambda \in R$, which happens only if $b = 0$. Thus $b = c = 0$.

Reverse implication is trivial, thus we have the desired result.

\subsection{}
\textit{Suppose $T \in  \mathcal{L}(F^n, F^m)$. Show that there exists scalars
  $A_{j, k} \in F$ for $j = 1, ..., m$ and $K = 1, ..., n$ such that}
$$T(x_1, ..., x_n) = (A_{1, 1}x_1 + ... + A_{1, n} x_n, ..., A_{m, 1} x_1 + ... + A_{m, n} x_n)$$
\textit{for every $(x_1, ..., x_n) \in F^n$.}

Because $(1, 0, ...), (0, 1, ...), ... $ is a basis of $F^n$ we can follow that
there vector in $F^m$, such that $T(v) \in F^m$. Thus let us denote 
$$T(1, 0, ...) = (A_{1, 1}, A_{2, 1}, ..., A_{m, 1})$$
$$T(0, 1, ...) = (A_{1, 2}, A_{2, 2}, ..., A_{m, 2})$$
$$...$$
Thus given given arbitrary vector $v = (x_1, x_2, ..., x_n)\in T^n$ we get that 
$$T(v) = T(x_1, x_2, ...) = T(x_1, 0, 0, ...) + T(0, x_2, 0, ...) + ... =
x_1T(1, 0, 0, ...) + x_2 T(0, 1, 0, ...) + ... = $$
$$ = (x_1 A_{1_1}, x_1 A_{2, 1}, ... ) +
(x_2 A_{1_2}, x_2 A_{2, 2}, ... ) = (x_1 A_{1, 1} + x_2 A_{1, 2} + ..., x_1 A_{2, 1} + x_2 A_{2, 2} + ... )$$
as desired.

\subsection{}
\textit{Suppose $T \in \mathcal{L}(V, W)$ and $v_1, v_2, ... v_m$ is a list of vectors in $V$
  such that $T v_1, ...,, T v_m$ is a linearly inndependent list in $W$. Prove that
$v_1, v_2, ..., v_m$ is linearly independent.}

Suppose that it isn't. Then we can follow that there exist $w_1 \in W$ such that
$$w_1 = \sum a_j v_j = 0$$
and not all of $a_j$'s are zeroes. Thus we can follow that
$$T(w) = T(\sum a_j v_j) = \sum T (a_j v_j) = \sum a_j T (v_j) = 0$$
But $T(v_j)$ is a list of linearly independent vectors, and therefore their sum is
equal to zero iff all $a_j$'s are zeroes, which is false. Thus we've got a contradiction.

\subsection{}
\textit{Prove the assertion in 3.7}

Let $T_1 = T, T_2 = S, T_3 \in L(V, W)$. Then it follows that

(1) $$(T_1 + T_2)(v) = T_1(v) + T_2(v) = T_2(v) + T_1(v) = (T_2 + T_1)(v)$$

(2) $$(T_1 + (T_2 + T_3))(v) = T_1(v) + (T_2 + T_3)(v) = T_1(v) + T_2(v) + T_3(v) =$$
$$ = (T_1 + T_2)(v) + T_3(v) = ((T_1 + T_2) + T_3)(v)$$

(3) $$\lambda((S + T)(v)) = \lambda ( S(v) + T(v)) = \lambda S(v) + \lambda T(v)
= (\lambda S + \lambda T)(v)$$

(4) $$T + 0 = T$$

(5) $$1T = T$$

(6) $$T + -1T = (1 - 1)T = 0T = 0$$

Thus $L(V, W)$ satisfies all reqirements of a vector space, as desired.

\subsection{}
\textit{Prove the assertion in 3.9}

Let $v \in V$.

(1) Then it follows that
$$((T_1 T_2)T_3)(v) = (T_1 T_2)( T_3(v)) =  T_1 (T_2( T_3(v))) = T_1 ((T_2 T_3)(v))
= (T_1(T_2 T_3))(v)$$
directly from definition. (I wonder if it's  true in general for all functions; it probably
is).

(2)
$$ TI v = T(I(v)) = T(v) = I (T(v))$$

(3)

$$(S_1 + S_2)T(v) = (S_1 + S_2)(T(v)) = S_1(T(v)) + S_2(T(v)) = S_1 T v + S_2 T v$$
$$S(T_1 + T_2)(v) = S((T_1 + T_2)(v)) = S(T_1(v) + T_2(v)) = S(T_1(v)) + S(T_2(v)) = S T_1 v +
S T_2 v$$

as desired.

\subsection{}
\textit{Show that every linear map from a 1-dimentional vector space to itself is
  multiplication by some scalar. More precisely, prove that if $\dim V = 1$ and
  $T \in L(V, V)$, then there exists $\lambda \in F$ such that $Tv = \lambda v$ for
  all $v \in V$.}

Because we've got a 1-dimentional space, it follows that there exists a basis of $V$ - $v_1$.
For this vector we've got that
$$T v_1 = v_2 = \lambda v_1$$
Thus we can follow that if $u \in V$ then
$$T u = T \sigma v_1 =  \sigma T v_1 = \sigma \lambda v_1 = \lambda \sigma v_1 = \lambda u$$
as desired.

\subsection{}
\textit{Give an example of a function $\phi: R^2 \to R$ such that }
$$\phi (av) = a\phi(v)$$
\textit{for all $a \in R$ and all $v \in R^2$ but $\phi$ is not linear.}

$$\phi(x, y) =
\begin{cases}
  x \text{ if } x \neq y \\
  0 \text{ otherwise}
\end{cases}
$$

\subsection{}
\textit{Give an example of a function $\phi: C \to C$ such that }
$$\phi (w + z) = \phi(w) + \phi(z)$$
\textit{for all $w, z \in C$ but $\phi$ is not linear.}

Let us define 
$$\phi(a + bi) = b + ai$$
Thus 
$$\phi(a + bi + c + di) = ai + ci + b + d = \phi(a + bi) + \phi(c + di)$$
but
$$i \phi(a + bi) = -a + bi$$
$$\phi(i(a + bi)) = \phi(ai - b) = -bi + a \neq i \phi(a + bi)$$

\subsection{}
\textit{Suppose $U$ is a subspace of $V$ with $U \neq V$. Suppose $S \in L(V, W)$ and
  $S \neq 0$. Define $T: V \to W$ by}
$$Tv =
\begin{cases}
  Sv \text{ if } v \in U \\
  0 \text{ if } v \in V \text{ and } v \notin U
\end{cases}
$$
\textit{Prove that $T$ is not a linear map on $V$.}

Let $u \neq 0 \in U$ such that $Su \neq 0$ and $v \in V \setminus U$. Then it follows that
$$v + u \notin U$$
(because otherwise $-(v + u)$ is in $U$, therefore $u - (v + u) = -v \in U$ and
thus $v \in U$, which is a contradiction)
Thus we can follow that
$$T(v + u) = 0$$
but
$$T(v) + T(u) = 0 + Su = Su \neq 0 = T(v + u)$$
therefore the function is not linear, as desired.

\subsection{}
\textit{Suppose $V$ is finite-dimentional. Prove that every linear map on a subspace of $V$
  can be extended to a lineaer map on $V$. In other words, show that if $U$ is a subspace of $V$
  and $S$ is a subspace of $V$ and $S = L(V, W)$, then there exists $T \in L(V, W)$ such that
  $Tu = Su$ for all $u \in U$.}

Because $V$ is finite-dimentional and $U$ is a subspace of $V$, we can follow that $U$
is finite-dimentional as well. Thus we can follow that there exists
$u_1, ..., u_m$ - basis of $U$. As we know, we can extend this basis to a basis of $V$ -
$u_1, ..., u_m, v_1, ... v_n$. Therefore we can define a map $P \in L(V, U)$ by 
$$P(x_1, x_2, ...) = (x_1, x_2, ... x_m, 0, 0, ...)$$
(basically trim every element of basis that is not in $U$). Thus we can follow that
$P(u) = u$ if $u \in U$. Proof that $P$ is linear is trivial.
Thus if $S \in L(U, W)$, then $T = SP \in (V, W)$ with the
desired properties.

\subsection{}

\textit{Suppose $V$ is finite-dimentional with $\dim V > 0$, and suppose $W$ is
  infinite-dimentional. Prove that $L(V, W)$ is infinite-dimentional.}

Let $v_1, ..., v_m$ be a basis of $V$ and
let $w_1, w_2, ...$ be a list of linearly independent vectors in $W$. Now
let us look at $T_n: V \to W$
$$T_n((x_1, x_2, ...) ) = x_1 w_n$$
Then it follows that by linear independence of $w_n$ there does not exist a linear
combination of $T_m$ such that
$$\sum_{m \neq n} a_m T_m \neq T_n$$
Thus we can follow that list $T_n$ is linearly independent.
Because list is not finite we can follow that the space $L(V, W)$ is infinite-dimentional,
as desired.

\subsection{}

\textit{Suppose $v_1, ..., v_m$ is a linearly dependent list of vectors in $V$. Suppose
  also that $W \neq \{0\}$. Prove that there exist $w_1, ... w_m \in W$ such that no
  $T \in L(V, W)$ satisfies $Tv_k = w_k$ for each $k = 1, ..., m$.}

Because $v_1, ..., v_m$ is linearly dependent we can reduce it to a linearly independent list
$v_1', ..., v_n'$. Thus resulting list will span some subspace of $V$ and will be its basis.

Thus we can take vector $v_j$ from the original list,
that does not appear in basis.
Then take some vectors $w_1, ... w_n$ in $W$. We know that there exists a unique map
$$Tv_n' = w_n$$
thus by adding to list $w_1, ... w_n$ any vectors from $W$, apart from $T(v_j)$ we create desired
list.

\subsection{}
\textit{Suppose $V$ is finite-dimentional with $\dim V \geq 2$. Prove that there
  exists $S, T, \in L(V, V)$ such that $ST \neq TS$ }

Let $v_1, v_2$ be a basis of $V$ and let
$$ S(x, y) = (y, x)$$
$$ T(x, y) = (x, 0)$$
Then
$$ST = (0, x)$$
and
$$TS = (y, 0)$$
as desired.


\section{Null Spaces and Ranges}

\subsection{}

\textit{Give an example of a linear map $T$ such that $\dim null T = 3$ and
  $\dim range T = 2$.}

$T(x, y, z) = (x, y)$

\subsection{}

\textit{Suppose $V$ is a vector space and $S, T \in L(V, V)$ are such that }
$$range S \subset null T$$
\textit{Prove that $(ST)^2 = 0$.}

Let $v \in V$. Then it follows that $S(T(v)) \in range S$. Thus $ST(v) \in null T$.
Therefore $TST(v) = 0$. And thus  $STST = (ST)^2 = 0$, as desired.

\subsection{}
\textit{Suppose $v_1, ...,  v_m$ is a list of vectors in $V$. Define $T \in L(F^m, V)$ by}
$$T(z_1, ..., z_m) = z_1 v_1 + ... + z_m v_m$$

\textit{(a) What property of $T$ corresponds to $v_1, ..., v_m$ spanning $V$?}

Surjectivity

\textit{(b) What property of $T$ corresponds to $v_1, ..., v_m$ being linearly independent?}

Injectivity

\subsection{}

\textit{Show that }
$$ \{T \in L(R^5, R^4): \dim null T > 2 \}$$
\textit{is not a subspace of $L(R^5, R^4)$.}

We can set

$$T_1(x, y, z, w, q) = (x, 0, 0, 0)$$
$$T_2(x, y, z, w, q) = (0, y, 0, 0)$$
$$T_3(x, y, z, w, q) = (0, 0, z, 0)$$
$$T_4(x, y, z, w, q) = (0, 0, 0, w)$$

all of which are in the desired subset, but their sum is
$$T(x, y, z, w, q) = (x, y, z, w, 0)$$
which has $\dim null = 1$. Thus this subset is not closed under addition and therefore it
is not a subspace.

\subsection{}

\textit{Give an example of a linear map $T: R^4 \to R^4$ such that }
$$range T = null T$$

$$T(x, y, z, w) = (z, w, 0, 0)$$.

\subsection{}

\textit{Prove that there does not exist a linear map $T R^5 \to R^5$ such that }
$$range T = null T$$

$\dim$ is always an integer, therefore for $\dim range T = \dim null T = n$ and
$$\dim T = 2n = 5$$
which is impossible.

\subsection{}

\textit{Suppose $V$ and $W$ are finite-dimentional with $2 \leq \dim V \leq \dim W$.
  Show that $\{T \in L(V, W): T \text{ is not injective}\}$ is not a subspace of $L(V, W)$.}

Suppose that $v_1, ..., v_m$ is a basis for $V$ and $w_1, ..., w_n$ is a basis of $W$.
We can follow that there exist, which maps $v_1$ to $w_1$ and so on. By adding all of
those maps together we get an injective map. Thus we can follow that given set is not
closed under addition and therefore is not a subspace.

\subsection{}

\textit{Suppose $V$ and $W$ are finite-dimentional with $2 \leq \dim W \leq \dim V$.
  Show that $\{T \in L(V, W): T \text{ is not surjective}\}$ is not a subspace of $L(V, W)$.}

By following the simular logic as in previous exercise, we get a desired result.

\subsection{}

\textit{Suppose $T \in L(V, W)$ is injective and $v_1, ..., v_n$ is linearly independent
  in $V$. Prove that $Tv_1, ..., Tv_n$ is linearly independent in $W$.}

Suppose that it is not the case. Then it follows that there exists $a_1, ... a_n \in F$ such
that not all of them are equal to zero and 
$$\sum a_n T v_n = 0$$
Thus we can follow that
$$T \sum a_n v_n = 0$$
Thus $\sum a_n v_n \in null T$. Because $T$ is injective we can follow that
$$\sum a_n v_n = 0$$
and some of $a_n$'s are not equal to zero. But $v_1, ..., v_n$ is linearly independent, thus
we get a contradiction.

\subsection{}

\textit{Suppose $v_1, ..., v_n$ spans $V$ and $T \in L(V, W)$. Prove that the list
  $T v_1, .... Tv_n$ spans range $T$.}

Suppose $w \in range T$. Thus we can follow that there exists $v \in V$ such that
$$Tv = w$$
Given that $v_1, ..., v_n$ spans $V$ we can follow that there exists $a_1, ... a_n$ such that
$$v = \sum a_n v_n$$
and thus
$$w = T \sum a_n v_n$$
$$w =  \sum T a_n v_n$$
thus we can follow that $v_1, ..., v_n$ spans the range of $T$, as desired.

\subsection{}
\textit{Suppose $S_1, ..., S_n$ are injective linear maps such that $S_1 S_2 ... S_n$ makes sense.
  Prove that $S_1 S_2 ... S_n$ is injective. }

Suppose that $T$ and $S$ are injective such that $ST$ makes sence. Suppose that
$$STv = 0$$
Then by injectivity of $S$ we get that $Tv \in null S$ and thus $Tv = 0$. Thus, by injectivity of
$T$ we get that $v$ = 0. Therefore $null ST = 0$. Therefore $ST$ is injective.

The case in the exercise is derived from induction on presented argument.

\subsection{}

\textit{Suppose that $V$ is finite-dimentional and that $T \in L(V, W)$. Prove that there exists
  a subspace $U$ of $V$ such that $U \cap null T = 0$ and $range T = \{Tu: u \in U\}$.}

Let $N$ be a nullspace of $T$. It follows that it is a subspace of $V$. Now let
$n_1, ..., n_m$ be a basis of $N$ and extend it to a basis of $V$: $n_1, ..., n_m, v_1, ..., v_n$.
Then if follows that $span(v_1, ... v_n) \cap N = 0$ (because otherwise the vector is in nullspace)
and if $w \in rangeT$, then there exists $u \in span(v_1, ... v_n)$ such that
$Tu = w$. Thus $span(v_1, ... v_n)$ is the desired subspace.

\subsection{}

\textit{Suppose $T$ is a linear map from $F^4$ to $F^2$ such that}
$$null T = \{(x_1, x_2, x_3, x_4) \in F^4: x_1 = 5x_2, x_3 = 7x_4\}$$
\textit{Prove that $T$ is surjective.}

$\dim null T = 2$, thus $\dim range T = 2$, therefore $T$ is surjective, as desired.

\subsection{}
\textit{Suppose $U$ is a 3-dimentional subspace of $R^8$ and that $T$ is a linear map from
  $R^8$ to $R^5$ such that $null T = U$. Prove that $T$ is surjcetive.}

We can follow that $\dim range T = 5$, and therefore $T$ is surjective, as desired.

\subsection{}

Very similar to previous one

\subsection{}

Same

\subsection{}

Same

\subsection{}

Same

\subsection{}

Same

\subsection{}

\textit{Suppose $W$ is finite-dimentional and $T \in L(V, W)$. Prove that $T$ is injective
  if and only if there exists $S \in L(W, V)$ such that $ST$ is the identity map on $V$.}

I don't know why it isn't stated explicitly, but by existence of injective $T$ we can
follow that $\dim V \leq \dim W$, and thus $V$ is finite-dimentional.

\textbf{In forward  direction}:

Suppose that $T$ is injective. Now let $v_1, ..., v_m$ be a basis of $V$. Then we can follow
that $Tv_1, ..., Tv_n$ is a basis of range $T$. Thus, extend this basis to a basis of $W$:
$Tv_1, ..., Tv_n, w_1, ..., w_m$. Now let us define $S \in L(W, V)$ such that
$$S Tv_n = v_n$$ 
$$S w_n = 0$$
Which will exist, and by the way, will be unique
because we're pairing basis of $W$ with a list of vectors in $V$. Thus we
can follow that if $v \in V$ then
$$ST v = ST \sum a_n v_n = S \sum T a_n v_n = S \sum a_n T  v_n = \sum a_n v_n = v$$
thus $ST = I$, as desired.

\textbf{In reverse dierction: }

Suppose that there exists $S \in L(W, V)$ such that $ST$ is an identity map on $V$. Suppose that
$T$ is not injective.  Then we follow that $null T \neq 0$.
Then let $v_1 \in null T \neq 0$. Then we can follow that
$$ST v_1 = S(T v_1) = S(0) = 0 \neq Iv_1$$
which is a contradiction. Thus we can conclude that $T$ is injective, as desired.

\subsection{}

\textit{Suppose $W$ is finite-dimentional and $T \in L(V, W)$. Prove that $T$ is surjective
  if and only if there exists $S \in L(W, V)$ such that $TS$ is the identity map on $W$.}

\textbf{In forward direction:}

Suppose that $T$ is surjective and let $w_1, ..., w_n$ be a basis of $W$. Then we can follow
that there exists $v_1, ..., v_m$ such that $T v_1 = w_1, ... T v_m = w_m$.
Thus we can follow that there exists a map in $L(W, V)$ such
that it maps
$$S w_1 = v_1$$
$$S w_n = v_n$$
Thus if $w \in W$, then we can follow that
$$TS w = TW(\sum a_n w_n) = T(\sum a_n W  w_n) =  T(\sum a_n v_n) =  \sum a_n T v_n =
\sum a_n w_n = w$$
for every $w \in W$. Thus we can follow that $TS = I$, as desired.

\textbf{In reverse direction:}

Suppose that there exists a map $S \in L(W, V)$ such that $TS$ is an identity map on $W$. 

Suppose now that $T$ is not surjective. Then we can follow that there exists $w \in W$ such that
there is no $v \in V$ such that $Tv = w$. But we've got that
$$TS w = T(Sw) = w$$
thus we've got a contradiction.

\subsection{}

\textit{Suppose $U$ and $V$ are finite-dimentional vector spaces and $S \in L(V, W)$ andd
  $T \in L(U, V)$. Prove that }
$$\dim null ST \leq \dim null S + \dim null T.$$

We know that if $T$ maps a vector to zero, then $STv = S(Tv) = S0 = 0$. Thus we can follow that
$$\text{null } T \subseteq \text{null } ST$$
Suppose that $STv = 0$. Then we can follow that $Tv \in null S$. Thus $ null ST $ exhaustively
decomposes into two sets: $null T$ and $\{u \in U : Tu \in range T \cap null S\}$. We know that
$$\dim (range T \cap null S) \leq \dim null S$$.
thus we can follow that
$$\dim null ST = \dim null T + \dim (range T \cap null S) \leq   \dim null S + \dim null T$$
as desired.

\subsection{}

\textit{Suppsoe $U$ and $V$ are finite-dimentional vector spaces and $S \in L(V, W)$ and
  $T \in L(U, V)$. Prove that}
$$\dim range ST \leq \min\{\dim range S, \dim range T\}$$

Given that $range ST \subseteq range S$ we can follow that
$$\dim range ST \leq \dim range S$$
Suppose that $U'$ is a preimage of range of $ST$. Then we can follow that if  $u' \in U'$, then
$u'$ is also in preimage of range of $T$. Thus we can follow that preimage of $ST$ is
a subset of preimage of $T$, and thus
$$\dim range ST \leq \dim range T$$
Because both equations must hold, in follows that  we  get out desired inequality.

\subsection{}
\textit{Suppose $W$ is finite-dimentional and $T_1, T_2 \in L(V, W)$. Prove that
  $null T_1 \subset null T_2$ if and only if there exists $S \in L(W, W)$ such that
  $T_2 = ST_1$.}

Firstly I should state that proposition in the exercise holds if we state that
$\subset$ does not donote a proper subset, but a regular subset. 

\textbf{In forward direction}: 
Suppose $null T_1 \subset null T_2$. This implies that $\dim range T_1 \geq \dim range T_2$.
Let $v_1, ..., v_n, u_1, ... u_n, r_1, ..., r_n$ be a basis of $V$ such that
$r_1, ... r_n$ is a basis of $null T_1$,  $u_1, ..., u_n, r_1, ... r_n$ is a basis of $T_2$.
Then we can follow that
$T_2 v_1, ..., T_2 v_n$ is a basis of range of $T_2$ and $T_1 v_1, ... T_1 v_n$ is a
basis of a subspace of range of $T_1$.
Thus we can create a map $S: W \to W$ such that
$S T_1 v_n = T_2 v_n$
Suppose that $v \in V$. Then it follows that
$$S T_1 v = S T_1 \sum a_n v_n =   \sum a_n ST_1 v_n = \sum a_n T_2 v_n = T_2 v$$
Thus we get our desired result.

\textbf{In reverse direction:}
Suppose that there exists $S \in L(W, W)$ such that $T_2 = S T_1$. Suppose that $v \in null T_1$.
Thus $T_1 v = 0 = S T_1 v = T_2 v$. Thus $v \in  null T_2$. Therefore $null T_1 \subset \null T_2$,
as desired.

\subsection{}

\textit{Suppose $W$ is finite-dimentional and $T_1, T_2 \in L(V, W)$. Prove that
  $range T_1 \subset range T_2$ if and only if there exists $S \in L(V, V)$ such that
  $T_2 = T_1S $.}

\textbf{In forward direction:}

Suppose that $range T_1 \subset range T_2$. Then let $q_1, ... q_n$ be a basis of range of $T_1$.
Thus we can extend it to be a basis of range of $T_2$ be adding $w_1, ..., w_m, q_1, ..., q_n$.
Thus we can follow that there exist $v_1, ... v_k \in V$ such that
$$T_1 v_n = w_n$$
and $v_1', ..., v_k' \in V$ such that
$$T_2 v_n' = w_n$$
Thus we can create a map $S \in L(V, V)$ such that
$$S v_n = v_n'$$
and thus
$$T_2 S v = T_2 S \sum a_n v_n = T_2  \sum a_n S v_n = T_2  \sum a_n  v_n'  = \sum a_n T_1 v_n 
=T_1 \sum a_n  v_n = T_1 v$$
as desired.

\textbf{In reverse direction: }

Suppose that there exists $S$ such that $T_1 = T_2 S$. Then it follows that if $u \in range T_1$,
then $u \in T_2$ as well. Thus $range T_1 \subset T_2$, as desired.


\subsection{}

\textit{Suppose $D \in L(P(R), P(R))$ is such that $\deg Dp = (deg p) -1$ for every nonconstant
  polynomial $p \in P(R)$. Prove that $D$ is surjective.}

Let us define a list of polynomials $p_n$ such that $\deg(p_n) = n$. Then it follows that
the list $D(p_n)$ is a list of polynomials such that $\deg (D(p_n)) = n - 1$, thus it
spans the space of polynomials. Thus we can follow that $D$ is surjective.

\subsection{}
\textit{Suppose $p \in P(R)$. Prove that there exists a polynomial $q \in P(R)$ such that
  $5q'' + 3q' = p$.}

By the exercise above we can state that differentiation is surjective. Thus double differentiation
is also surjective. Thus there exists $k \in P(R)$ such that $q'' = k'$, therefore
$5q'' = 5k'$. Thus by surjectivity of differentiation we've got the desired result.

\subsection{}
\textit{Suppose $T \in L(V, W)$. and $w_1, ..., w_m$ is a basis of range $T$. Prove that there
  exist $\phi_1, ... \phi_m \in L(V, F)$ such that }
$$T(v) = \phi_1(v) w_1 + ... \phi_n(v) w_n$$
\textit{for every $v \in V$}

Suppose that $v_1, ..., v_n$ is a basis of $V$. It follows that we can get coefficients
$$T v_j = A_{j,1} w_1 + ... + A_{j,n} w_n$$
thus if we set
$$\phi_j(v) = \phi_j(\sum a_n v_n) = \sum a_j A_{j, n}$$
then we get that
$$T v = T \sum a_n v_n = \sum a_n T v_n = \sum {a_n \sum A_{n, j} w_j} =
\sum {\sum a_n A_{n, j} w_j} = \sum {\phi_n(v) w_n} $$\
as desired.

\subsection{}

\textit{Suppose $\phi \in L(V, F)$. Suppose $u \in B$ is not in null $\phi$. Prove that}
$$V = null \phi \oplus \{au: a \in F\}$$

$\phi$ maps into a space of dimention one. Thus we can follow that its range is either
1 or 0. In this case there exists $u \in V$, such that it is not in null space of $\phi$,
therefore we can follow that $\dim range \phi = 1$. Thus the space, that is not in $null T$
has dimention $1$. Thus we can follow that this space is scalar multiples of $u$. Therefore
$$null \phi + \{au: a \in F\} = V$$
because $u \notin null \phi$ we follow that 
$$null \phi \cap \{au: a \in F\} = 0$$
and thus we can state that
$$null \phi \oplus \{au: a \in F\} = V$$
as desired.

\subsection{}

\textit{Suppose $\phi_1$ and $\phi_2$ are linear maps from $V$ to $F$ that have the same
  null space. Show that there exists a constant $c \in F$ such that $\phi_1 = c \phi_2$.}

If $\dim range \phi = 0$, then the case is trivial. Thus let us assume that $\dim range \phi = 1$.
Because they have the same null space we can follow that they have the same preimage of
the range. Thus we follow that if $v_1, ..., v_n$ is a basis of nullspace, then
$v_1, ..., v_n, w$ is a basis of $V$, therefore $w$ is a basis of a preimage. Thus
$$\phi_1 v = a_{n + 1}  \phi_1 w = a_{n + 1}c_1 $$
$$\phi_2 v = a_{n + 1}  \phi_2 w = a_{n + 1}c_2 $$
thus
$$\phi_1 = c_2 / c_1 \phi_2$$
as desired.

\subsection{}

\textit{Give an example of two linear maps $T_1$ and $T_2$ from $R^5$ to $R^2$ that have the same
  null space but are such that $T_1$ is not a scalar multiple of $T_2$}

$$T_1(x, y, z, w, q) = (x, y)$$
$$T_2(x, y, z, w, q) = (y, x)$$

\section{Matrices}

\subsection{}

\textit{Suppose $V$ and $W$ are finite-dimentional and $T \in L(V, W)$. Show that with respect
  to each choice of bases of $V$ and $W$, the matrix of $T$ has at least $\dim range T$ nonzero
  entries.}

Suppose that we've there exists a choice of bases of $V$ and $W$, such that matrix of this
linear map has less nonzero entries, then $\dim range T$. Then it follows, that
$\dim range T$ is spanned by list of vectors, that has length less than $\dim range T$,
which is impossible.


\subsection{}

\textit{Suppose $D \in L(P_3(R), P_2(R))$ is the differentiation map defined by $Dp = p'$. Find
  a basis of $P_3(R)$ and a basis of $P_2(R)$ such that the matrix of $D$ with resprect to these
  bases is }
$$
A =
\begin{pmatrix}
  1 & 0 & 0 & 0 \\
  0 & 1 & 0 & 0 \\
  0 & 0 & 1 & 0 \\
\end{pmatrix}
$$

I think that we can use standart basis for $P_3(R)$, and for $P_2(R)$ we gotta use
basis $1, 2x, 3x^2$.

\subsection{}

\textit{Suppose $V$ and $W$ are finite-dimentional and $T \in L(V, W)$. Prove that there exist
  a basis of $V$ and a basis of $W$. such that with respect to these bases, all entries of $M(T)$
  are 0 except that the entries in row $j$, column $j$, equal $1$ for
  $1 \leq j \leq \dim range T$.}

We can create a basis out of preimage of range of $T$. Thus if we set $v_1, ..., v_n$ to be a
basis of preimage and $T v_1, ..., T v_n$ to be the basis of range. Thus if we extend those
lists to be a bases of $V$ and $W$ respectively, we get the desired result.

\subsection{}

\textit{Suppose $v_1, ..., v_m$ is a basis of $V$ and $W$ is finite-dimentional. Suppose
  $T \in L(V, W)$. Prove that there exists a basis $w_1, ..., w_n$ of $W$ such that
  all the entries in the first column of $M(T)$ (with respect to the bases $v_1, ..., v_m$ and
  $w_1, ..., w_m$) are 0 except for possibly a 1 in the first row, first column.}

We can plug in $v_1$ into $T$ to get $T v_1$. If $T v_1 = 0$, then $v_1$ is in the nullspace
and any basis will do. Otherwise we can extend $T v_1$ to a basis of $W$ and get the desired
result.

\subsection{}

\textit{Suppose $w_1, ..., w_n$ is a basis of $W$ and $V$ is finite-dimentional. Suppose
  $T \in L(V, W)$. Prove that there exists a basis $v_1, ..., v_m$ of $V$ such that
  all the entries in the first row of $M(T)$ (with respect to the bases $v_1, ..., v_m$ and
  $w_1, ..., w_n$) are 0 except for possibly a 1 in the first row, first column.}

Suppose that we've got a random basis $v_1, ..., v_n$  of $V$ and then
map it through $T$. Then pick a vector $v_j$ such that $T v_j = a_1 w_1 + ... + a_m w_m$
such that $a_1 \neq 0$. If there is no such vector, then we're set. Otherwise go
through all the other vecrors $v_k$ and look at the representation
$$v_k = a_1' w_1 + ... + a_m' w_m$$
and set 
$$v_k'  = v_k - b_n v_n$$
where $b_n$ satisfies $a_1/a_1'$ (or vice versa) such that $v_k'$ represented as
$$T v_k' = 0 w_1 + ... + a_m w_m$$
Thus by linear independence of $v_1, ..., v_n$ we've got that $v_j, ..., v_1', ..., v_n'$ is
also linearly independent. Then, by plugging this vector into a matrix in this order, we
get the desired result.

\subsection{}

\textit{Suppose $V$ and $W$ are finite-dimentional and $T = L(V, W)$. Prove that
  $\dim range T = 1$ if and only if there exist a basis of $V$ and a basis of $W$
  such that with respect to these bases, all entries of $M(T)$ equal 1.}

\textbf{In forward direction: }
Suppose that we've got $T$ and $\dim range T = 1$.
Suppose that $v_1, ..., v_n$ is the resulting basis of $V$. Thus we can follow that
$T v_1 = T v_2 = ... = T v_n = w \neq 0$. Thus we can follow that $w_1 + ... + w_m = w$ is
the basis of range of $T$.

Thus we can create a vector $v_1$ such that $T v_1 = w$, then expand it to a basis of
$V$ $v_1, ..., v_n$ and then it'll follow that $v_2, ..., v_n$ is a basis of a nullspace.
Thus we can make a list $v_1, v_2 + v_1, v_3 + v_1, ..., v_n + v_1$, that will also a
basis of $V$ and for it it'll follow that
$$T v_j' = T (v_j + v_1) = 0 + w = w$$
Thus the only thing that is left is to create a basis of $W$ such that
$$w_1 + ... + w_n = w$$
we can actually do it by expanding $w$ to a basis of $W$, and getting $w, w_1, ..., w_n$. Then
we can set the first vector to be $w - w_1 - w_2 - ... - w_n$ and we'll get the desired
property. Thus we can construct the bases such that we have the desired property.

\textbf{In reverse direction: }
Suppose that there exist a basis of $V$ and basis of $W$ such that all entries of $M(T)$  are
equal to 1. Then we can follow that if we plug any vector into $T$, then we'll get the
constant multiple of the vector $w_1 + ... + w_n$. Thus we can follow that
$\dim range T = 1$, as desired.

\subsection{}
\textit{Verify 3.36}

3.36 states that if $S, T \in L(V, W)$, then $M(S + T) = M(S) + M(T)$.

Suppose that $S, T \in L(V, W)$, $v_1, ..., v_n$ is a basis of $V$ and
$w_1, ..., w_m$ is a basis of $W$. Then we can follow that
values at $j$'th column of $M(S + T)$ are obtained through
$$(S + T)(v_j) =  S(v_j) + T(v_j) = (a_1 + a_1') w_1 + ... + (a_m + a_m')w_m$$
where $a_1, ... a_m$ will be numbers in $j$'th row of $M(S)$ and $a_1', ..., a_m'$ will
be numbers in $j$'th row of $M(T)$. Thus we can follow that $M(S + T) = M(S) + M(T)$.

\subsection{}

\textit{Verify 3.38}

3.38 states that if $\lambda \in F$ and $T \in L(V, W)$, then $M(\lambda T) = \lambda M(T)$
$$(\lambda T) v_j = \lambda a_1 w_1 + ... + \lambda a_m w_m =
\lambda (a_1 w_1 + ... + a_m w_m) = 
\lambda(T v_j)$$
thus by the same reasoning as in previous exercise we've got that $M(\lambda T) = \lambda M(T)$,
as desired.

\subsection{}

\textit{Verify 3.52}

Follows directrly from the definition.

\subsection{}

\textit{Suppose $A$ is an $m$-by-$n$ matrix and $C$ is an $n$-by-$p$ matrix. Prove that }
$$(AC)_{j, \cdot} = A_{j, \cdot}C$$
\textit{for $1 \leq j \leq m$. In other words, show that row $j$ of $AC$ equals (row $j$ of $A$)
  times $C$.}

$$(AC)_{j, k} = A_{j, \cdot} C_{k, \cdot}$$
thus
$$(AC)_{j, \cdot} = (A_{j, \cdot} C_{1, \cdot}, A_{j, \cdot} C_{2, \cdot}, ..., A_{j, \cdot} C_{k, \cdot}) =
A_{j, \cdot} C
$$

\subsection{}

\textit{Suppose $a = (a_1, ..., a_n)$ is a $1$-by-$n$ matrix and $C$ is an $n$-by-$p$ matrix.
  Prove that }
$$aC = a_1 C_{1, \cdot} + ... + a_n C_{n, \cdot}$$
\textit{In other words, show that $aC$ is a linear combination of the rows of $C$,
  with the scalars that multiply the rows coming from $a$.}

This follows directly from a definition of matrix multiplication.

\subsection{}

\textit{Give an example with 2-by-2 matrices to show that matrix multiplication is not
  commutative. In other words, find 2-by-2 matrices $A$ and $C$ such that
  $AC \neq CA$}

$$A =
\begin{pmatrix}
  1 & 0 \\
  0 & 0
\end{pmatrix}
$$

$$C =
\begin{pmatrix}
  0 & 1 \\
  1 & 0
\end{pmatrix}
$$

$$AC =
\begin{pmatrix}
  0 & 1 \\
  0 & 0
\end{pmatrix}
$$

$$CA =
\begin{pmatrix}
  0 & 0 \\
  1 & 0
\end{pmatrix}
$$

\textit{The rest of the exercises are just basic applications of definitions, and equating them
  rigorously. Nothing interesting in there}

\section{Invertibility and Isomorphic Vector Spaces}

\subsection{}

\textit{Suppose $T \in \map (U, V)$ and $S \in \map (V, W)$ are both invertible linear maps.
  Prove that $ST \in \map(U, W)$ is invertible and that $(ST)^{-1} = T^{-1} S^{-1}$}

Let $u \in U$. Then
$$u = Iu = (T \inv T) u = T \inv (T u) = T \inv I (T u) = T \inv (S \inv S) (T u) =
(T \inv S \inv) (S T) u $$
thus we can follow that $(ST)$ is invertible and $(T \inv S \inv) = (ST) \inv$, as desired.

\subsection{}

\textit{Suppose $V$ is finite-dimentional and $\dim V > 1$. Prove that the set of
  noninvertible operators on $V$ is not a subspace of $\map(V)$.}

We can have
$$S_1(a_1 v_1 + ... a_n + v_n) = a_1 v_1 + ... a_j + v_j$$
$$S_2(a_1 v_1 + ... a_n + v_n) = a_{j + 1} v_{j + 1} + ... a_n + v_n$$
for some $1 \leq j < n$. (you can interpret it as an upper part of the identity and a lower
part). Both of them are non-invertible (by non-surjectivity), but their sum is the identity,
which is invertible. Thus the subset is not closed under addition and therefore it is not a
subspace.

\subsection{}

\textit{Suppose $V$ is finite-dimentional, $U$ is a subspace of $V$, and $S \in \map (U, V)$.
  Prove that there exists an invertible operator $T \in \map(V)$ such that $T(u) = S(u)$ for
  every $u \in U$ if and only if $S$ is injective.}

\textbf{In forward direction: }
Suppose that there exists such an operator and $S$ is not injective. Thus there exists
$u_1$ such that $u_1 \neq 0$ and $S u_1 = 0$. Thus $T u_1 = S u_1 = 0$, therefore
$T$ is not injective, therefore it is not invertible, which is a contradiction.

\textbf{In reverse direction: }
Suppose that $S$ is injective. Let $u_1, ..., u_n, v_1, ..., v_m$ be a basis of $V$ such
that $u_1, ..., u_n$ is a basis of $U$. Thus, by FTLM we've got that 
$$\dim U  = \dim \range S + \dim \ns S$$
Given that $S$ is injective, we can follow that $\dim \ns S = 0$. Thus
$$\dim U  = \dim \range S$$
Thus, because $\range S$ is a subspace,  we can create a basis of it
$u_1', ..., u_n'$, which will have the same length as the basis of $U$. By expanding this basis
to a basis of $V$ we can create $u_1', ..., u_n', v_1', ..., v_m'$.
Then if we map $u_1, ..., u_n, v_1, ..., v_m$ to $u_1', ..., u_n', v_1', ..., v_m'$, we'll
get $T \in \map(V)$, which  by uniqueness of representation that will have
$$Tu = Su$$
and because $u_1', ..., u_n', v_1', ..., v_m'$ is a basis of $V$ we'll get that $T$ is
surjective, and thus invertible, as desired.

\subsection{}

\textit{Suppose $W$ is finite-dimentional and $T_1, T_2 \in \map(V, W)$. Prove that
  $\ns T_1 = \ns T_2$ if and only if there exists an invertible operator $S \in \map(W)$ such that
  $T_1 = S T_2$.}

\textbf{In forward direction: }
Suppose that $\ns T_1 = \ns T_2$. Then we can follow that $\dim \range T_1 = \dim \range T_2$.
We can also follow that there exists $v_1, ... v_n$ - basis of preimage of $T_1$ and $T_2$.
Thus, $T_1 v_1, ... T_1 v_n$ is a basis of $\range T_1$ and $T_2 v_1, ..., T_2 v_n$ is a
basis of $\range T_2$. Thus we can create a unique map $S \in \map(\range T_2, \range T_1)$
$$S'(a_1 T_2 v_1 +  ... +  a_n T_2 v_n) = a_1 T_1 v_1 +  ... +  a_n T_1 v_n$$
Thus, if $v \in V$, then
$$S T_2 v = S (a_1 T_2 v_1 + ... + a_n T_2 v_n) = a_1 T_1 v_1 + ... + a_n T_1 v_n =
T_1(a_1 v_1 + ... + a_n v_n) = T_1 v$$

Given that $S'$ is injective and $S' \in (\range T_2, \range T_1) \to S' \in (\range T_2, W)$
(because $\range T_2 \subseteq W$) , we can follow by results of our previous exercise, that there
exists invertible  $S \in \map (W)$ such that
$$S(w) = S'(w)$$
and therefore
$$T_1 = S T_2$$
as desired.

\textbf{In reverse direction: }
Suppose that there exists invertible $S \in \map(W)$ such that $T_1 = S T_2$.
Thus if $v \in \ns T_1$, then we can follow that $S T_2 v = 0$. By invertability of $S$ we've got
that there exists $S \inv$ and therefore
$$S T_2 v = 0$$
$$S \inv S T_2 v = S \inv 0$$
$$T_2 v = 0$$
Thus, $v \in \ns T_2$. Therefore we can follow that $\ns T_1 \subseteq \ns T_2$. By the same
logic, but in other direction we've got that $\ns T_2 \subseteq \ns T_1$, thus
$$\ns T_1 = \ns T_2$$
as desired.


\subsection{}

\textit{Suppose that $V$ is finite-dimentional and $T_1, T_2 \in \map(V, W)$. Prove that
  $\range T_1 = \range T_2$ if and only if there exists an invertible operator $S \in \map (W)$
  such that $T_1 = T_2 S$.}

\textbf{In forward direction: }
Let $w_1, ... w_n$ be a basis of $\range T_1 = \range T_2$. Thus we can follow that there exists
basis $v_1, ... v_n \in V$ of preimage of $T_1$  and $v_1', ... v_n' \in V$  - basis of preimage
of $T_2$ such that $T_1 v_j = w_j = T_2 v_j'$.
Because those lists are linearly independent and have the same length, we
can follow that there exists an isomorphy $S \in \map (V)$ such that
$$S(a_1 v_1 + ... a_n v_n) = a_1 v_1' + ... a_n v_n'$$
Thus we can follow that for $v \in V$
$$T_2 S v = T_2 S (a_1 v_1 + ... a_n v_n) = a_1 w_1 + ... + a_n w_n = T_1 v$$
as desired.

\textbf{In reverse direction: }
Suppose that there exists $S \in \map (W)$ such that $T_1 = T_2 S$. Then it is obvious that
$\range T_1 = \range T_2$.

\subsection{}

\textit{Suppose that $V$ and $W$ are finite-dimentional and $T_1, T_2 \in \map (V, W)$. Prove that
  there exists invertible operators $R \in \map L(V)$ and $S \in L(W)$ such that
  $T_1 = S T_2 R$ if and only if $\dim \ns T_1 = \dim \ns T_2$.}

\textbf{In forward direction: }
By resullts of previous exercise we can follow that $\ns T_1 = \ns T_2 R$.
Thus by injectivity of $R$ we've got that $\dim \ns T_1 = \dim \ns T_2$.

\textbf{In reverse direction: }
Suppose that $\dim \ns T_1 = \dim \ns T_2$. We can follow that there exists isomorphism
$S \in \map (W)$ such that $\ns T_1 = \ns T_2 R$. Thus, by results of previous exercises we
can folllow that there exists $S \in \map (V)$ such that $T_1 = S T_2 R$, as desired.

\subsection{}

\textit{Suppose $V$ and $W$ are finite-dimentional. Let $v \in V$. Let }
$$E = \{T \in \map(V, W): Tv = 0 \}$$

\textit{(a) Show that $E$ is a subspace of $\map (V, W)$.}

Suppose that $S, T \in E$. Then for $v \in V$ we've got that 
$$(S + T)v = S(v) + T(v) = 0$$
thus $S + T \in E$ and $E$ is closed under addition.

Let $\lambda \in F$.  Then
$$(\lambda T)v = \lambda (T v) = \lambda 0 = 0$$
thus $(\lambda T) \in E$, therefore $E$ is closed under scalar multiplication. Given that
$\map (V, W)$ is a vector space we can follow that $E$ is a subspace, as desired.

\textit{(b) Suppose $v \neq 0$. What is $\dim E$?}

Extend $v$ to a basis $v, v_1 ... v_{n - 1}$  of $V$ and let $w_1, ..., w_n$ be
arbitrary basis of $W$. Because $Tv = 0$, we require that first column  of $M(T)$ will be
zeroes. Then we can follow that $\mathcal M$ is an
isomorphism between $E$ and $F^{m, (n - 1)}$. Thus
$$\dim E = (\dim V - 1)(\dim W)$$

\subsection{}

\textit{Suppose $V$ is finite-dimentional and $T: V \to W$ is a surjective linear map
  of $V$ onto $W$. Prove that there is a subspace $U$ of $W$ such that
  $T|_U$ is an isomorphism of $U$ onto $W$. }

Let $v_1, ..., v_n$ be a basis of $\ns V$. Then we can extend it to  $v_1, ..., v_n, u_1, ..., u_m$,
which will be a basis of $V$. Let $U = \Span(u_1, ..., u_m)$. We can follow by FTLM that
$\dim U = \dim \range T$. Also, because $U$ is surjective, we can follow that it is invertible
and therefore is isomorphism, as desired.

\subsection{}

\textit{Suppose $V$ is finite-dimentional and $S, T \in \map(V)$. Prove that $ST$ is invertible
  if and only if both $S$ and $T$ are invertible.}

\textbf{In forward direction: }

Suppose that $ST$ is invertible. Suppose that $T$ is not invertible. Then it is not injective.
Therefore theree exists $v \in V \neq 0$ such that $Tv = 0$. Therefore $STv = S0 = 0$, which
is a contradiction. Thus $T$ is injective and therefore invertible.

Suppose that $S$ is not invertible. Then we can follow that there exists $v \in V \neq 0$ such
that $Sv = 0$. Given that $T$ must be invertible we can follow that there exists $w \in V \neq 0$
such that $Tw = v$. Thus $STw = Sv = 0$. Therefore $ST$ is not injective, which is a contradiction.

Thus we can follow that in order for $ST$ to be invertible, both $S$ and $T$ must be invertible
as well, as desired.

\textbf{In reverse direction: }
Suppose that $S$ and $T$ are invertible. Then we can follow that both of them are injective.
Thus by some exercise 
in this chapter (looked it up, it's 3.2.11) $ST$ is injective as well. Thus $ST$ is invertible, as
desired.


\subsection{}

\textit{Suppose $V$ is finite0dimentional and $S, T \in \map(V)$. Prove that $ST = I$ if
  and only if $TS = I$.}

From previous exercise we can follow that both $S$ and $T$, as well as $ST$ and $TS$ are
invertible.

All of the following are equivalences and not  implications, therefore we can prove everything
in one go.
$$ST = I$$
$$STS = IS$$
$$STS = SI$$
$$TS = I$$

\subsection{}

\textit{Suppose $V$ is finite-dimentional and $S, T, U \in \map(V)$ and $STU = I$.
  Show that $T$ is invertible and that $T \inv = US$.}

Firstly I should state that we assume here that $S$ and $U$ are invertible, otherwise we default
to one of the previous exrecises. 

Suppose that $T$ is not invertible. Then it isn't injective, and therefore there exists
$v \in V \neq 0$ such that $Tv = 0$. Because $U$ is invertible we can follow that there exists
$w \in V \neq 0$ such that $Uw = v$. Thus
$$STUw = STv = S0 = 0 \neq w$$
Therefore $STU \neq I$, which is a contradiction.

Now we can use some algebra in here
$$STU = I$$
$$TU = S \inv I$$
$$TU = S \inv U \inv$$
$$T = S \inv U \inv$$
and by first exercise in this chapter
$$T \inv = (S \inv U \inv) \inv = US$$
as desired.

\subsection{}

\textit{Show that the result in the previous exercise can fail withoud the hypothesis that $V$
  is finite-dimentional}

We can set $U$ to be $I$,
$$T(x_1, x_2, ...) = (0, x_1, x_2, ...)$$
$$S(x_1, x_2, ...) = (x_2, x_3, x_4, ...)$$
and so on. Then $T$ will not be surjective, and therefore will not be invertible (which doesn't
follow from our usual equivalence, but by the fact that there does not exist a map such
that $TS = I$, because there is no way to represent (1, 1, ...))

\subsection{}

\textit{Suppose $V$ is a finite-dimentional vector space and $R, S, T \in \map(V)$. are
  such that $RST$ is surjective. Prove that $S$ is injective.}

Suppose that it isn't. We can follow that $RST$ is invertible.
Then we can follow that there exits $v \in V \neq 0$ such that
$Sv = 0$. By invertability of $T$ we follow that there exists $w \in V \neq 0$
such that $Tw = v$. thus
$$RSTw = RSv = R0 = 0$$
Thus $RST$ is not invertible and isn't surjective, which is a contradiction.

\subsection{}

\textit{Suppose $v_1, ..., v_n$ is a basis of $V$. Prove that the map $T: V \to F^{n, 1}$
  defined by }
$$Tv = \mathcal M (v)$$
\textit{is an isomorphism of $V$ onto $F^{n, 1}$.}

Only way to represent 0 in $M(v)$ is that if $v = 0$, therefore the map is injective. Also,
by common sense, map is surjective. Thus it is invertible and therefore it is an isomorphism.

\subsection{}

Trivial, seen simular in previous chapter

\subsection{}

\textit{Suppose $V$ is finite-dimentional and $T \in \map(V)$. Prove that $T$ is a scalar
  multiple of the identity if and only if $ST = TS$ for every $S \in \map (V)$.}

Forward direction is trivial.

Suppose that $ST = TS$ for every $S \in \map(V)$. Suppose that $Tv \neq \lambda v$ for some
$v \neq 0$ and $\lambda \in F$. Then we can follow that there exsits $S$ such that
$$S(Tv) = v$$
and
$$S(v) = 0$$
Thus
$$ S (Tv) = v \neq 0 = S(v) = T(Sv)$$

Thus we can follow that for every $v \in V$ there exists $a \in F$ such that $Tv = av$.

Now suppose that $v, w \in V$. If $v \neq \lambda w$, then
$$T(v + w) = a_{v + w}(v + w) = a_{v + w}v + a_{v + w}w = T(v) + T(w) = a_v v + a_w w$$
thus
$$a_v = a_w = a_{v + w}$$
by the unique representation of zero.

If $v = \lambda w$,  then
$$Tv = a_v v = T(\lambda w) = a_w \lambda w$$
$$a_v v =  a_w \lambda w$$
$$a_v v =  a_w v$$
thus $a_v = a_w$. Therefore for any given $v, w$ we follow that $a_v = a_w$. Thus, $T$ is a
scalar multiple of $I$, as desired. (Proof aquired after reading another proof in supplimentary
material).

\subsection{}

\textit{Suppose $V$ is finite-dimentional and $\mathcal E$ is a subspace of $\map(V)$ such
  that $ST \in  \mathcal E$ and $TS \in \mathcal E$ for all $S \in \map(V)$ and all
  $T \in \mathcal E$. Prove that $\mathcal E  = \{0\}$ or $\mathcal E = \map(V)$.}

It's trivial to proof that $\{0\}$ and $\map(V)$ are such supspaces. We also should
mention that in order for it not to be a trivial case we assume that $\dim V > 1$ (otherwise
we don't have any other subspaces other than $\map (V)$ and $\{0\}$).

Our strategy with this proof is to show that there exists an invertible map in $\mathcal E$.
If we've got that, then the rest of the proof is trivial.

Thus suppose that
$E \neq \{0\}$ and $E \neq \map(V)$. Then we can follow that there exists a map
$T \in E$ such that $T \neq 0$. Because we want to prove that there exists an invertible map
in $E$, suppose that $T$ is not invertible (otherwise $T T \inv = I \in \mathcal E$, and we
can skip to the later part).
Thus there exists a vector $v \in V \neq 0$ such that 
$Tv \neq 0$ and $w \in V \neq 0$ such that $Tw = 0$.
Therefore extend $v$ to a basis $v, v_1, ..., v_n$ of $V$ and make a map
$$S (v) = v$$.
$$S (v_k) = 0$$.
Then it follows that there exist maps $T S \in E_j$ such that
$$TS(a_0 v + a_1 v_1 + ... a_n v_n) = Ta_0 v = a_0 T v$$
Thus there exists a map 
$$Q_j (a_0 v + ... + a_n v_n) = a_j v_j$$
also in $\mathcal E$.
Thus we can follow that $I \in \mathcal E$. Thus we can follow that for every $S \in \map(V)$
we've got that
$$SI = S$$
is also in $\mathcal E$. Thus $\map(V) \subseteq \mathcal E$. Thus  we can follow that
$\map(V) = \mathcal E$, which is a contradiction.

Thus we can follow that $\mathcal E$ is either $\map(V)$ and $\{0\}$, as desired.

\subsection{}

\textit{Show that $V$ and $\map(F, V)$ are isomorphic vector spaces.}

For finite-dimentional spaces we've got that

$$\dim \map (F, V) = (\dim F)(\dim V) = 1 (\dim V) = \dim V$$

Thus they are isomorphic.

Otherwise we suppose that $v_1, ...$ is a basis of $V$. Given that
$$v = \sum a_j v_j$$
we can follow that we've got bijectivity between $\map(F, V)$ and $V$, as desired.

\subsection{}

\textit{Suppose $T \in \map(P(R))$ is such that $T$ is injective and $\deg Tp \leq \deg p$
  for every nonzero polynomial $p \in P(R)$.}

\textit{(a) Prove that $T$ is surjective.}

Because $T$ is injective, we can follow that $\ns T = \{0\}$ (looked it up, this one applies
to any vector space).

Now let's try to prove that $\deg Tp = \deg p$.
We probably don't need the induction here, but we'll use it anyways.

For $p = 0$ we've got that $Tp = 0$. Let us prove that this is the case also
for $\deg p = 0$, just in case. Suppose that $\deg p = 0$ and $\deg Tp < 0$, then $\ns T$ is
not equal to zero, which is a contradiction.

For inductive step let us assume that $\deg Tp = \deg p$ for $p$ such that  $\deg p = n - 1$.
Now suppose that $\deg p = n$ and $\deg Tp < n$. By our assumption,  it follows that there exists
a basis $p_0, ..., p_{n - 1}$ such that $\deg T p_j = j$. Thus we can follow that $T p_0, ... T p_n$
spans $P_{n - 1}(R)$. Thus we follow that there exists $a_0, ... a_{n - 1}$ such that
$$a_0 T p_0 + ... a_{n - 1} T p_{n - 1} = T p$$
then it follows that $T$ is not injective, which is a contradiction.

This concludes the proof that $\deg p = \deg Tp$. By this we can follow that
given $p \in P(R)$ with $\deg p = n$ there exists $p_0, ... p_n$ and by extension
$T p_0, ... Tp_n$ such that $p \in \range(T p_0, ... Tp_n)$. Thus there exists
$p' \in P(R)$ such that $Tp' = p$. Thus $T$ is surjective, as desired (this dragged
on for waaay too long).

\subsection{}

not gonna repeat the text of the exercise, but it basically reduces to our usual eqivalence
of surjectivity/injectivity on finite-dimentional operators.

\section{Products and Quotients of Vector Spaces}

\subsection{}

\textit{Suppose $T$ is a function from $V$ to $W$. The graph of $T$ is the subset of
  $V \times W$ defined by }
$$\text{graph of } T = \{(v, Tv) \in V \times W: v \in V\}$$
\textit{Prove that $T$ is a linear map if and only if the graph of $T$  is a subspace
  of $V \times W$.}

Firstly, $G(T)$ denotes the graph of $T$. 

\textbf{In forward direction: }
Suppose that $T$ is a linear map. Suppose that $v, w \in G(T)$. Then it follows that
$$v + w = (v + w, T(v + w))$$
Thus $G(T)$ is closed under addition.
$$\lambda v = (\lambda v, \lambda Tv)$$
thus it is also closed under scalar multiplication. Given that $G(T)$ is a subset of
a product of vector spaces, which is a vector space, we follow that $G(T)$ is a subspace,
as desired.

\textbf{In reverse direction: }

Suppose that $G(T)$ is a subspace of $V \times W$. Suppose that $v, w \in V$. Then
$$T(v + w) = T(v) + T(w)$$
and
$$\lambda T(v) = T(\lambda v)$$
by properties of a product of vector spaces. Thus we can follows that $T$ is linear, as desired.

\subsection{}

\textit{Suppose $V_1, ..., V_m$ are vector spaces such that $V_1 \times ... \times V_m$ is
  finite-dimentional. Prove that $V_j$ is finite-dimentional for each $j = 1, ..., m$.}

We can follow that the combined list of bases of $V_n$'s spans $V_1 \times ... \times V_m$
and is linearly independent. Given that this list is finite, we
can follow that  $V_1 \times ... \times V_m$ is finite-dimentional, as desired.


\subsection{}

\textit{Give an example of a vector space $V$ and subspaces $U_1, U_2$ of $V$ such that
  $U_1 \times U_2$ is isomorphic to $U_1 + U_2$, but $U_1 + U_2$ is not a direct sum.}

$$U_1 = (0, x_1, 0, x_2, 0, x_3, ...)$$
$$U_2 = (x_1, x_2, x_3, 0, x_4 ...)$$

\subsection{}

\textit{Suppose $V_1, ... V_m$ are vector spaces. Prove that $\map(V_1 \times ... \times V_m, W)$
  and $\map(V_1, W) \times ... \times \map(V_m, W)$ are isomorphic vector spaces.}

For finite-dimentional spaces we can just follow the standard equations for calculating the
dimentions of given spaces and produce the desired result.


Conversely, if any of the spaces is infinite-dimentional, then we gotta produce the bijective
map between the two spaces.


Our strategy here will be to prove that there exists a bijective map between the two.
Let $T: \map(V_1 \times ... \times V_m, W) \to \map(V_1, W) \times ... \times \map(V_m, W)$
be defined by

$$T(S) = S_1 \times ... \times S_n$$
where
$$S_j(v_j) = S(0, ..., v_j, .., 0) = w_j$$
thus
$$T(\lambda S) = \lambda S_1 \times ... \times \lambda S_n$$
and
$$T(S + M) = (S + M)_1 \times ... \times (S + M)_n = S_1 \times ... \times S_n +
M_1 \times ... \times M_n$$
thus it is linear.

Injectivity and surjectivity are given with this one. Thus we follow that there exists a linear
bijectivity between the two, thus they are isomorphic.

We can actually follow here that if the space is infinite-dimentional, then they are isomorhic
by default.

\subsection{}

\textit{Suppose $W_1, ... W_m$ are vector spaces. Prove that
  $\map(V, W_1 \times  ... W_n)$ and $\map(V, W_1) \times ... \times \map(V, W_n)$ are isomorphic }

Same logic as in previous ones works on this oen too.

\subsection{}

\textit{For a posisitivee integer $n$, define $V^n$ by}
$$V^n = V \times ...\text{ n times }... V$$
\textit{Prove that $V^n$ and $\map(F^n, V)$ are isomorphic vector spaces.}

$$\dim \map(F^n, V) = (\dim F^n) ( \dim V) =  n \dim V = \dim V^n$$
as desired.

\subsection{}

\textit{Suppose $v, x$ are vectors in $V$ and $U, W$ are subspaces of $V$ such that
  $v + U = x + W$. Prove that $U = W$.}

Let $v'= x - v$. It follows that $x = v + v'$. Given that $v + U = x + W$ we follow
that there exists $-w \in W$ such that $x - w = v + 0$. Thus
$$x - w = v + 0$$
$$v + v' - w = v$$
$$v' - w = 0$$
$$v' = w$$
Thus we follow that $v' \in W$. Therefore we've got that
$$x + W = v + v' + W = v + W = v + U$$
thus we follow that
$$W = U$$
as desired.

\subsection{}

\textit{Prove that a nonempty subset $A$ of $V$ is an affine subset of $V$ if and only if
  $\lambda v + (1 - \lambda)w \in A$ for all $v, w \in A$ and all $\lambda \in F$.}

\textbf{In forward direction: }

Suppose that $A$ is an affine subset of $V$. Then we follow that there exists vector
$x \in V$ and subspace $U \subseteq V$ such that
$$A = x + U$$
Now let $v, w \in A$. Then we can follow that there exist $v', w' \in U$ such
that
$$v = x + v'$$
$$w = x + w'$$

Thus we follow that

$$\lambda v + (1 - \lambda) w = \lambda x + \lambda v' + x + w' - \lambda x - \lambda w' =
\lambda v' + x + w' - \lambda w' $$
Because $v', w' \in U$ we follow that
$$\lambda v' + w' - \lambda w' \in U$$
thus
$$\lambda v' + x + w' - \lambda w' \in A$$
as desired.

\textbf{In reverse direction: }

Suppose that for every $\lambda \in F$ and $v, w \in A$ we've got that
$$\lambda v + (1 - \lambda) w \in A $$

Fix $x \in A$ and suppose that $v, w \in A$. Then it follows that
$$ \lambda(v  - x) = \lambda v  - \lambda x =
\lambda v  - \lambda x + x - x =
(\lambda v  - (1 - \lambda) x) - x
$$
Thus space $A - x$ is closed under scalar multiplication. And
$$ (v - x) + (w - x) = 2(v/2 + w/2 - x) = 2(\frac{1}{2} v + (1 - \frac 1 2) w - x) $$
by the fact that $v, w \in A \to \frac{1}{2} v + (1 - \frac 1 2) w \in A$ we follow that
$(\frac{1}{2} v + (1 - \frac 1 2) w - x) \in A - x$. Thus, by the fact that
$A - x$ is close under scalar multiplication we follow that
$$2(\frac{1}{2} v + (1 - \frac 1 2) w - x) \in A - x$$
thus we can conclude that $A - x$ is a subspace of $V$. Thus, $A - x + x = A$ is an affine
subset of $V$, as desired.

\subsection{}

\textit{Suppose $A_1$ and $A_2$ aare affine subsets of $V$. Prove that the intersection
  $A_1 \cap A_2$ is either an affine subset of $V$ or the empty set.}

Fisrtly, let us denote that
$$A_1 = x_1 + U_1$$
$$A_2 = x_2 + U_2$$

Firstly, if $A_1$ and $A_2$ are parallel, then they are either equal to each other (in which
case their intersection is an affine subset), or empty. Thus we can follow that
intersection of two affine subsets can be empty.

Suppose that their intersection is nonempty and let $x \in A_1 \cap A_2$. Then it follows
that $U_1 = A_1 - x$ and $U_2 = A_2 + x$. Intersection of two vector spaces is a vector
space, thus we follow that
$$x + U_1 \cap U_2 = A_1 \cap A_2$$
is an affine space, as desired.

\subsection{}

\textit{Prove that the intersection of every collection of affine subsets of $V$ is either
  an affine subset of $V$ or the empty set}

It follows by induction from the previous exercise.

\subsection{}

\textit{Suppose $v_1, ..., v_m \in V$. Let }
$$A = \{\lambda_1 v_1 + ... + \lambda_m v_m: \lambda_{[1, m]} \in F \text{ and }
\sum \lambda_j = 1\}$$

\textit{(a) Prove that $A$ is an affine subset of $V$.}

Let $v, m \in A$. Then it follows that
$$\kappa v + (1 - \kappa) w =
\kappa (\lambda_1 v_1 + ... + \lambda_m v_m) + (1 - \kappa)
(\lambda_1' v_1 + ... + \lambda_m' v_m) =
$$
$$=
\kappa (\lambda_1 v_1 + ... + \lambda_m v_m) + (1 - \kappa)
(\lambda_1' v_1 + ... + \lambda_m' v_m) = (\kappa \lambda_1 + (1 - \kappa) \lambda_1') v_1 + ...
(\kappa \lambda_n + (1 - \kappa) \lambda_n') v_n $$
Thus the sum of the coefficients is equal to
$$\sum{(\kappa \lambda_n + (1 - \kappa) \lambda_n')} =
\sum{(\kappa \lambda_n)} + \sum{[(1 - \kappa) \lambda_n']} =
\kappa \sum{( \lambda_n)} + (1 - \kappa) \sum{[ \lambda_n']} =
\kappa  + (1 - \kappa)  = 
$$
$$
= 1
$$
Thus we follow that $A$ is an affine subset by equivalence, that was proven in a couple
of exercises above.

\textit{(b) Prove that every affine subspae of $V$ that contains $v_1, ..., v_m$ also
  contains $A$}

Suppose that some affine subset $B$ contains $v_1, ..., v_m$.
Let $a \in A$. 

We know that there exists $U \subseteq V$ such that $v_1 + U = A$ and $W \in V$ such
that $B = v_1 + W$.
Thus we follow that $v_2 - v_1 \in B - v_1$, ..., $v_n - v_1 \in B - v_1$. Thus
$$\Span(v_2 - v_1, ... v_n - v_1) \subseteq B - v_1$$
Suppose that $a \in A$. Then
$$a - v_1 \in \Span(v_2 - v_1, ... v_n - v_1)$$
Thus
$$a - v_1 \in B$$
as desired.

\textit{(c) Prove that $A = v + U$ for some $v \in V$ and some subspace $U$ of $V$
  with $\dim U \leq m - 1$}

We know, that the list $(v_2 - v_1, ... v_m - v_1)$ spans $A - v_1$. Therefore we can follow
that $\dim U \leq m - 1$.

\subsection{}

\textit{Suppose $U$ is a subspace of $V$ such that $V/U$ is finite-dimentional. Prove that
  $V$ is isomorphic to $U \times (V / U)$.}

For finite-dimentional case is trivial, for infinite-dimentional we have isomorphism by default.

\subsection{}

\textit{Suppose $U$ is a subspace of $V$ and $v_1 + U, ..., v_m + U$ is a basis of $V/U$
  and $u_1, ..., u_n$ is a basis of $U$. Prove that $v_1, ..., v_m, u_1, ..., u_m$ is a basis
  of $V$.}

Because $v_1 + U, ..., v_m + U$ is a basis of $V/U$, we can follow that $v_1, ..., v_m$ is
linearly independent in $V$.

If $v_j \in U$, then $v_j + U = U = 0 + U$, therefore we follow that $v_j \notin U$.

Thus we can follows that $v_1, ..., v_n, u_1, ..., u_m$ is a linearly independent list of
vectors.

Given that $v_1, ..., v_n, u_1, ..., u_m$ is a list of vectors in $V$, we can follow that
$\Span(v_1, ..., v_n, u_1, ..., u_m)$ is a subspace of $V$.

Now suppose that $v \in V$ and $v \notin \Span(v_1, ..., v_n, u_1, ..., u_m)$. We can follow that
$v \notin U$. Thus we can follow that $v + U$ is an element of $V/U$. In this case we
follow that $v \in \Span(v_1, ..., v_n)$, which is a contradiction. Thus we conclude that
such a vector does not exist and therefore $V = \Span(v_1, ..., v_n, u_1, ..., u_m)$.
Given that $v_1, ..., v_n, u_1, ..., u_m$ is linearly independent, we follow that
it is a basis of $V$, as desired.

\subsection{}

\textit{Suppose
  $U = \{(x_1, x_2, ...) \in F^{\infty}: x_j \neq 0 \text{ for only finitely many }j\}$ }

\textit{(a) Show that $U$ is a subspace of $F^{\infty}$}

Let $v, u \in U$. We can follow, that sinse finitely many x's are not zero, then we
follow that for $v + u$
$$\{j_1, ..., j_n\} \cup \{j_1', ..., j_n'\} $$
is a set of positions, in which the $v + u$ might not be zero. Since union of finite sets is
finite, we follow that $U$ is closed undeer addition. The same reasoning, but applied to
intersection, rather then union, can be applied to  get closure for multiplication.
Thus we follow that $U$ is a subspace of $F^\infty$.

\textit{(b) Prove that $F^\infty/U$ is infinite-dimentional}

We can follow that there exist
$$x_j = (0, 0, 0, ...\text{ j times }... 0, 1, 1, 1...)$$
such that each $x_j$ is linearly independent from one another. Since $x_j \notin U$, we can
follow that there is no basis of $F^\infty/U$, therefore it is infinite-dimentional, as desired.


\subsection{}

\textit{Suppose $\phi \in \map(V, F)$ and $\phi \neq 0$. Prove that $\dim V / (\ns \phi) = 1$}

Because $\phi \neq 0$, we can follow that there exists $v \in V$ such that
$$\phi v \neq 0$$
thus we follow that $\dim \range \phi = 1$. Given that $V / \ns \phi$ is isomorphic to
$\range \phi$, we follow that its dimention is also one, as desired.

\subsection{}

\textit{Suppose $U$ is a subspace of $V$ such that $\dim V/U = 1$. Prove that there exists
  $\phi \in \map(V, F)$ such that $\null \phi = U$.}

Given that $\dim V/U = 1$ we can follow that there exists $v \in V$ such that $v \notin U$.
Thus $v$ is a basis for $V/U$. Thus we can define $g: V/U \to F$
$$g(kv + U) = k$$

Then, by plugging $\pi: V \to V/U$ such that  $\pi(v) = v + U$ and
making
$$\phi(v) = g(\pi(v))$$
we get $\phi: V \to F$ such that $u \in U \to \phi(u) = 0$. Thus $U \subseteq \ns \phi$.

Then suppose that $v \neq 0$ and $v \notin U$. Then it follows that $\phi(v) \neq 0$. Thus
we can conclude that $\ns \phi = U$, as desired.

\subsection{}

\textit{Suppose $U$ is a subspace of $V$ such that $V/U$ is finite-dimentional. Prove that
  there exists a subspace $W$ of $V$ such that $\dim W = \dim V/U$ and
  $V = U \oplus W$.}

Because $V/U$ is finite-dimentional, we can follow that there exists a basis of $V$.
$$v_1 + U, ..., v_n + U$$
We can follow that $v_1, ..., v_n$ is linearly independent. It is also will be helpful
later to mention that none of $v_j$'s are in $U$. Then, define
$$W = \Span(v_1, ..., v_n)$$

Suppose that $v \in V$. Then we can follow that $v \in v' + U$ for some $v$.
Thus we follow that $v' = a_1 v_1 + ... a_n v_n$ and therefore
there exists $u \in U$ and $v' \in W$ such that $v = u + v'$. Thus $V = W + U$.

Suppose that $w \in W$ and $w \in U$. Then it follows that
$$w = a_1 v_1 + ... a_n v_n$$
given that none of  $v_1, ... v_n$ are in $U$, we follow that the only way that it is
possible is when $a_1, ..., a_n = 0$. Thus we follow that
$$W \cap U = \{0\}$$.
Thus we can conclude that
$$V = W \oplus V$$
as desired.




\subsection{}

\textit{Suppose $T \in \map(V, W)$ and $U$ is a subspace of $V$. Let $\pi$ denote the
  quotient map from $V$ onto $V/U$. Prove that there exists $S \in (V/U, W)$
  such that $T = S \circ \pi$ if and only if $U \subset \ns T$. }

\textbf{In forward direction: }

Suppose that there exists $S$ such that $T = S \circ \pi$.
Let $u \in U$. Then it follows that
$$\pi(u) = u + U = 0 + U$$

Because $S$ is a linear function, we follow that $S(0) = S(U) = 0$. Thus we follow that
$u \in \ns T$. Thus we can conclude that $U \subseteq T$

\textbf{In reverse direction: }

Suppose that $U \subseteq \ns T$. Then we follow that $\pi(U) = 0$. Create a map
$S': V/U \to V/U$ such that $\ns (S' \circ \pi ) = \ns T$. Then there exists
an invertible operator $R$ such that $R S' \circ \pi = T$. Thus we follow that
there exists a map $RS' = S$ such that $S \circ \pi = T$, as desired.

\subsection{}

\textit{Find a correct statement analogous to 3.78 that is applicable to finite sets, with
  unions analogous to sums of subspaaces and disjoint unions analagous to direct sums }

Suppose $A$ is a finite set and $U_1, ..., U_m$ are subsets of $A$. Then
$$U_1 \cap ... \cap U_n = A$$
is a disjoint union if and only if
$$\sum |U_j| = |A|$$

\subsection{}

\textit{Suppose $U$ is a subspace of $V$. Define $\Gamma: \map(V/U, W) \to \map(V, W)$ by}
$$\Gamma(S) = S \circ \pi$$

\textit{(a) Show that $\Gamma$ is a linear map}

Let $S, T \in \map(V/U, W)$. Then
$$\Gamma(S + T) = (S + T) \circ \pi = S \circ \pi + T \circ \pi = \Gamma(S) + \Gamma(T)$$

If $\lambda \in F$, them
$$\Gamma(\lambda S) = (\lambda S) \circ \pi = \lambda S \circ \pi = \lambda \Gamma(S)$$
Therefore we can follow that $\Gamma$  is linear

\textit{(b) Show that $\Gamma$ is injective}

Let $\Gamma(T) = 0$. It follows that
$$T \circ \pi = 0$$
Then we can follow that for $v \in V$
$$T \circ \pi(v) = 0 = T(v + U)$$
Thus $T = 0$. Therefore we follow that $\ns \Gamma = \{0\}$, therefore it is injective.

\textit{(c) Show that $\range \Gamma = \{T \in \map(V, W): Tu = 0 \text{ for every } u \in U \}$}

Suppose that $T \in \range \Gamma$. Then it follows that there exists $S \in \map(V/U, W)$
such that
$$S \circ \pi = T$$
thus if $u \in U$ then
$$S \circ \pi (u) = S(u + U) = S(U) = 0$$

Suppose that $S \in \{T \in \map(V, W): Tu = 0 \text{ for every } u \in U \}$. Then
it follows by the results in exercise 18, that there exists $T \in \map(V/U, W)$ such
that
$$T = S \circ \pi$$
therefore by double inclusion we've got that 
$\range \Gamma = \{T \in \map(V, W): Tu = 0 \text{ for every } u \in U \}$
as desired.


\section{Duality}

\subsection{}

\textit{Explain why every linear functional is either surjective or the zero map}

Dimention of its codomain is 1, therefore we've got that range is either 1 or 0. In former
case it is surjective, in latter it's a null map.

\subsection{}

\textit{Give three distinct examples of linear functionals on $R^{[0, 1]}$}

$$f(1) - f(0)$$
$$f(0.5)$$
$$f(0.2) + f(0.3)$$

\subsection{}

\textit{Suppose $V$ is finite-dimentional and $v \in V$ with $v \neq 0$. Prove that there exists
  $\phi \in V'$ such that $\phi(v) = 1$}

We can extend $v$ to a basis $v, v_1, ..., v_n$ of $V$, then define the dual basis
$\phi, \phi_1, ..., \phi_n$ on this basis, and then get the desired function.

\subsection{}

\textit{Suppose $V$ is finite-dimentional and $U$ is a subspace of $V$ such that $U \neq V$. Prove
  that there exists $\phi \in V'$ such that $\phi = 0$ for every $u \in U$ but $\phi \neq 0$.}

Simular to the previous one, we define $u_1, ..., u_n$ to be the basis of $U$, then expand it
to a basis of $V$ - $v_1, ..., v_m, u_1, ..., u_n$, and define dual basis on this basis.

The property that $U \neq V$ guaranteed that there exists $v_1$, therefore
there exists $\phi_1$ such that
$$\phi_1(v_1 ) = 1$$
thus $\phi_1 \neq 0$. And by definition of dual basis we get that $\phi_1 (u_j) = 0$.
Thus we follow that
$$u \in U \to u \in \Span(u_1, ..., u_n) \to \phi(u) = \phi(a_1 u_1) + ... + \phi(a_n u_n) = 0$$
as desired.

\subsection{}


\textit{Suppose $V_1, ..., V_m$ are vector spaces. Prove that $(V_1 \times  ... \ V_m)'$
  and $V_1' \times ... \times V_m'$ are isomorphic vector spaces.}

We follow it from the exercise 3.5.4.

\subsection{}

\textit{Suppose $V$ is finite-dimentional and $v_1, ..., v_m \in V$. Define a linear
  map $\Gamma: V' \to F^m$ by}

$$\Gamma(\phi) = (\phi(v_1), ..., \phi (v_m))$$

\textit{(a) Prove that $v_1, ..., v_m$ spans $V$ if and only if $\Gamma$ is injective.}

\textbf{In forward direction: }
Suppose that $v_1, ... v_m$ spans $V$. Let $\phi \in V'$ be such that
$$\Gamma(\phi) = 0$$
Then we follow that
$$\Gamma(\phi) = (0, 0, ..., 0)$$
thus for $1 \leq j \leq m$ we've got that
$$\phi(v_j) = 0$$
Because $v_1, ..., v_m$ spans $V$ we follow that if $v \in V$ then there exist
$a_1, ..., a_m$ such that
$$v = \sum{a_j v_j}$$
thus we follow that
$$\phi v = \sum {\phi v_j} = 0$$
for any $v \in V$. Thus $\phi = 0$. Therefore we've got that
$$\Gamma \phi = 0 \to \phi = 0$$
thus we can follow that $\Gamma$ is injective.

\textbf{In reverse direction: }
Suppose that $\Gamma$ is injective. We've going to proceed with a proof by contradiction on
this one.

Suppose that $v_1, ..., v_m$ does not span $V$. Then we follow that we can create 
linearly independent list  $v_1, ..., v_n$ by removing some elements from $v_1, ..., v_m$, if
it is not already linearly independent. Then we can expand this list to
$v_1, ..., v_n, v_1', ...$ - basis of $V$. Then we can create a dual basis for this basis
and get our $\phi'$, which will be a dual basis for $v_1'$. Given that $\phi'(v_j) = 0$
by definition of the dual basis, we follow that
$$\Gamma(\phi') = 0$$
Given that $\phi'(v_1') \neq 0$ we follow that $\phi' \neq 0$. Thus
$\ns \Gamma \neq \{0\}$, therefore it is not injective, which is a contradiction.

\textit{(b) Prove that $v_1, ..., v_m$ is linearly independent if and only if $\Gamma$
  is surjective }

\textbf{In forward direction: }
Suppose that $v_1, ..., v_m$ is linearly independent. Then  we follow that there exists
a dual basis $\phi_1, ..., \phi_m$ of $v_1, ..., v_m$. Thus we follow that range of $\Gamma$
has
$$(\phi_1(v_1), ..., \phi_1(v_m)) = (1, 0, ..., 0)$$
Therefore we can conclude that range of $\Gamma$ contains a list of length  $m$  of linearly
independent vectors. Given that $\dim (F^m) = m$, we follow that $F^m = \range \Gamma$. Thus
$\Gamma$ is surjective, as desired.

\textbf{In reverse direction: }
Suppose that $v_1, ..., v_m$ are linearly independent. Then we follow
that there exists a space $\Span(v_1, ..., v_m)$, for which $v_1, ..., v_m$ is a basis.
Thus we follow that for this basis there exists a dual basis $\phi_1, ..., \phi_m$,
where $\phi_j \in V'$. Thus we follow that
$$\Gamma(\phi_1) = (\phi_1(v_1), ..., \phi_1(v_m)) = (1, 0, ... 0) \in \range \Gamma$$
$$\Gamma(\phi_j) = (\phi_j(v_1), ..., \phi_j(v_j), ..., \phi_1(v_m)) = (0, ..., 1, ... 0)
\in \range \Gamma $$
Thus we follow that there exist a linearly independent list of length $m$ in $\range \Gamma$. Thus
we can follow that the dimention of $\range \Gamma$ is at least $m$. Then we can state, that
because $\dim F^m = m$, $\range \Gamma$ is a subspace of $F^m$ and the fact that
a subspace has a dimention less or equal to the original space, we can follow that
$\range \Gamma = F^m$. Thus we follow that $\Gamma$ is surjective, as desired.

\subsection{}

\textit{Suppose $m$ is a positive integer. Show that the dual basis of the basis
  $1, x, x^2, ..., x^m$ of $P_m(R)$ is $\phi_0, ..., \phi_m$ where
  $\phi_j(p) = \frac{p^{(j)}(0)}{j!}$.}

It's straightforward to check that $(x^j)^{(j)} =  j!$ (proof by induction will be useful in this
case). Thus we follow that
$$\phi_j(x^j) = 1$$
If $k < j$, then we can follow that
$$\phi_j(x^k) = 0$$
and if $k > j$, then 
$$\phi_j(x^k) = \frac{k!}{j!}0^{k - j} = 0$$
thus we get that
$$\phi_j(p_k) =
\begin{cases}
  0 \text{ if } k \neq j \\
  1 \text{ if } k = j 
\end{cases}
$$
thus it is a dual basis by definition.

\subsection{}

\textit{Suppose $m$ is a positive }

\textit{(a) Show that $1, x - 5, ..., (x - 5)^m$ is a basis for $P_m(R)$.}

All of them are linearly independent, since all of them  have different degrees, and the fact
that length of the list is equal to the dimention of the space that it is in, implies that
it is a basis for this space.

\textit{(b) What is a dual basis of the basis in part (a)}

If we try to create this basis by drawing inspiration from the previous exercise, then we'll
get
$$\phi_j(p_k) = \frac{p_k^{(j)}(5)}{j!}$$
some basic implication will show that it is indeed the dual basis for given basis.

\subsection{}

\textit{Suppose $v_1, ..., v_m$ isi a basis of $V$ and $\phi_1, ..., \phi_n$ is the
  corresponding dual basis of $V'$. Suppose $\psi \in V'$. Prove that}
$$\psi = \sum \psi(v_j) \phi_j$$

We know that beacause $\phi_1, ..., \phi_m$ is a basis of $V'$ that
$$\psi = \sum a_j\phi_j $$
for some $a_1, ..., a_m \in F$. Thus we follow that
$$\psi(v_j) =  \sum a_j \phi_j(v_j) = \sum_{k \neq j} {[a_k \phi_k(v_j)]} + a_j \phi_j(v_j) =
0 +  a_j * 1 =  a_j$$
thus
$$\psi = \sum {a_j \phi_j} = \sum{ \psi(v_j) \phi_j}$$
as desired.

\subsection{}

\textit{Prove the first two bullet points in 3.101}

Specifically we want to prove that
$$(S + T)' = S' + T'$$
$$(\lambda T)' = \lambda T'$$

$$(S + T)'(\phi) = \phi (S + T) = \phi S + \phi T = S' + T'$$
where the second equality comes from distributive properties of linear maps.
$$(\lambda T)'(\phi) = \phi (\lambda T) = \lambda \phi T = \lambda T'$$

\subsection{}

\textit{Suppose $A$ is an $m$-by-$n$ matrix with $A \neq 0$. Prove that the rank of $A$ is
  $1$ if and only if there exist $(c_1, ..., c_m) \in F^m$ and $(d_1, ..., d_n) \in F^n$
  such that $A_{j, k} = c_j d_k$ for every $j = 1, ..., m$ and every $k = 1, ..., n$.}

\textbf{In forward direction: }
Suppose that rank of $A$ is equal to 1. Then we follow that the dimension of span of columns is 1.
Thus we follow that every column is a scalar multiple of the first non-zero column. Thus
we can set
$$v_j = a_j v_m$$
where $v_m$ is the first non-zero  column and $a_j$ is corresponding coefficient. Thus
we can create $(1, a_2, ..., a_m)$ - vector of corresponding multiplicities. Thus we can conclude
that there exists vectors $v_1$ and $(1, a_2, ..., a_m)$ with the desired properties.

\textbf{In reverse direction: }
Suppose that there exist two vectors, as defined in the exercise. Then we can follow that
every column is a scalar multiple of the first non-zero column. Therefore we follow that
the column rank of given matrix is 1, as desired.

\subsection{}

\textit{Show that the dual map of the identity map on $V$ is the identity map on $V'$}

Let $I'$ be the dual map of the identity. Then we can follow that for every $\phi \in V'$
$$I'(\phi) = \phi I = \phi$$
thus $I'$ is the identity on $V'$, as desired.

\subsection{}

\textit{Define $T: R^3 \to R^2$ by $T(x, y, z) = (4x + 5y + 6z, 7x + 8y + 9z)$. Suppose
  $\phi_1, \phi_2$ denotes the dual basis of the standart basis of $R^2$ and
  $\psi_1, \psi_2, \psi_3$ denotes the dual basis of the standard basis of $R^3$.}

\textit{(a) Describe the linear functions $T'(\phi_1)$ and $T(\psi_2)$}

We can follow that
$$M(T) = 
\begin{pmatrix}
  4 & 5 &  6 \\
  7 & 8 & 9
\end{pmatrix}
$$
thus 
$$M(T') = (M(T))^t = 
\begin{pmatrix}
  4 & 7 \\
  5 & 8 \\
  6 & 9 \\
\end{pmatrix}
$$
also, $\phi_1$ is represented by $(1, 0)$ and $\phi_2$ is represented by $(0, 1)$.
Therefore
$$M(T'(\phi_1)) =
\begin{pmatrix}
  4 &
  5 &
  6 
\end{pmatrix}
$$
and
$$M(T'(\phi_2)) =
\begin{pmatrix}
  7 &
  8 &
  9 
\end{pmatrix}
$$
thus
$$T'(\phi_1) = 4x + 5y + 6z$$
$$T'(\phi_2) = 7x + 8y + 9z$$

\textit{(b) Write $T'(\phi_1)$ and $T'(\phi_2)$ as linear combinations of
  $\psi_1, \psi_2, \psi_3$.}

Given that
$$\psi_1 = (1, 0, 0)$$
$$\psi_2 = (0, 1, 0)$$
$$\psi_3 = (0, 0, 1)$$
we follow that

$$T'(\phi_1) = 4 \psi_1 + 5 \psi_2 + 6 \psi_3$$
$$T'(\phi_2) = 7 \psi_1 + 8 \psi_2 + 9 \psi_3$$

\subsection{}

\textit{Define $T: P(R) \to P(R)$ by $(Tp)(x) = x^2p(x) + p''(x)$ for $x \in R$.}

\textit{(a) Suppose $\phi \in P(R)'$ is defined by $\phi(p) = p'(4)$. Describe the linear
  functional $T'(\psi)$ on $P(R)$.}

$$T'(\psi)(p) = \psi (Tp) = \phi(x^2p + p''(x)) = 2xp + x^2p + p'''(x)$$

\textit{(b) Suppose $\phi \in P(R)'$ is defined by $\phi(p) = \int_0^1{p(x) dx}$. Evaluate
  $(T'(\phi))(x^3)$}

$$T'(\phi)(p) = \phi(Tp) = \phi( x^2p + p''(x)) = \phi( x^2 x^3 + 6x) = \phi(x^5 + 6x) = $$
$$ =  \int_0^1{x^5 + 6x} = [x^6/6 + 3x^2]_0^1 = 1/6 + 3 = 3 \frac{1}{6}$$

\subsection{}

\textit{Suppose $W$ is finite-dimentional and $T \in \map(V, W)$. Prove that $T' = 0$
  if and only if $T = 0$.}

\textbf{In forward direction: }
Suppose that $T' = 0$. Then we can follow that $\phi T (v) = 0$ for every
$\phi \in W'$ and $v \in v$. Suppose that $T \neq 0$. Then there exists $v \in V$ such that
$Tv \neq 0$. Thus we can create a basis $Tv, w_1, ..., w_m$ of $W$ and define the dual
basis of this basis. Thus we follow that there exists $\phi_1$ such that 
$$\phi_1(Tv) = 1$$
Thus
$$\phi_1(Tv) = T'(\phi)(v) \neq 0$$
therefore we follow that $T' \neq 0$, which is a contradiction. Thus we can conclude that
$T' = 0$ implies that $T = 0$.

\textbf{In reverse direction: }
Suppose that $T = 0$. Then we follow that $\phi Tv = T'(\phi)(v) = \phi 0 = 0$. Thus we
conclude that for any $\phi \in V$ $T'(\phi) = 0$. Thus $T' = 0$.

\subsection{}

\textit{Suppose $V$ and $W$ are finite-dimentional. Prove that the map that takes
  $T \in \map(V, W)$ to $T' \in \map(W', V')$ is an isomorphism of
  $\map(V, W)$ onto $\map(W', V')$.}

Let us denote this map as  $\pi: \map(V, W) \to \map(W', V')$

We know that
$$\pi(S + T) = (S + T)' = S' + T' = \pi S + \pi T$$
and
$$\pi(\lambda T) = (\lambda T)' = \lambda (T') = \lambda (\pi T)$$
thus we can follow that $\pi$ is a linear map.

We've already proven that $T = 0$ if and only if $T' = 0$, thus we can follow that
given map is injective.

Because $V$ and $W$ are finite-dimentional, we can follow that $\map(V, W)$ and $\map(W', V')$ are
finite dimentional. Thus we can make bases of those vector spaces
$$\kappa_1, ..., \kappa_m \in \map(V, W)$$
$$\gamma_1, ..., \gamma_n \in \map(W', V)$$
and create an invertible map $\zeta \in \map(\map(V, W) , \map(W', V'))$ such that
$$\zeta (\sum a_j \kappa_j) = \sum a_j \gamma_j$$
therefore we can follow that $\pi \zeta^{-1}$ is an operator on $\map(W', V')$. By invertibility
of $\zeta$ we follow that $\zeta^{-1}$ is invertible as well and therefore it is injective.
Thus $\pi \zeta^{-1}$ is a composition of injective linear maps, and therefore it is itself
injective. Given that $\pi \zeta^{-1}$ is an operator on a finite-dimentional vector space, we
follow that its injectivity implies inversibility. Therefore we can follow that $\pi$ is
also inversible, as desired. (I just wanted to go this road to get surjectivity, but got
the whole inversibility from it.)

\subsection{}

\textit{Suppose $U \subseteq V$. Explain why $U^0 = \{\phi \in V': U \subseteq \ns \phi\}$}

Suppose that $\phi \in U^0$. Then we can follow that $\phi(u) = 0$, therefore $u \in \ns \phi$
for every $u \in U$. Thus $U \subseteq \ns \phi$. Thus
$U^0 \subseteq \{\phi \in V': U \subseteq \ns \phi\}$.

Now let $\phi$ be such that $U \subseteq \ns \phi$. Then we can follow that for all
$u \in U$
$$\phi(u) = 0$$
thus $\phi \in U^0$. Therefore $U^0 \supseteq \{\phi \in V': U \subseteq \ns \phi\}$
Therefore by double inclusion we've got the desired equality.

\subsection{}

\textit{Suppose $V$ is finite-dimentional and $U \subseteq V$.
  Show that $U = \{0\}$ is and only if $U^0  =V'$.} 

\textbf{In forward direction: }
Suppose that $U = \{0\}$. Then we can follow that
$$U^0 = \{\phi \in V': \phi(u) = 0 \text{ for all } u \in U\}$$
$U^0$ is a subspace of $V'$, therefore we've got that $U^0 \subseteq V'$.
Suppose that $\phi \in V'$. Then it follows that
$$\phi(0) = 0$$
because it's a linear function. Thus we follow that $V \subseteq U^0$. Thus
by double inclusion we've got that $V' = U^0$.

\textbf{In reverse direction: }
Suppose that $U^0 = V'$. Now suppose that $u \in U$.
Then we can follow that there exist $a_1, ..., a_m$ such that
$$u = \sum a_j v_j$$
for some basis $v_1, ..., v_m$ of $V$. For this basis there exists a dual basis
$\phi_1, ..., \phi_m$. Thus we can follow that
$$0 = \phi_j(u) = \phi(\sum a_k v_k) = \phi_j(\sum_{k \neq j}a_k v_k) + \phi(a_j v_j)=
a_j$$
Thus $a_1 = ... = a_m = 0$. Therefore $u = 0$. Thus $U = \{0\}$, as desired.

\subsection{}

\textit{Suppose $V$ is finite-dimentional and $U$ is a subpace of $V$. Show that
  $U = V$ if and only if $U^0 = \{0\}$.}

\textbf{In forward direction: }
Suppose that $U = V$. Then it follows that $\dim U = \dim V$. Thus
$$\dim V = \dim U + \dim U^0$$
$$\dim V = \dim V + \dim U^0$$
$$\dim U^0 = 0$$
thus $U^0 = \{0\}$.

\textbf{In reverse direction: }

Suppose that $U^0 = 0$. Then
$$\dim V = \dim U + \dim U^0$$
$$\dim V = \dim U$$
Thus $U = V$. 

\subsection{}

\textit{Suppose $U$ and $W$ are subsets of $V$ with $U \subseteq W$. Prove that
  $W^0 \subseteq U^0$.}

Suppose that $\phi \in W^0$. Then we follow that
if $u \in U$ then $u \in W$, and therefore $\phi(u) = 0$ for any $u \in U$. Thus $\phi \in U^0$.
Therefore $\phi \in W^0 \to \phi \in U^0$. Thus $W^0 \subseteq U^0$, as desired.

\subsection{}

\textit{Suppose $V$ is finite-dimetionmal and $U$ and $W$ are subspaces of $V$ with
  $W^0 \subseteq U^0$. Prove that $U \subseteq W$.}

Let $w_1, ..., w_m$ be a basis of $W$ and let us extend this basis to basis
$w_1, ..., w_m, ..., w_n$ of $V$. Then we can follow that there exists a dual
basis $\phi_1, ..., \phi_m, ..., \phi_n$ of $V$. Therefore we can follow that
if $\psi \in \Span(\phi_{m + 1}, ..., \phi_n)$, then
$$\psi(w) = 0$$
therefore $\Span(\phi_{m + 1}, ..., \phi_n) \subseteq W^0$. Conversely if $\psi \in W^0$, then
we can follow that
$$\psi = 0 \phi_1 + ... + 0 \phi_m + a_{m + 1} \phi_{m + 1} + ...$$
(otherwise $\psi(w_j) \neq 0$.)
Therefore we can follow that $W^0 \subseteq \Span(\phi_{m + 1}, ..., \phi_n)$. Thus
we've got that $W^0 = \Span(\phi_{m + 1}, ..., \phi_n)$. Therefore
$\phi_{m + 1}, ..., \phi_n$ is a basis for $W^0$.

Suppose now that there exists $u \in U$ such that $u \notin W$. Then we can follow that
$$u = \sum{a_j w_j}$$
Because $u \notin W$ we can follow that there exists $k > m$ such that  $a_k \neq 0$.
Thus $\phi_k(u) = a_k \neq 0$. Given that $k > m$, we follow that $\phi_k \in W^0$. Thus we
follow that $\phi_k \in U^0$. Therefore $\phi_k(u) = 0$, which is a contradiction.

Therefore we follow that there does not exist $u \in U$ such that $u \notin W$. Thus
we can follow that $u \in U \to u \in W$. Therefore $U \subseteq W$, as desired.


\subsection{}

\textit{Suppose $U, W$ are subspaces of $V$. Show that $(U + W)^0 = U^0 \cap W^0$.}

Suppose that $\phi \in (U + W)^0$. Then we can follow that for every $u \in U$, $\phi(u) = 0$.
therefore $\phi \in U^0$. By the same logic we have that $\phi \in W^0$. Thus we
can follow that
$$\phi \in (U + W)^0 \to \phi \in U^0 \land \phi \in W^0$$
$$\phi \in (U + W)^0 \to \phi \in U^0 \cap W^0$$
$$(U + W)^0 \subseteq U^0 \cap W^0$$

Conversely, suppose that $\phi \in U^0 \cap W^0$. Then we follow that
if $v = u + w \in U + W$, then
$$\phi(v) = \phi(u + w) = \phi(u) + \phi(w) = 0 + 0 = 0$$
Thus we follow that
$$\phi \in U^0 \cap W^0 \to \phi \in (U + W)^0$$
$$U^0 \cap W^0 \subseteq (U + W)^0$$
thus by double inclusion we've got the desired equality.

\subsection{}

\textit{Suppose $V$ is finite-dimentional and $U$ and $W$ are subsets of $V$. Prove that
  $(U \cap W)^0 = U^0 + W^0$}

Because $V$ is finite-dimentional we can follow that $U$ and $W$ are both finite-dimentional,
and therefore $U \cap W$ is finite dimentional (the fact that it is a subspace was proven in the
exercises before). We can therefore make a basis $u_1, ..., u_n, ..., u_m, ..., u_l, ..., u_k$
where $u_n, ..., u_m$ is a basis of $U \cap W$ (empty in case if the dimention of intersection
is zero), $u_1, ..., u_{n - 1}$ is the basis of $U$, $u_{m + 1}, ..., u_l$ is the rest of the
basis of $W$ and the rest is the basis of $V$. Then we can create a dual basis on this basis and
get that
$$U^0 + W^0 = \Span(\phi_1, ..., \phi_{n - 1}, ..., \phi_{m + 1}, ..., \phi_k) = (U \cap W)^0$$
as desired.

\subsection{}

\textit{Prove 3.106 using the ideas scketched in the discussion before the statement of 3.106}

We've outlined this proof in 3.6.21.

\subsection{}

\textit{Suppose $V$ is finite-dimentional and $U$ is a subspace of $V$. Show that }
$$U = \{v \in V: \phi(v) = 0 \text{ for every } \phi \in U^0 \}$$

Let $u_1, ..., u_n$ be a basis of $U$, and extend this basis to $u_1, ..., u_n, ..., u_m$ - a
basis of $V$. Then let us define dual basis on this basis $\phi_1, ..., \phi_m$, and it'll
follow that if $v \notin U$, then there will exist $\phi_k$ for some $k > n$ such that
$$\phi_k(v) \neq 0$$
Thus we can follow that
$$v \notin U \to v \notin  \{v \in V: \phi(v) = 0 \text{ for every } \phi \in U^0 \}$$
and therefore 
$$ v \in  \{v \in V: \phi(v) = 0 \text{ for every } \phi \in U^0 \} \to v \in U$$
thus
$$\{v \in V: \phi(v) = 0 \text{ for every } \phi \in U^0 \} \subseteq U$$

Conversely, suppose
that $u \in U$. Then it follows that $\phi \in U^0 \to \phi(u) = 0$. Thus we've got that
$U \subseteq  \{v \in V: \phi(v) = 0 \text{ for every } \phi \in U^0 \}$.

Thus by double inclusion we've got our desired equality.

\subsection{}

\textit{Suppose $V$ is finite-dimentional and $\Gamma$ is a subspace of $V'$. Show that }
$$\Gamma = \{v \in V: \phi(v) = 0 \text{ for every } \phi \in \Gamma \}^0$$

Suppose that $\Gamma$ is a subspace of $V'$. Then we can follow that there exists a
basis of $\Gamma$ -  $\phi_1, ..., \phi_n$, and that we can extend this basis to a basis of
$V'$ - $\phi_1, ..., \phi_m$.

For every $\phi_j$ we've got that $\phi_j \neq 0$ and therefore by FTLM we've got that
$$\dim V' = \dim \ns \phi_j + \dim \range \phi_j$$
$$\dim V' = \dim \ns \phi_j + 1$$
$$\dim V' - 1 = \dim \ns \phi_j$$
thus we can follow that if we take a basis of $\ns \phi_j$, extend it to a basis of $V$ by adding
one vector $v_j'$, then
$$\phi_j v_j' \neq 0$$
then we can define
$$v_j = v_j' * \frac{1}{\phi_j v_j}$$
so that
$$\phi_j(v_j) = 1$$
By making $v_j$ for each corresponding $\phi_j$ we can get the list $v_1, ..., v_m$.
Because every $\phi_m$ has a diffenent nullspace (otherwise they are scalar multiple
of each other) we can follow that they have different preimage, and therefore
$v_1, ..., v_m$ is linearly independent. By length of this linearly independent list we
can follow that it is a basis of $V$.

Then we can follow that there exist vectors
$v_1, ..., v_m$ such that
$$\phi_j(v_j) = 1$$
and for $k \neq j$
$$\phi_k(v_j) = 0$$
Therefore we'll have a subspace $U = \Span(v_{n + 1}, ..., v_m)$. Then it follows that
$$\Gamma = U^0$$
and by results of our previous exercise we'll have that
$$U =  \{v \in V: \phi(v) = 0 \text{ for every } \phi \in U^0 \}$$
$$U^0 =  \{v \in V: \phi(v) = 0 \text{ for every } \phi \in U^0 \}^0$$
$$\Gamma =  \{v \in V: \phi(v) = 0 \text{ for every } \phi \in \Gamma \}^0$$
as desired.

\subsection{}

\textit{Suppose $T \in \map(P_5(R), P_5(R))$ and $\ns T' = \Span(\phi)$, where $\phi$ is
  the linear functional on $P_5(R)$ defined by $\phi(p) = p(8)$. Prove that
  $\range T = \{p \in P_5(R): p(8) = 0\}$.}

By a theorem in the chapter we've got that
$$\ns T' = (\range T)^0$$
and by results of the previous exercise we've got that
$$\Span(\phi) = \{p \in P_5(R): \phi(p) = 0\}^0$$
(we've shortened the right-hand side, since
$\psi \in \Span(\phi) \land \psi(v) = 0 \to \phi(v) = 0$).
thus we can follow taht 
$$\ns T' =  \Span(\phi)$$
is equivalent to stating that 
$$(\range T)^0 = (\{p \in P_5(R): \phi(p) = 0\})^0$$

For the next implication we'll probably need a little lemma

\textbf{Lemma: Equivalent annihilators implies equivalent subspaces}

\textit{
  Suppose that $V$ is finite-dimentional, 
  $U$ and $W$ are subspaces of $V$. Then $U^0 = W^0$ implies $U = W$.
}

By exercise 21 in this chapter we've got that
$$W^0 \subseteq U^0 \to U \subseteq W$$
thus we follow that
$$W^0 = U^0 \to W^0 \subseteq U^0 \land U^0 \subseteq W^0 \to
U \subseteq W \land W \subseteq U \to W = U$$
as desired. (If we think about it, exercise 20 gives us that this statement is actually an
equivalence and not implicaton. For infinite-dimentional vector spaces we probably got
just the implication)

Thus we can indeed follow that
$$(\range T)^0 = (\{p \in P_5(R): \phi(p) = 0\})^0$$
implies that
$$\range T = \{p \in P_5(R): \phi(p)$$
as desired

\subsection{}

\textit{Suppose $V$ and $W$ are finite-dimentional, $T \in \map (V, W)$, and there exists
  $\phi \in W'$ such that $\ns T' = \Span \phi$. Prove that $\range T = \ns \phi$}


This is a generalization of the previous exercise

$$\{v \in V: \psi(v) = 0 \text{ for every } \psi \in \Span(\phi)\} =
\{v \in V: \lambda \phi(v) = 0 \} = \{v \in V: \phi(v) = 0 \} = \ns \phi$$
Thus
$$\ns T = (\range T)^0 = (\ns \phi)^0$$
thus
$$\range T = \ns \phi$$
as desired.

\subsection{}

\textit{Suppose $V$ and $W$ are finite-dimentional, $T \in \map (V, W)$, and there exists
  $\phi \in V'$ such that $\range T' = \Span \phi$. Prove that $\ns T = \ns \phi$}

$$\range T' = (\ns T)^0 = \Span \phi = (\ns \phi)^0$$
thus
$$(\ns T)^0 = (\ns \phi)^0$$
$$\ns T = \ns \phi$$
as desired.

\subsection{}

\textit{Suppose $V$ is finite-dimentional and $\phi_1, ..., \phi_m$ is a linearly independent
  list in $V'$. Prove that }
$$\dim ((\ns \phi_1) \cap ... \cap (\ns \phi_m)) = (\dim V) - m$$


$$\dim ((\ns \phi_1) \cap ... \cap (\ns \phi_m)) +
\dim ((\ns \phi_1) \cap ... \cap (\ns \phi_m))^0= (\dim V) $$
Thus what we probably are intended to prove is that
$$((\ns \phi_1) \cap ... \cap (\ns \phi_m))^0 = \Span(\phi_1, ..., \phi_m)$$

By exercise 26 we can get that
$$\Span(\phi_1, ..., \phi_m) = \{v \in V: \phi(v) = 0 \text{ for every } \phi \in \Gamma\}^0$$
Since $\phi \in \Span(\phi_1, ..., \phi_m) \to \phi = \sum a_j \phi_j$, we follow that
$$\Span(\phi_1, ..., \phi_m) = \{v \in V: \phi_j(v) = 0 \text{ for every } \phi_j\}^0$$
If $\phi_j(v) = 0$ for every $\phi_j$, then $v \in \ns \phi_1 \cap ... \cap \ns \phi_m$
du definition of nullspace. If $v \in \ns \phi_1 \cap ... \cap \ns \phi_m$, then it is
obliously true that $\phi_j(v)  = 0$ by the same definition. Thus by double inclusion we get
that
$$\{v \in V: \phi(v) = 0 \text{ for every } \phi \in \Gamma\} =
\ns \phi_1 \cap ... \cap \ns \phi_m$$
thus 
$$\Span(\phi_1, ..., \phi_m) = (\ns \phi_1 \cap ... \cap \ns \phi_m)^0$$
and therefore
$$\dim ((\ns \phi_1) \cap ... \cap (\ns \phi_m)) +
\dim ((\ns \phi_1) \cap ... \cap (\ns \phi_m))^0= (\dim V) $$
$$\dim ((\ns \phi_1) \cap ... \cap (\ns \phi_m)) +
\dim \Span(\phi_1, ..., \phi_m) = (\dim V) $$
since $\phi_1, ..., \phi_m$ are linearly independent, we can follow that
$$\dim ((\ns \phi_1) \cap ... \cap (\ns \phi_m)) +
m = (\dim V) $$
$$\dim ((\ns \phi_1) \cap ... \cap (\ns \phi_m)) +
= (\dim V) - m$$
as desired.

\subsection{}

\textit{Suppose $V$ is finite-dimentional and $\phi_1, ..., \phi_n$ is a basis of $V'$.
  Show that there exists a basis of $V$, whose dual basis is $\phi_1, ..., \phi_n$.}

From previous exercice we can follow that there for given basis there exists a space
$$U_k = \cap_{i \neq k} {\ns \phi_i}$$
such that
$$\dim U_k = \dim V - (n - 1) = n - (n - 1) = 1$$
From the definition we can follow that
$$u \in U_k \to u \in \ns \phi_{i \neq k} \to  \phi_{i \neq k}(u) = 0$$

Suppose that $u \neq 0$. Then we can follow that we can extend this vector to the basis of $V$.
Thus we can follow that we can create a map $\psi \in \map(V, F) \iff \psi \in V'$ such that
$$\psi(u) = 1$$
Given that $\phi_1, ..., \phi_n$ spans $V'$, we can follow that there exist $a_1, ..., a_n$
such that
$$\psi = \sum {a_j \phi_j}$$
from this we can follow that
$$\psi(u) = \sum {a_j \phi_j}(u) = \sum_{j \neq k} {a_j \phi_j}(u) + a_k \phi_k(u) = 0 +
a_k \phi(u) = a_k \phi_k(u)$$
Thus we can follow that
$$a_k \phi(u) = \psi(u) = 1$$
$$a_k \phi(u) = 1$$
thus we follow that $a_k \neq 0$ and $\phi(u) \neq 0$. Thus we follow that $\phi(a_k u) = 1$.
Set $v_k = a_k u$.

In this fashion we can create list $v_1, ..., v_n$ with the property that
$$\phi_k (v_k) = 1$$
$$\phi_{i \neq k}(v_k) = 0$$

The only thing that is left is to show that this list is linearly independent.
Suppose that it isn't. Then it follows that
$$v_k = \sum_{j \neq k} a_j v_j$$
thus
$$\phi_k(v_k) = \phi_k(\sum_{j \neq k} a_j v_j)$$
$$1 = 0$$
which is false. Thus we follow that $v_1, ..., v_n$ is linearly independent list with desired
properties in $V$. Given that its length is the dimention of the space, we follow that
it is a basis, for which $\phi_1, ..., \phi_n$ is dual basis, as desired.

\subsection{}

\textit{Suppose $T \in \map(V)$, and $u_1, ..., u_n$ and $v_1, ..., v_n$ are bases of $V$. Prove
  that the following are equivalent: }

\textit{(a) $T$ is invertible}

\textit{(b) The columnns of $M(T)$ are linearly independent in $F^{n, 1}$.}

\textit{(c) The columns of $M(T)$ span $F^{n, 1}$}

Because $T$ is invertible, we can follow that the dimention of its range is equal to $n$.
Thus we can follow that rank of its matrix is $n$. Therefore its row rank and column rank is $n$.
Thus (a) implies (b). By the size of the mateix we get that (b) implies (c). And (c) implies that
with given bases the function is invertible, therefore (c) implies (a).

Because $T$ is invertible if and only if $T'$ is invertible,  we get the (d) and
(e) for free.

\subsection{}

\textit{Suppose $m$ and $n$ are positive integers. Prove that teh function that takes
  $A$ to $A^t$ is a linear map from $F^{m, n}$ to $F^{n, m}$. Furtermore, prove that this linear
  maps is invertible.}


Linearity follows directly from definitions of the transpose, matrix addition and scalar
multiplication. Invertability follows directily from injectifity and surjectivity of this
transformation.

\subsection{}

\textit{The double dual space of $V$, denoted $V''$, is defined to be the dual space of $V'$. In
  other words, $V'' = (V')'$. Define $\Lambda: V \to V''$ by}
$$(\Lambda(v))(\phi) = \phi(v)$$
\textit{for $v \in V$ and $\phi \in V'$}

Since I haven't fully understood the definition, I'll try to dumb it down a notch. Suppose that
$v \in V$. Then we follow that there exists $\kappa \in V''$ such that
$$\Lambda(v) = \kappa$$

Thus
$$\kappa \in V'' \to \kappa \in (V')' \to \kappa \in \map(V', F)
\to \kappa \in \map(\map(V, F), F)$$
Thus we can plug in some $\phi \in V'$ into $\kappa$, and get some number from it.

What could be an example of such a map? Suppose that
$p \in P(R)$. Then we can define $\phi(p) = \int_0^1{p}$. And thus we can define
$\Lambda$ to be a function, that inputs a polynomial, and returns a function, that
inputs a linear functional on a polynomial, and returns the result of applying inputed
polynomial into the linear functional.

That kind of made a dent in the understanding of what the hell is going on, but nothing major
had happened.

\textit{(a) Show that $\Lambda$ is a linear map from $V$ to $V''$.}

$$(\Lambda (\lambda v))(\phi) = \phi(\lambda v) = \lambda (\phi (v)) =
\lambda ((\Lambda(v))(\phi))$$
$$(\Lambda (v + w))(\phi) = \phi(v + w) = \phi(v) + \phi(w) = (\Lambda(w))(\phi) +
(\Lambda(w))(\phi)$$

Thus we follow that $\Lambda$ is linear, as desired.

\textit{(b) Show that if $T \in \map(V)$, then $T'' \circ \Lambda = \Lambda \circ T$, where
  $T'' = (T')'$.}

Let $v \in V$ and $\phi \in V'$. Then we can follow that
$$(T '' \circ (\Lambda v)) (\phi) =  (\Lambda v) (T'(f)) = (T'f)(v) = f(T(v)) = \Lambda (Tv) (f)$$
as desired.

\textit{(c) Show that if $V$ is finite-dimentional, then $\Lambda$ is an isomorphism from
  $V$ onto $V''$}

If $V$ is finite-dimentional, then $V''$ is finite-dimentional, and their dimentions are the same.
Thus injectivity of $\Lambda$ implies the invertibility. Suppose that $\Lambda v = 0$. Then we
follow that $\phi(v) = 0$ for any $\phi \in V'$. By exercise 19 we've got that it happens if
and only if $v = 0$. Thus $\Lambda$ is injective, and therefore invertible, as desired.


\subsection{}

\textit{Show that $(P(R))'$ and $R^\infty$ are isomorphic.}

$P(R)$ and $(P(R))'$ are isomorphic, $P(R)$ and $R^\infty$ are also isomorphic, therefore
$(P(R))'$ and $R^\infty$ are isomorphic, as desired.

\subsection{}

\textit{Suppose $U$ is a subspace of $V$. Let $i: U \to V$ be the inclusion map defined
  by $i(u) = u$. Thus $i' \in \map(V', U')$.}

\textit{(a) Show that $\ns i' = U^0$}

$$\ns i' = (\range i)^0 = U^0$$

\textit{(b) Prove that if $V$ is finite-dimentional, then  $\range i' = U'$.}

Let $v_1, ..., v_n, ..., v_m$ be a basis of $V$, where $v_1, ..., v_n$ is a basis of $U$. Define
dual basis of it. Then we follow that $v_{n + 1}, ..., v_n$ is a basis of $\ns i$, and therefore
$\psi_1, ... \psi_n$ is a basis of $(\ns i)^0$, which is equal to basis of $U'$. THus
$$(\ns i)^0 = U'$$
and since
$$\range i' = (\ns i)^0$$
we follow that
$$\range i' = U'$$
as desired.

\textit{(c) Prove that if $V$ is finite-dimentional, then $\overline{i'}$ is an isophormism from
  $V'/U^0$ onto $U'$.}

$$\dim V'/U^0 = \dim V' - \dim U^0 = \dim V - (\dim V - \dim U) = \dim U$$
and
$$\dim U' = \dim U$$
thus we follow that the dimentions of those vector spaces are the same.

Since $\range i' = U'$, we can follow that $\range \overline{i'} = \range i' = U'$. Thus we
follow that this map is sujective, and therefore by their identical dimention we follow that
$\overline{i'}$ is an isomorphism, as desired.

\subsection{}

\textit{Suppose $U$ is a subspace of $V$. Let $\pi: V \to V/U$ be the usual quotent map.
  Thus $\pi' \in \map((V/U)', V')$.}

\textit{(a) Show that $\pi'$ is injective.}

Sinse $\pi$ is surjective (suppose that $a \in (V/U)$, then $a = v + U$, then there exists
$v \in V$, then $\pi(v) = v + U$, therefore it is surjective.)
we can follow that $\pi'$ is injective, as desired.

\textit{(b) Show that $\range \pi' = U^0$.}

$$\range \pi' = (\ns \pi)^0 = U^0$$
(the fact that $\ns \pi = U$ is somewhat followed in the proof of 3.89, where it is derived from
3.85)

\textit{(c) Conclude that $\pi'$ is an isomorphism from $(V/U)'$ onto $U^0$.}

$\pi'$ is inhective and is surjective, if we restrict its codomain to $U^0$. Thus it is invertible,
therefore it is an isomorphism.

\chapter{Polynomials}

\section{Polynomials}


\subsection{}

\textit{Verify all assertions in 4.5 except the last one.}

Firstly, let us state that $z, w \in C$ and
$$z = a + bi$$
$$w = c + di$$

Then we follow that 
$$z + \overline{z} = a + bi + a - bi = 2a = 2 Re(z)$$
$$z - \overline{z} = a + bi - a + bi = 2bi = 2 Im(z) i$$
$$z \overline{z} = (a + bi) (a - bi) = a^2 + abi - abi - b^2 = a^2 + b^2 =
(\sqrt{a^2 + b^2})^2 = (\sqrt{Re(z)^2 + Im(z)^2})^2 = |z|^2$$
$$\overline{w + z} = \overline(a + bi + c + di) =
\overline(a + c + (b + d)i) = a + c - (b + d)i = a - bi + c - di = \overline{z} + \overline{w}$$
$$\overline{\overline{z}} = \overline{\overline{a + bi}}  =
\overline{a - bi} = a + bi = z$$
$$|a^2|  = a^2 \leq a^2 + b^2 = |a^2 + b^2| \to |a|^2 \leq a^2 + b^2 \to |a| \leq \sqrt{a^2 + b^2}
\to  |Re(z)| \leq |z| $$
$$|\overline{z}| = \sqrt{a^2 + (-b)^2} = \sqrt{a^2 + b^2} = |z|$$
$$|wz| = |(a + bi) (c + di)| = |ac + adi + cbi - bd| = |ac - bd +  (ad + cb)i| =
\sqrt{(ac - bd)^2 + (ad + cb)^2} = $$
$$ = \sqrt{(ac)^2 - 2abcd + (bd)^2 + (ad)^2 + 2abcd  + (cb)^2} =
\sqrt{(ac)^2  + (bd)^2 + (ad)^2  + (cb)^2} = $$
$$ = \sqrt{a^2(c^2 + d^2)  + b^2(d^2 + c^2)}  = \sqrt{(a^2 + b^2)(c^2 + d^2))}  = |z||w| = |w||z|$$

\subsection{}

\textit{Suppose $m$ is a positive integer. Is the set}
$$ \{0\} \cup \{p \in \mathcal{P}(F): \deg p = m\}$$
\textit{a subspace of $\mathcal{P}(F)$?}

Suppose $m \neq 1$
Let $p_1 = x^m + 1$ and $p_2 = -x^m + 1$. Then we follow that
$$\deg(p_1 + p_2) = \deg(2) = 0 \neq m $$
(we use the fact that $m$ must be positive). Thus given set is not closed under addition,
therefore it is not a subspace.

\subsection{}

\textit{Is the set}
$$\{0\} \cup \{p \in \mathcal{P}(F): \deg p \text{ is even}\}$$
\textit{a subspace of $\mathcal P (F)$?}

Let $p_1 = x^2 + x$ and $p_2 = -x^2 + x$. Then we follow that
$$\deg(p_1 + p_2) = \deg(2x) = 1$$
thus the space is not closed under addition, therefore it is not a subspace.

\subsection{}

\textit{Suppose $m$ and $n$ are positive integers with $m \leq n$, and suppose
  $\lambda_1, ..., \lambda_m \in F$. Prove that there exists a polynomial $p \in P(F)$
  with $\deg(p) = n$ such that $0 = p(\lambda_1) = ... = p(\lambda_n)$ and such that
  $p$ has no other zeroes.}

Let
$$p = (z - \lambda_1) ... (z - \lambda_m)^{m - n + 1}$$
Then we can follow that the only zeroes of $p$ are precisely zeroes of
$$p_1 = (z - \lambda_1) ... (z - \lambda_{m})$$
which are $\lambda_1, ..., \lambda_{m}$
Then  we follow that zeroes of
$$p_2 = (z - \lambda_1) ... (z - \lambda_{m})^2 = p_1(z - \lambda_m)$$
are zeroes of $p_1$ and $\lambda_m$. Given that zeroes of $p_1$ already have $\lambda_m$, we
follow that $p_1$ and $p_2$ have the same zeroes. Then by induction we follow that
zeroes of $p$ are zeroes of $p_1$, which are $\lambda_1, ..., \lambda_m$, as desired.

\subsection{}

\textit{Suppose $m$ is nonnegative integer, $z_1, ..., z_{m + 1}$ are distinct elements of $F$,
  and $w_1, ..., w_{m + 1} \in F$. Prove that there exists a unique polynomial $p \in P_m(F)$
  such that }
$$p(z_j) = w_j$$
\textit{for $j = 1, ... m + 1$.}

Let $T: P_m(F) \to F^{m + 1}$ be defined as
$$T(p) = (p(z_1), p(z_2), ..., p(z_{m + 1}))$$
We can follow that if
$$T(p) = 0$$
then
$$p(z_1) = p(z_2) = ... = p(z_{m + 1}) = 0$$
But if $p \neq 0$, then it has at most $m$ roots. Thus we follow that $p = 0$. Therefore $T$ is
injective. By FTLM we have that
$$\dim V = \dim \ns T + \dim \range T$$
Therefore for this case we have
$$\dim P_m(F) = \dim \ns T + \dim \range T$$
$$m + 1 =  \dim \range T$$
given that $\dim F^{m + 1} = m + 1$ we follow that $\range T = F^{m + 1}$. Therefore we
follow that it is surjective. Thus for any vector $(w_1, ..., w_{m + 1}) \in F^{m + 1}$
there exists $p \in P_m(F)$ such that
$$p(z_j) = w_j$$
as desired.

\subsection{}

\textit{Suppose $p \in \mathcal P(C)$ has degree $m$. Prove that $p$ has $m$ distinct zeroes if
  and only if $p$ and its derivative $p'$ have no zeroes in common}

Suppose that $p$ has $m$ distinct zeroes and suppose that $p(\lambda_j) = 0$ for
some $\lambda \in F$. Then we follow that

$$p = a(x - \lambda_1) (x - \lambda_2) ... (x - \lambda_n)$$
By the product rule of differentiation we've got that 
$$p' = a (\prod_{i \neq j}(x - \lambda_i) + ...\text{ some terms that have }(x - \lambda_j)...)$$
and therefore
$$p'(\lambda_j) = a (\prod_{i \neq j}{\lambda_j - \lambda_i)}$$
Given that every $\lambda$ is distinct, we follow that $(\lambda_j - \lambda_i) \neq 0$, and
therefore $p'(\lambda_j) \neq 0$. Thus $\lambda_j$ is not a zero for $p'$. for any
$1 \leq  j \leq n$.

Now supppose that $p$ and $p'$ don't have any zeroes in common. Thus we can follow that 
$$p' = a (\sum_{i = 1}^{n}{\prod_{i \neq j}{(x - \lambda_i)}})$$
thus we follow that if $p$ has less then $m$ distinct values, then $p'(\lambda) = 0$, therefore
we've got that the polinomial and its derivative does not have any common zeroes.

\subsection{}

\textit{Prove that every polynomial of odd degree with real coefficient has a real zero.}

Suppose that $p$ is a polynomial of an odd degree $m$. Then it follows that
it has can be factorized as
$$p(z) = c (z - \lambda_1) ... (z - \lambda_m)$$
By the fact that $p(\lambda ) = 0 \to p(\overline{\lambda} ) = 0$ we can follow that there must
exsist $\lambda_j$ such that $\lambda_j = \overline{\lambda_j}$, therefore $\lambda_j \in R$, as
desired.


\subsection{}

\textit{Define $T: P(R) \to R^R$ by}
$$Tp =
\begin{cases}
  \frac{p - p(3)}{x - 3} \text{ if } x \neq 3 \\
  p'(3) \text{ if } x = 3
\end{cases}
$$
\textit{Show that $Tp \subseteq P(R)$ for every polynomial $p \in P(R)$ and that $T$ is a
  linear map.}

We know that $p$ is a polynomial, therefore $p(3)$ is a constant, which is also a polynomial.
Thus $p(x) - p(3)$ is a polynomial. Thus if $x = 3$, then $p(x) - p(3) = 0$. Thus we can follow
that $p(x) - p(3)$ has a factorization
$$p(x) - p(3) = (x - 3)q(x)$$
thus
$$  \frac{p - p(3)}{x - 3} = \frac{(x - 3)q(x)}{x - 3}$$
given that $x \neq 3$ we follow that
$$\frac{(x - 3)q(x)}{x - 3} = q(x)$$

When I looked at this thing one more time, I've noticed that it looks suspiciously
simular to a definition of the derivative of the polynomial.

Thus we can conclude that by definition of derivative 
$$p'(3) = \lim_{x \to 3}{\frac{p(x) - p(3)}{x - 3}} = \lim_{x \to 3}{\frac{(x - 3)q(x)}{x - 3}} =
\lim_{x \to 3}{q(x)} = q(3)$$

Thus we can follow that $Tp = q$, where $q$ is a polynomial.

Proof of linearity is trivial, therefore I'll skip it.

\subsection{}

\textit{Suppose $p \in P(C)$. Define $q: C \to C$ by}
$$q(z) = p(z) \overline{p(\overline z)}$$
\textit{Prove that $q$ is a polynomial with real coefficients.}

Given that $p(z)$ is a polynomial, we can state that we can factor it as 
$$p(z) = c \prod{(z - \lambda_j)}$$
thus
$$\overline{p(\overline{z})} = \overline{c \prod{(\overline z - \lambda_j)}} =
\overline{c} \prod{\overline {(\overline z - \lambda_j)}} =
\overline{c} \prod{(z - \overline  \lambda_j)}$$
Thus we follow that
$$q(z) = p(z) \overline{p(\overline z)} =
c \prod{(z - \lambda_j)} \overline{c} \prod{(z - \overline  \lambda_j)} =
|c|^2 \prod{(z - \overline  \lambda_j)(z - \lambda_j)} =$$
$$ = 
|c|^2 \prod{(z^2 - z(\overline  \lambda_j + \lambda_j) + |\lambda_j|^2)} =
|c|^2 \prod{(z^2 - 2\Re(\lambda_j)z  + |\lambda_j|^2)} $$
Since every expression in the parenthesis is a polynomial in real coefficients, we follow that
their product is a polynomial in real coefficients. And given that $|c|^2$ is a polynomial
in real coefficients as well, we conclude that $q(z)$ is a polynomial in real coefficients,
as desired.

\subsection{}

\textit{Suppose $m$ is a nonnegative integer and $p \in P_m(C)$ is such that there exist
  distinct real numbers $x_0, ..., x_m$ such that $p(x_j) \in R$ for
  $j = 0, 1, ..., m$. Prove that all the coefficients of $p$ are real.}

We can follow that from the exercise 5, where we add a  restriction that  $F = R$.

From the uniqueness clause in the exercise, we can follow that for any
set $x_0, ..., x_m \in R$ we'll have unique collection of real coefficients.

\subsection{}

\textit{Suppose $p \in P(F)$ with $p \neq 0$. Let $U = \{pq: q \in P(F)\}$.}

\textit{(a) Show that $\dim P(F)/U = \deg p$.}

We know that
$$p = sq + r$$
thus we can follow that
$$P(F) = U \oplus P_{(\deg p - 1)}(F)$$
thus we conclude that
$$\dim (P(F)/U) = \deg p$$

\textit{(b) Find a basis of $\dim P(F)/U$.}

$$1 + U, x + U, ... x^m + U$$

\chapter{Eigenvalues, Eigenvectors, and Invariant Subspaces}

\section{Invariant Subspaces}

\subsection{}

\textit{Suppose $T \in \map(V)$ and $U$ is a subspace of $V$.}

\textit{(a) Prove that if $U \subseteq \ns t$, then $U$ is invariant under $T$}

Suppose that $u \in U$. Then $u \in \ns T$ and therefore $Tu = 0 \in U$, because $U$ is
a subspace. Thus $u \in U \to Tu \in U$, therefore $U$ is invariant under $T$.

\textit{(b) Prove that if $\range T \subseteq U$, then $U$ is invariant under $T$. }

Suppose that $u \in U$. Then $Tu \in \range T \to Tu \in U$.
Thus $u \in U \to Tu \in U$, therefore $U$ is invariant under $T$.

\subsection{}

\textit{Suppose $S, T \in \map(V)$ are such that $ST = TS$. Prove that $\ns S$ is invariant under
  $T$.}

Suppose that $v \in \ns S$. Then we follow that $Sv = 0$ and therefore $TSv = 0$. Thus
by our equality we've got that $STv = 0$. Therefore we follow that $Tv \in \ns S$. Thus
$v \in \ns S \to Tv \in \ns S$. Thus we follow that $\ns S$ is invariant under $T$.

\subsection{}

\textit{Suppose $S, T \in \map(V)$ are such that $ST = TS$. Prove that $\range S$ is invariant under
  $T$.}

Suppose that $w \in \range S$. Then we follow that there exists $v \in V$ such taht
$Sv = w$. Thus $TSv = Tw = STv$. Because $Tw = S(Tv)$, we follow that $Tw \in \range S$. Thus
we conclude that $w \in \range S \to Tw \in \range S$. Thus $\range S$ is invariant under $T$.

\subsection{}

\textit{Suppose $T \in \map(V)$ and $U_1, ..., U_m$ are subspaces of $V$ invariant under $T$.
  Prove that $U_1 + ... + U_m$ is invariant under $T$.}

Suppose that $u \in U_1 + ... + U_m$. Then we follow that
$$u = \sum{u_j}$$
for $u_j \in U_j$ for $1 \leq j \leq m$. Thus we follow that 
$$T u = T \sum{u_j} = \sum{T u_j}$$
Given that $U_j$ is invariant under $T$, we follow that $T u_j \in U_j$, and therefore
$Tu \in U_1 + ... + U_m$. Thus we follow that $U_1 + ... + U_m$ is invariant under $T$, as desired.


\subsection{}

\textit{Suppose $T \in \map(V)$. Prove that the intersection of every collection of subspaces
  of $V$ invariant under $T$ is invariant under $T$.}

Suppose that $U_1, ..., U_m$ are invariant subspaces. Then we follow that $U_1 \cap ... \cap U_m$
is also a subspace. Suppose that $u \in U_1 \cap ... \cap U_m$. Then it follows by
invariance of respective subspace that $Tu \in U_1 \land ... Tu \in U_m$, thus we can
conclude that $Tu \in U_1 \cap ... U_m$. Therefore we conclude that $U_1 \cap ... \cap U_m$ is
an invariant subspace under $T$.


\subsection{}

\textit{Prove or give a counterexample: if $V$ is finite-dimentional and $U$ is a subspace of $V$
  that is invariant under every operator on $V$, then $U = \{0\}$ or $U = V$.}

Suppose that  $U$ is a subspace of $V$ such that $U \neq \{0\}$ and $U \neq V$. Then we
can follow that $0 \neq \dim U$ and $\dim U \neq \dim V$. Thus we can follow that
there exists a non-empty basis $u_1, ..., u_n$ of $U$, that we can extend with a non-empty
list of vectors to $u_1, ..., u_m$ - basis of $V$. Now define a linear map
$$Tu_j = u_{m - j + 1}$$
then we follow that $T u_1 = u_m \notin U$. Therefore we conclude that $U$ is not invariant
under $T$. Thus we can conclude that the statement is correct.

\subsection{}

\textit{Suppose $T \in \map(R^2)$ is defined by $T(x, y) = (-3y, x)$. Find the eigenvalues
  of $T$.}

$$\lambda (x, y) = (-3y, x)$$
$$ (\lambda x, \lambda y) = (-3y, x)$$
$$
\begin{cases}
  \lambda x = -3y \\
  \lambda y = x
\end{cases}
$$
$$\lambda^2 y = -3y $$
$$\lambda^2  = -3 $$
which cannot be the case in real numbers. In complex numbers though we've got that
$$\lambda = [\sqrt{3}i, -\sqrt{3}i]$$
Suppose that we've got another real eigenvalue of $T$. Then we can follow that this
eigenvalue is also an eigenvalue in complex numbers, therefore we've got more distinct eigenvalues,
then dimentions, which is a contradiction. Thus we can conclude that $T$ does not
have real eigenvalues.

\subsection{}

\textit{Define $T = \map(F^2)$ by}
$$T(w, z) = (z, w)$$.
\textit{Find all eivenvalues and eigenvectors of $T$}

$$\lambda (w, z) = (z, w)$$
$$ (\lambda w, \lambda z) = (z, w)$$
$$
\begin{cases}
  \lambda w = z \\
  \lambda z = w
\end{cases}
$$
$$\lambda^2 w = w$$
$$\lambda = [1, -1]$$

Thus we follow that for $w, z \in F \setminus \{0\}$ we've got
that eigenvectors $(w, w)$ have an eigenvalue of $1$, and eigenvectors $(z, -z)$ have eigenvalues
of $-1$.

\subsection{}

\textit{Define $T \in \map(F^3)$ by}
$$T(z_1, z_2, z_3) = (2z_2, 0, 5z_3)$$
\textit{Find all eigenvalues and eigenvectors of $T$.}
$$\lambda (z_1, z_2, z_3) = (2z_2, 0, 5z_3)$$
$$ (\lambda z_1, \lambda z_2, \lambda z_3) = (2z_2, 0, 5z_3)$$
$$
\begin{cases}
  \lambda z_1 = 2 z_2 \\
  \lambda z_2 = 0 \\
  \lambda z_3 = 5z_3
\end{cases}
$$
the only values that work here are 0 with vectors $(z, 0, 0)$ and 5 with $(0, 0, z)$ with
$z \in F \setminus \{0\}$.

\subsection{}

\textit{Define $T \in \map(F^n)$ by}
$$T(x_1, x_2,  ..., x_n) = (x_1, 2x_2, 3x_3, ..., nx_n)$$

\textit{(a) Find all eigenvalues and eigenvectors of $T$.}

We obliously have 1 for $(z, ..., 0)$, 2 for $(0, z, 0, ..., 0)$ and so on. Given that
the number of distinct eigenvectors is equal to the number of the dimentions, we follow that
those are the only eigenvalues and eigenvectors.

\textit{(b) Find all invariant subspaces of $T$.}

We can state that $\lambda (1, 0, .. 0)$ and such are the invariant subspaces. By the
things that we've concluded earlier we can follow that sums of those subspaces are
also invariant.

% Let $\lambda_1, ..., \lambda_n$ be distinct eigenvectors with $v_1, ..., v_n$ -- corresponding
% eigenvectors. If $U \subseteq \Span(v_1, ..., v_n)$ is invariate under $T$, then
% $U$ is a span of set of some eigenvectors.

% Suppose that $U$ is not a span of some eigenvectors. Let $u_1, ..., u_m \in U$ be a set of
% eigenvectors, that are contained in $U$, and expand this list to the basis of $U$ -
% $u_1, ..., u_m, w_1, ..., w_l$. Let $K = \Span(w_1, ..., w_l)$. Then we can follow that
% since $u_1, ..., u_m$ are eigenvectors, then $\Span(u_1, ..., u_m)$ is an invariate subspace.
% Thus we follow that $K$ is invariate subspace itself. Then we follow that
% $$w_i = \sum v_j a_j$$
% and therefore
% $$T(w_i) = T(\sum v_j a_j) = \sum v_j T a_j = \sum v_j \lambda_j a_j$$

\subsection{}

\textit{Define $T: P(R) \to P(R)$ by $Tp = p'$. Find all eigenvalues and eigenvectors of $T$.}

Let $Tp = p' = \lambda p$. Since differentiation always reduces a degree of a polynomial by 1,
we follow that the only eigenvalue that can exist is 0 with eigenvector $c \in F \setminus \{0\}$.

\subsection{}

\textit{Define $T \in \map(P_4(R))$ by}
$$(Tp)(x) = xp'(x)$$
\textit{for all $x \in R$. Find all eigenvalues and eigenvectorss of $T$.}

$$\lambda p = x p'(x)$$

$0$ is still a viable eigenvalue with eigenvector $c \in F \setminus \{0\}$.

$$ \sum \lambda a_j x^j = \sum j a_j x^j$$
since polynomials are uniquely determined by their coefficients, we follow that
$$ \lambda a_j = j a_j$$
thus we can follow that we've got eigenvalues $0, 1, 2, 3, 4, 5$ for eigenvectors
$$1, x, x^2, x^3, x^4$$ and their non-zero scalar multiples.

Since the number of distinct eigenvalues is equal to the dimention of the space, we are done.

\subsection{}

\textit{Suppose $V$ is finite-dimentional, $T \in \map(V)$, and $\lambda \in F$. Prove that there
  exists $\alpha \in F$ such that $|\alpha - \lambda| < 1/1000$ and $T - \alpha I$ is
  invertible.}

Since $V$ is finite-dimentional, we know that there can exist only a finite amount of distinct
eigenvalues. Since the amount of elements of $F$ is infinite in the neighborhood around
$\lambda$ (I don't know if neighborhoods apply to the complex numbers, but I think that they do),
we follow that there exists a number $\alpha$ such that $|\alpha - \lambda| < 1/1000$ and
such that $\alpha$ is not an eigenvalue of $T$. Thus we follow that $T - \alpha I$ is
invertible, as desired.

\subsection{}

\textit{Suppose $V = U \oplus W$, where $U$ and $W$ are nonzero subspaces of $V$. Define
  $P \in \map(V)$ by $P(u + w) = u$ for $u \in U$ and $w \in W$. Find all eigenvalues
  and eigenvectors of $P$.}

$0$ and $1$ for eigenvectors $u$ and $w$ respectively.

\subsection{}

\textit{Suppose $T \in \map(V)$. Suppose $S \in \map(V)$ is invertible.}

\textit{(a) Prove that $T$ and $S \inv T S$ have the same eigenvalues.}

let $\lambda$ be an eigenvalue of $T$ and $v$ is a corresponding eigenvector. Then we follow that
$$Tv = \lambda v$$
Because $S$ is invertible, we follow that it is surjective, and
there exists $w \in V$ such that $Sw = v$ and thus $S \inv v = w$. Thus
$$S \inv T S w = S \inv T v = S \inv (\lambda v) = \lambda  S \inv v = \lambda w$$
Thus if $\lambda$ is an eigenvalue of $T$, then it is an eigenvalue of $S \inv T S$.

Suppose that $\lambda$ is an eigenvalue of $S \inv T S$ with corresponding eigenvector $w$.
Then we follow that
$$S \inv T S w = \lambda w$$
$$S (S \inv T S w) = \lambda S w$$
$$T (S w) = \lambda (S w)$$
thus we follow that $\lambda$ is an eigenvalue for $T$. Therefore we follow that the set of
eigenvalues of $T$ and $S \inv T S$ is equal by double inclusion.

\textit{(b) What is the relationship between the eigenvectors of $T$ and the eigenvectors
  of $S \inv T S$?}

Every eigenvector of $T$ can be aquired by putting eigenvetor of $S \inv T S$ through
$S \inv$.

\subsection{}

\textit{Suppose $V$ is a complex vector space, $T \in \map(V)$, and the matrix of $T$ with
  respect to some basis of $V$ contains only real entries. Show that if $\lambda$ is an eigenvalue
  of $T$, then so is $\overline{\lambda}$.}

Suppose that $\lambda$ is an eigenvalue of $T$ with corresponding eigenvector $w$. Let
$v_1, ..., v_n$ be a basis of $V$, with respect to which the matrix of $T$ contains only
real entries. Then we follow tha t
$$w = \sum{a_j v_j}$$
$$T w = T \sum{a_j v_j} = v_1 {\sum a_j A_{1, j} }  + ... + v_n {\sum a_j A_{n, j} } = \lambda w
= \lambda a_1 v_ 1 + ... + \lambda a_n v_n$$
thus we follow that
$${\sum a_j A_{1, j} } = \lambda a_1$$
from this we follow that 
$$\overline{{\sum a_j A_{1, j} }} = \overline{\lambda a_1}$$
$${\sum \overline{a_j} A_{1, j} } = \overline{\lambda} \overline{ a_1}$$
thus we follow that for
$$w' = \sum{\overline{a_j} v_j}$$
it is true that
$$Tw' = T \sum{\overline{a_j} v_j} = v_1 {\sum \overline{a_j} A_{1, j} }
+ ... + v_n {\sum \overline{a_j} A_{n, j}} = \overline{\lambda} \overline{a_1} v_1  + ...
\overline{\lambda} \overline{a_n} v_n = \overline{\lambda} \sum{\overline{a_j} v_j} =
\overline{\lambda} w' $$
thus $w'$ is an eigenvector of $T$ with eigenvalue $\overline{\lambda}$, as desired.

\subsection{}

\textit{Give an example of an operator $T \in \map(R^4)$ such that $T$ has no (real) eigenvalues.}

Since the operator in exercise 7 didn't have any real eigenvalues, the idea is to double it,
and see what happens. If we create a map
$$T(x, y, z, w) = (-3y, x, 3w, z)$$
then after plugging this thing into some sophisticated software we get that it doesn't
have any real eigenvalues.

\subsection{}

\textit{Show that the operator $T \in \map(C^\infty)$ defined by}
$$T(z_1, z_2, ...) = (0, z_1, ... )$$
has no eigenvalues.

Suppose that it has one. Then we follow that
$$\lambda z_1 =  0$$
$$\lambda z_2 = z_1$$
$$\lambda z_3 = z_2$$
and so on. If $z_1 = 0$, then we follow that the other values in the supposed eigenvector are
also equal to zero, thus this won't do. If $z_1 \neq 0$, then we follow that
$$\lambda z_2 = z_1  \neq 0$$
but
$$\lambda z_1 = 0$$
thus we can follow that $\lambda = 0$, and by extension $\lambda z_2 = z_1 = 0$, which is a
contradiction. Thus there is no suitable value for $z_1$, therefore there does not exist an
eigenvector for this map.


\subsection{}

\textit{Suppose $n$ is a positive integer and $T \in \map(F^n)$ is defined by}
$$T(x_1, ..., x_n) = (x_1 + ... + x_n, ..., x_1 + ... x_n)$$
\textit{In other words, $T$ is the operator whose matrix (with respect to the standart basis)
  consists of all 1's. Find all eigenvalues and eigenvectors of $T$.}


Since $\dim \range T = 1$, we follow that $\dim \ns T = n - 1$, and as long as $n \neq 1$, we
follow that $T$ is not injective. Thus
$$T - 0I$$
is not injective, and $0$ is an eigenvalue of $T$. Eigenvectors of $T$ consist of any vector in
$\ns T$ with the exception of $0$.

The other eigenvalue is n with the eigenvector $(x, x, ..., x)$

\subsection{}

\textit{Find all eigenvectors fo the backward shift operator $T \in \map(F^\infty)$ defined by}
$$T(z_1, ...) = (z_2, z_3, ...)$$

Obliously $(1, 1, ...)$ and its nonzero scalar are eigenvectors of $T$ with the eigenvalue
of $1$.

$$\lambda z_1 = z_2$$
$$\lambda z_2 = z_3$$
Suppose that $z_j \neq z_k$. Then we follow that all the values before $z_j$ are equal to $z_j$
and all the values after $z_k$ are equal to $z_k$. There is no such $\lambda$ that can do
the trick, thus we follow that $1$ is the only eigenvalue of $T$.

\subsection{}

\textit{Suppose $T \in \map(V)$ is invertible.}

\textit{(a) Suppose $\lambda \in F$ with $\lambda \neq 0$. Prove that $\lambda$ is an eigenvalue
  of $T$ if and only if $1/\lambda$ is an eigenvalue of $T \inv$}

Suppose that $\lambda \in F$ with $\lambda \neq 0$ is an eigenvalue of $T$. Then we follow that
$$Tv = \lambda v$$
thus
$$T \inv T v = I v = T \inv \lambda v =  \lambda T \inv v$$
$$v = \lambda T \inv v$$
$$1/\lambda v = T \inv v$$
thus $1/\lambda v$ is the eigenvalue of $T \inv$.

Given that $T = (T \inv)\inv$ and $1/(1/\lambda) = \lambda$ we follow the converse result from
forward implication (how nice).

\textit{(b) Prove that $T$ and $T \inv $ have the same eigenvectors}

If we look at the deriviation of the previous result more closely, we see that we've shown
it for the forward case, and by the fact that $T \inv \inv  = T$ we get the converse as well.


\subsection{}

\textit{Suppose $T \in \map(V)$ and there exist nonzero vectors $v$ and $w$ in $V$ such that }
$$Tv = 3w \text{ and } Tw = 3v$$
\textit{Prove that $3$ or $-3$ is an eigenvalue of $T$}
$$Tv + Tw = T(v + w) = 3w + 3v = 3(v + w)$$
and
$$Tv - Tw = T(v - w) = 3w - 3v = -3(v - w)$$
given that $v$ and $w$ are nonzero, we follow that $v + w$ or $v - w$ bound to be nonzero.
Thus either 3 or -3 (or both) are eigenvalues of $T$.


\subsection{}

\textit{Suppose $V$ is finite-dimentional and $S, T \in \map(V)$. Prove that $ST$ and
  $TS$ have the same eigenvalues.}

Let $\lambda$ be ean eigenvalue of $ST$ with corresponding eigenvector $v$. Then we follow that
$$ST v = \lambda v$$
$$TST v = T\lambda v$$
$$TS(T v) = \lambda (T v)$$
thus $\lambda$ is an eigenvalue of $TS$.

Since we can apply this by setting $S = T$ and $T = S$ and getting the same result, we can follow
that they have the same eigenvalues.

\subsection{}

\textit{Suppose $A$ is an $n-by-n$ matrix with entries in $F$. Define $T \in \map(F^n)$
  by $Tx = Ax$, where elements of $F^n$ are thought of as $n-by-1$ column vectors.}

\textit{Suppose the sum of the entries in each row of $A$ equals 1. Prove that $1$ is an
  eigenvalue of $T$}

By plugging  vector $(1, ..., 1)$  into the matrix we get the desired result.

\textit{(b) Suppose the sum of the entries in each column of $A$ equals 1. Prove that $1$
  is an eigenvalue of $T$.}

Let $V$ be finite-dimentional vector space and let $T \in \map(V)$. Suppose that $\lambda$ is
an eigenvalue of $T$. Then we follow that 
$$T - \lambda I$$
is not invertible and therefore not surjective. Thus we can also follow that
$$(T - \lambda I)'$$
is not injective and therefore also not invertible. Thus we conclude that
$$T' - \lambda I'$$
is not invertible and thus $\lambda$ is an eigenvalue of $T'$.

Since every implication in previous paragraph is also an equivalence, we follow that
$T$ and $T'$ have the same eigenvalues. Thus we follow that since $T'$ for which
$\mathcal(T') = A^t$ has $1$ as an eigenvalue by previous paragraph, then we conclude that $T$
also has an eigenvalue $1$, as desired.


\subsection{}

\textit{Suppose $T \in \map(V)$ and $u, v$ are eigenvectors of $T$ such that $u + v$ is also an
  eigenvector of $T$. Prove that $u$ and $v$ are eigenvectors of $T$ corresponding to the same
  eigenvalue.}

Suppose that they aren't. Then we follow that
$$T(u + v) = \lambda_3(u + v) =  \lambda_3 u + \lambda_3 v =
T(u) + T(v) = \lambda_1 u + \lambda_2 v$$
$$\lambda_3 u + \lambda_3 v = \lambda_1 u + \lambda_2 v$$
$$(\lambda_3 - \lambda_1) u =  - (\lambda_3  - \lambda_2) v$$
Since $\lambda_1 \neq \lambda_1$ we follow that either of (or both) $\lambda_3 - \lambda_1$ or
$\lambda_3  - \lambda_2$ is not equal to zero.
If  $\lambda_3 - \lambda_1$ is equal to zero, then we follow that
$$0 = (\lambda_3 - \lambda_2) v$$
since $(\lambda_3 - \lambda_2)$ is not equal to zero we follow that $v = 0$, which is
a contradiction. Thus we follow that both $\lambda_3  - \lambda_2$ and $\lambda_3  - \lambda_2$
are not equal to zero. Thus we conclude that 
$$\frac{ - (\lambda_3  - \lambda_2)}{(\lambda_3 - \lambda_1)} u =  v$$
therefore $u$ and $v$ are not linearly independent, which is a contradiction. Thus we
conclude that the case when $\lambda_1 \neq \lambda_2$ is impossible, and therefore
we've got our desired conclusion.


\subsection{}

\textit{Suppose $T \in \map(V)$ is such that every nonzero vector in $V$ is an eigenvector of $T$.
  Prove that $T$ is a scalar multiple of the identity operator.}

Let $v, w \in V \setminus \{0\}$.
Then we follow that $v + w$ is also an eigenvector, and therefore by the
results of the previous exercise we've got that they correspond to the same eigenvalue. Thus
we follow that all of the vectors in $V \setminus \{0\}$ correspond to the same eigenvalue.
Let $\lambda$ be this eigenvalue. Then we follow that for $v \in V$
$$Tv = \lambda v$$
if $v \neq 0$ and if $v = 0$, then
$$Tv = T0 = 0 = \lambda * 0 = \lambda v$$
thus we conclude that
$$Tv = \lambda v = (\lambda I)v$$
for every $v \in V$. Thus we follow that $T = \lambda I$, as desired.

\subsection{}

\textit{Suppose $V$ is finite-dimentional and $T \in \map(V)$ is such that every subspace of $V$
  with dimention $\dim V - 1$ is invariant under $T$. Prove that $T$ is a scalar multiple of
  the identity operator.}

If $T = 0$, then we've got that $T = 0I$, thus assume that $T \neq 0$.
Let $v_1$ be such that $Tv_1 \neq 0$. Expand $v_1$ to the basis of $V$ - $v_1, ..., v_n$.
Then we follow that $\Span(v_2, ..., v_n)$ is invariant under $T$.
Let $Tv = \sum{a_j v_j}$. We can follow that since 
$\Span(a_1, ..., a_{i - 1}, a_{i + 1} ..., a_n)$ is invariant under $T$, then $a_i = 0$
for $i \neq 1$. Thus we conclude that
$$Tv = \sum{a_j v_j} = a_1 v_1 + \sum_{j \neq 1}{a_j v_j} = a_1 v_1$$
Thus we can follow that $v$ is an eigenvalue of $T$. Thus we've got that every nonzero
vector of $V$ is an eigenvalue of $T$, therefore by previous exercise we follow that
$T$ is a scalar multiple of identity.

\subsection{}

\textit{Suppose $V$ is finite-dimentional with $\dim V \geq 3$ and $T \in \map(V)$ is such
  that every 2-dimentional subspace of $V$ is invariant under $T$. Prove that $T$ is a
  scalar multiple of the identity operator.}

Let $U$ be a subspace of $V$ such that $\dim U = 3$ and let $u_1, u_2, u_3$ be a basis of this
subspace. Let $w \in U$. Then we follow that 
$$w = a_1 v_1 + a_2 v_2 + a_3 v_3$$
thus
$$Tw = T(a_1 v_1 + a_2 v_2 + a_3 v_3) = T(a_1 v_1 + a_2 v_2) + T(a_3 v_3)$$
since every 2-dimentional subspace is invariant in $V$, we follow that
$T(a_1 v_1 + a_2 v_2) + T(a_3 v_3) = $ is in
$\Span(u_1, u_2) + \Span(u_2 + u_3) = \Span(u_1, u_2, u_3)$
Thus we follow that $U$ is invariant under $T$. Therefore we've got that every 3-dimentional
subspace is invariant under $T$, and by induction we can follow that this is true for
any subspace of $V$. Thus we follow that every subspace with dimention $n - 1$ is invariant
under $T$, therefore by our previous exercise we've got the desired result.

\subsection{}

\textit{Suppose $T \in \map(V)$ and $\dim \range T = k$. Prove that $T$ has at most $k + 1$
  distinct eigenvalues.}

Because eigenvectors for distinct eigenvalues have got to be linearly independent, we can follow
that we can we can have at most $k$ nonzero distinct eigenvalues, since
$Tv = \lambda v \in \range T \neq 0$.

If $T$ is not injective, we can have an additional distinct eigenvalue of $0$, thus increasing
total number of disctinct eigenvalues to $k + 1$.

\subsection{}

\textit{Suppose $T \in \map(R^3)$ and $-4, 5, \sqrt{7}$ are eigenvalues of $T$. Prove that
  there exists $x \in R^3$ such that $Tx - 9x = (-4, 5, \sqrt{7})$.}

Since 9 is not an eigenvalue of $T$, we can follow that $T - 9I$ is injective and therefore
invertible and surjective. Thus by surjectivity of $T - 9I$ we follow that
there exists a desired vector.


\subsection{}

\textit{Suppose $V$ is finite-dimentional and $v_1, ..., v_m$ is a list of vectors in $V$. Prove
  that $v_1, ..., v_m$ is linearly independent if and only if there exists $T \in \map(V)$ such
  that $v_1, ..., v_m$ are eigenvectors of $T$ corresponding to distinct eigenvalues.}

\textbf{In forward direction: }

We can make a map
$$T(a_1 v_1 + ... + a_m v_m) = (a_1 v_1 +  2 a_2 v_2 + ... + m a_m v_m)$$
that satisfies required parameters.

\textbf{In reverse direction: }

Trivial, follows directly from the fact that eigenvectors for distinct eigenvalues are linearly
independent.

\subsection{}

\textit{Suppose $\lambda_1, ..., \lambda_n$ is a list of distinct real numbers. Prove that
  the list $\exp(\lambda_1 x), ..., \exp(\lambda_n x)$ is linearly independent in the vector
  space of real-valued functions on $R$.}

Let $V = \Span(\exp(\lambda_1 x), ..., \exp(\lambda_n x)$ and define an operator $T \in \map(R^R)$
by $Tf =  f'$ (linearity of this thing was proven somewhere in the 3rd chapter). Then we
follow that
$$T(\exp(\lambda_j x)) = \lambda_j \exp(\lambda_j x)$$
which comes from calculus. Thus we follow that $\lambda_1, ..., \lambda_m$ are distinct
eigenvalues of $T$. Therefore we can conclude that $\exp(\lambda_1 x), ..., \exp(\lambda_n x)$
is linearly independent.

\subsection{}

\textit{Suppose $T \in \map(V)$. Prove that $T/(\range T) = 0$}

Let $v \in V$. Then we follow that
$$Tv \in \range T$$
and
$$0 \in \range T$$
Thus we can follow that
$$T(v + (\range T)) = 0 + (\range T)$$
Thus $T/(\range T) = 0$, as desired.


\subsection{}

\textit{Suppose $T \in \map(V)$. Prove that $T/(\ns T)$ is injective if and only if
  $\ns T \cap \range T = \{0\}$}

Suppose that $T/(\ns T)$ is injective. Then we can follow that
$$T/(\ns T)(v + \ns T) = (Tv + \ns T) = 0 + \ns T$$
if and only if $Tv + \ns T = 0 + \ns T$. $Tv + \ns T = 0 + \ns T$ if and only if
$$Tv - 0\in \ns T$$
$$Tv \in \ns T$$
thus we follow that $\ns T \cap \range T = \{0\}$. Since we've only used equivalences here,
we've got both cases at the same time.

\subsection{}

\textit{Suppose $V$ is finite-dimentional, $T \in \map(V)$, and $U$ is invariant under $T$. Prove
  that each eigenvalue of $T/U$ is an eigenvalue of $T$.}

Let $\lambda$ be an eigenvalue of $T/U$. Then we follow that there exists $v \in V$ such that
$$(T/U)(v + U) = Tv + U = \lambda v + U $$
If $v \in U$, then $v + U = 0$, which is impossible, because eigenvectors are not zero.
Let $u_1, ..., u_m$ be a basis of $U$ and extend this basis to $u_1, ..., u_m, v_1, ..., v_n$ -
basis of $V$. Then we follow that
$$Tv + U = \lambda v + U $$
$$Tv - \lambda v + U = 0$$
$$(Tv - \lambda v) \in U$$
$$(T - \lambda I) (v) \in U$$
$$(T - \lambda I) (\sum {a_j v_j} + \sum {a_i u_i}) = \sum{b_j u_j}$$
$$(T - \lambda I) (\sum {a_j v_j})  + (T - \lambda I) (\sum {a_i u_i}) = \sum{b_j u_j}$$
Since $U$ is invariant under $U$, we can follow that $(T - \lambda I) (\sum {a_i u_i}) \in U$.
Thus
$$(T - \lambda I) (\sum {a_j v_j}) = \sum{b_j u_j} -  (T - \lambda I) (\sum {a_i u_i})$$
$$(T - \lambda I) (\sum {a_j v_j}) \in U$$
Since $v \notin U$, we follow that $\sum {a_j v_j} \neq 0$ and therefore $\sum {a_j v_j}$ and
$u_1, ..., u_m$ are linearly independent. 
Therefore we follow that  $(T - \lambda I)$ maps $\dim U + 1$ linearly independent vectors
into a space with $\dim U$. Thus we follow that it is not injective, and therefore
$\lambda$ is an eigenvalue of $T$, as desired.

\subsection{}

\textit{Give an example of a vector space $V$, an operator $T \in \map(V)$, and a subspace $U$
  of $V$ that is invariant under $T$ such that $T/U$ has an eigenvalue that is not an eigenvalue
  of $T$.}

$$T(z_1, z_2, ...) = (0, z_2 + z_1, z_3 + z_1, ...)$$

If we set $U = (0, z_1, z_2, ....)$, then $T/U$ has an eigenvalue of $0$ and $T$ doesn't.

\section{Eigenvectors and Upper-Triangular Matrices}

\subsection{}

\textit{Suppose $T \in \map(V)$ and there exists a positivee integer $n$ such that $T^n = 0$.}

\textit{(a) Prove that $I - T$ is invertible and that}
$$(I - T)^{-1} = I + T + ... + T^{n - 1}$$

Suppose that $I - T$ is not invertible. Then we follow that $1$ is an eigenvalue of $T$. Thus
we follow that there exists an eigenvector $v \neq 0$, corresponding to this value, for which it
is true that 
$$v = Tv = T(Tv) = T^2v = T^3v = ... = T^nv$$
Since $v \neq 0$, we follow that $T^nv \neq 0$, and therefore $T^n \neq 0$ which is a
contradiction. Therefore we can follow that $T - I$ is invertible.

$$(I - T)(I + T + ... + T^{n - 1}) = (I - T)I + (I - T)T + ... + (I - T)T^{n - 1} = $$
$$ = I - T + T - T^2 + ... + T^{n - 1}  - T^n =  I - T^n = I - 0 = I$$
$$(I - T)\inv = I + T + ... + T^{n - 1} $$

\textit{(b) Explain how you would guess the formula above.}

From its deriviation

\subsection{}

\textit{Suppose $T \in \map(V)$ and $(T - 2I)(T - 3I)(T - 4I) = 0$. Suppose $\lambda$
  is an eigenvalue of $T$. Prove that $\lambda = 2$, $\lambda = 3$ or $\lambda = 4$.}

Suppose that $\lambda \neq 2$, $\lambda \neq 3$ and $\lambda \neq 4$. Then we follow that
$(T - 2I)$, $(T - 3I)$, and $(T - 4I)$ are all invertible. Since the product of invertible
matrices is invertible, we follow that it is also injective and therefore for $v \neq 0$
$$(T - 2I)(T - 3I)(T - 4I)v \neq 0$$
thus
$$(T - 2I)(T - 3I)(T - 4I) \neq 0$$
which is a contradiction.

\subsection{}

\textit{Suppose $T \in \map(V)$ and $T^2 = I$ and $-1$ is not an eigenvalue of $T$. Prove that
  $T = I$.}

$$T^2 = I$$
$$T^2 - I = 0$$
$$(T + I)(T - I) = 0$$
from the previous exercise we follow that if $\lambda$ is an eigenvalue of $T$, then it's either
$1$ or $-1$. Because $-1$ is not an eigenvalue of $T$, we follow that $(T + I)$ is invertible
and therefore
$$(T + I)(T - I) = 0$$
$$(T + 1)\inv(T + I)(T - I) = (T + 1)\inv 0$$
$$I(T - I) = 0$$
$$T - I = 0$$
$$T = I$$
as desired.

\subsection{}

\textit{Suppose $P \in \map(V)$ and $P^2 = P$. Prove that $V = \ns P \oplus \range P$}

$V = \ns P \oplus \range P$ means that $V = \ns P + \range P$ and $\ns P \cap \range P = \{0\}$.

To prove the latter part  we
suppose that $v \in \ns P$ and $v \in \range P$. If $v \neq 0$, then we follow that
there exists $w \in V \neq 0$ such that $Pw = v$. Because $v \in \ns P$ we follow that
$$Pv = 0$$
$$P(v) = 0$$
$$P(Pw) = 0$$
$$P^2w = 0$$
$$Pw = 0$$
$$v = 0$$
which is a contradiction. Thus we follow that there does not exist $v \neq 0$ such that
$v \in \ns P$ and $v \in \range P$ at the same time. Therefore we can conclude that
$$\ns P \cap \range P = \{0\}$$

We know that for every $v \in V$ we've got that 
$$v = v$$
$$v = v - Pv + Pv $$
Since
$$P(v - Pv) = Pv - P^2v = Pv - Pv = P(v - v) = P(0) = 0$$
we follow that $v - Pv \in \ns P$. $Pv \in \range P$. Thus we follow that
$$V = \ns P + \range P$$


Thus we can conclude that $V = \ns P \oplus \range P$, as desired.


\subsection{}

\textit{Suppose $S, T \in \map(V)$ and $S$ is invertible. Suppose $p \in P(F)$ is a polynomial.
  Prove that }
$$p(STS\inv) = S p(T) S\inv$$

$$(STS\inv)^m = (STS\inv)(STS\inv)...(STS\inv) =
ST(S\inv S)T(S\inv ...S)TS\inv) = STITI...ITS\inv = ST^mS\inv$$
thus we follow that
$$p(STS\inv) = \sum {a_j (STS\inv)^j} = \sum {a_j ST^jS\inv} = S\sum {[a_j T^j]}S\inv = Sp(T)S\inv$$
as desired.

\subsection{}

\textit{Suppose $T \in \map(V)$ and $U$ is a subspace of $V$ invariant under $T$. Prove that
  $U$ is invariant under $p(T)$ for every polynomial $p \in P(F)$}

Suppose that $u \in U$ is invariant under $T$. Then we follow that
$$Tu \in U$$
and by induction
$$T^mu = T(T^{m - 1}u) \in U$$
Thus we follow that $a_j T^ju  \in U$ by  closure under scalar multiplication of subspaces and 
$$\sum{a_j T^ju} = p(T) \in U$$
by additive closure of subspaces for any $p \in P(F)$.

\subsection{}

\textit{Suppose $T \in \map(V)$. Prove that $9$ is an eigenvalue of $T^2$ if and only if
  $3$ or $-3$ is an eigenvalue of $T$.}

Suppose that $9$ is an eigenvalue of $T^2$. Then we follow that $(T^2 - 9I)$ is not injective.
Thus
$$(T^2 - 9I) = (T - 3I)(T + 3I)$$
is not injective. Thus we follow that either one of $T - 3I$ of $T + 3I$ is not injective. Thus
we follow that $-3$ or $3$ is an eigenvalue of $T$.

Suppose that either $3$ or $-3$ is an eigenvalue of $T$. Then we follow that
$$T^2v = 3(3v) = 9v$$
or
$$T^2v = -3(-3v) = 9v$$
thus $9$ is an eigenvalue of $T$, as desired.

\subsection{}

\textit{Give an example of $T \in \map(R^2)$ such that $T^4 = -1$}

After some though I came up with the
$$
\frac{\sqrt{2}}{2}
\begin{pmatrix}
  1 & -1 \\
  1 & 1
\end{pmatrix}
$$

Which seems to be working.

\subsection{}

\textit{Suppose $V$ is finite-dimentional, $T \in \map(V)$ and $v \in V$ with $v \neq 0$.
  Let $p$ be a nonzero polynomial of smallest degree such that $p(T)v \neq 0$. Prove that every
  zero  of $p$ is an eigenvalue of $T$.}

We've got that
$$p(T)v = 0$$
$$\prod{(T - \lambda_j I)}v = 0$$
Suppose that $\lambda_i$ is not an eigenvalue of $T$. Then we follow that $(T - \lambda_i)$ is
invertible and therefore we've got that for 
$$(T - \lambda_i)\left(\prod_{j \neq i}{(T - \lambda_j I)} v \right) = 0$$
$$\prod_{j \neq i}{(T - \lambda_j I)} v  = 0 =
(T - \lambda_i)\left(\prod_{j \neq i}{(T - \lambda_j I)} v \right)$$
thus we can follow that $p$ is not in its lowest degree, which is a contradiction.

\subsection{}

\textit{Suppose $T \in \map(V)$ and $v$ is an eigenvector of $T$ with eigenvalue $\lambda$.
  Suppoe $p \in P(F)$. Prove that $p(T)v = p(\lambda)v$}

Suppose that $\lambda$ is an eigenvalue for an eigenvector $v$. Then we follow that
$$T^m(v) = \lambda^m v$$
thus for our problem we've got that
$$p(T)v = \sum{(a_j T^j)}v = \sum{(a_j T^j v)} = \sum{(a_j \lambda^j v)} =
\sum{(a_j \lambda^j )}v = p(\lambda)v $$
as desired.

\subsection{}

\textit{Suppose $F = C$, $T \in \map(V)$, $p \in P(C)$ is a polynomial, and $a \in C$.
  Prove that $a$ is an eigenvalue of $p(T)$ if and only if $a = p(\lambda)$ for some
  eigenvalue $\lambda$ of $T$.}

\textbf{In forward direction: }

Suppose that $a$ is an eigenvalue of $p(T)$. Then we can follow that there exists eigenvector
$v \neq 0$ such that
$$p(T)v = av$$
$$p(T)v - av = 0$$
$$p(T)v - aIv = 0$$
$$(p(T) - aI)v = 0$$
Since $p \in P(C)$, we follow taht $p - a \in P(C)$. Thus there exists factorization
$c\prod{(z - \lambda_i)} = p - a$. Thus
$$c\prod{(T - \lambda_iI)} v = 0$$
If $c = 0$, then we follow that $p(z) - a = 0$, and therefore $p(z) = a$ and $p(\lambda) = a$ for
any eigenvalue of $T$. Thus suppose that $c \neq 0$. Then we've got that one of $(T - \lambda_iI)$
is not injective and therefore $\lambda_i$ is an eigenvelue of $T$, for which
$$c(z - \lambda_i ) = 0$$
thus
$$p(\lambda_i) - a = 0$$
$$p(\lambda_i)  = a$$
as desired.

\textbf{In reverse direction: }

Suppose that $a = p(\lambda)$ for some eigenvalue of $p$. Then we follow that
for eigenvector $v$ , that correspond to this eigenvalue 
$$Tv = \lambda v$$
thus
$$p(T)v = p(\lambda)v = av$$
thus $a$ is an eigenvalue of $p(T)$, as desired.


\subsection{}

\textit{Show taht the result in the previous exericse does not hold if $C$ is replaced with $R$.}

With $F = R$ we're not guaranteed that $T$ has an eigenvalue in finite dimnetions. Thus we
can follow that for our map from previous chapteer
$$T(x, y) = (-3y, x)$$
and polynomial
$$p(z) = z^2$$
we've got that $p(T)$ has an eigenvalue 3, but no eigenvalues exist for $T$.


\subsection{}

\textit{Suppose $W$ is a complex vector space and $T \in \map(W)$ has no eigenvalues. Prove that
  every subspace of $W$ invariant under $T$ is either $\{0\}$ or
  infinite-dimentional.}

Suppose that $U$ is an invariant finite-dimentional nonzero subspace of $W$. Then we follow that
it has an eigenvalue $\lambda$ under $T$ with a corresponding eigenvector $v \neq 0$
(as does any finite-dimentional nonzero complex space).
Thus we follow that $Tv = \lambda v$, therefore
$T$ has an eigenvalue, which is a contradition. Thus we follow that there does not exist an
invariant finite-dimentional nonzero subspace of $W$, that is invariant under $T$. Therefore
we follow that if a subspace is invariant under $T$, then it's either not finite-dimentional
(i.e. infinite dimentional), or zero, as desired.

\subsection{}

\textit{Give an example of an operator whose matrix with respect to some basis contains only $0$'s
on the diagonal, but the operation is invertible}

$$T \in \map(C^2): T(x, y) = (-3y, x)$$

\subsection{}

\textit{Give an example of an operator whose matrix with respect to some basis contains only
  nonzero numbers on the diagonal, but the operator is not invertible}

$$
\begin{pmatrix}
  1 & 1 \\
  1 & 1
\end{pmatrix}
$$

\subsection{}

\textit{Rewrite the proof of 5.21 using the linear map that sends $p \in P_n(C)$ to $(p(T))v \in V$
  (and use 3.23)}


We follow that there exists a linear map $S$ from $P_n(C)$ to $V$ defined by
$$S(p) = p(T)v$$
thus we follow that $\dim P_n(C) = n + 1 > n = \dim V$, therefore $S$ is not injective (by 3.23),
and thus there exists a polynomial $p \in P_n(C) \neq 0$ such that
$$S(p) = \sum (a_j T^j) v = 0 $$
after this we've got the same proof.


\subsection{}

\textit{Rewrite the proof of 5.21 using the linear map that sends $p \in P_{n^2}(C)$ to
  $p(T) \in \map(V)$ (and use 3.23)}

We follow that $\dim \map(V) = n^2 < n^2 + 1 = \dim P_{n^2}(C)$. Thus we follow that
the map that sends
$$S(p) = p(T)$$
is not injective. Thus there exists $p \in P_{n^2}(C)$ such that
$$S(p) = p(T) = 0$$
the rest is the same.

\subsection{}

\textit{Suppose $V$ is a finite-dimentional complex vector space and $T \in \map(V)$.
  Define a function $f: C \to R$ by}
$$f(\lambda) = \dim \range (T - \lambda I)$$
\textit{Prove that $f$ is not a continous function}

Let $V_\delta(\lambda)$ be a neighborhood for $\lambda$ for arbitrary $\delta$. Since any
neighborhood has infinite amount of distinct numbers, we follow that there exists
$x \in V_\delta(\lambda)$ that is not an eigenvalue of $T$. Thus we follow that
there is no neighborhood of $\lambda$ such that $x \in V_\delta(\lambda) \to
\dim \range (T - x I) = \dim \range (T - \lambda I)$. Thus we follow that
the function is not continous, as desired.

\subsection{}

\textit{Suppose $V$ is finite-dimentional with $\dim V > 1$ and $T \in \map(V)$. Prove
  that }
$$\{p(T): p \in P(F)\} \neq \map(V)$$

For now, let us concentrate on $F = C$. If $T$ has only 1 eigenvalue, then we follow that
by the virtue of the fact that there exists an upper-triangular matrix for it, that
there exist at least two vectors such that
$$p(T)v_1 = p(\lambda)v_1 = p(\lambda)v_2 = p(T)v_2$$
therefore we follow that $\{p(T): p \in P(F)\}$ does not have maps  $S$ such that $Sv_1 \neq Sv_2$.
If $T$ has more then 1 eigenvalues, then the case is reversed.

Case of $F = R$ is a somewhat specific case of $F = C$, which is resolved with the same logic
and maybe minor modifications

\subsection{}

\textit{Suppose $V$ is finite-dimentional complex vector space and $T \in \map(V)$. Prove
  that $T$ has an invariant supspace of dimention $k$ for $1 \leq k \leq \dim V$.}

$T$ has an upper-triangular map for some basis $v_1, ..., v_n$. Thus it has a subspace
$$\Span(v_1, ..., v_k)$$
with desired properties

\section{Eigenspaces and Diagonal Matrices}

\subsection{}

\textit{Suppose $T \in \map(V)$ is diagonalizable. Prove that $V = \ns T \oplus \range T$.}

Although it wasn't mentioned specifically, I think that we can follow that if $T$ is
diagonalizable in some space $V$, then this space if finite-dimentional. Thus we can follow that
$V$ has a basis $v_1, ..., v_n$ of eigenvectors of $T$ with corresponding eigenvalues
$\lambda_1, ..., \lambda_n$, where $\lambda$'s can repeat. Thus for $v \in V$
$$v = \sum {a_j v_j}$$

If $\lambda_j \neq 0$, then we follow that there exists $\frac{a_j}{\lambda_j} v_j$ such that
$$T \frac{a_j}{\lambda_j} v_j  = a_j v_j$$

If $\lambda_j = 0$, then we follow that $v_j \in \ns T$. Thus
$$v = \sum {a_j v_j} = \sum_{\lambda \neq 0} {a_j v_j} + \sum_{\lambda = 0} {a_j v_j}$$
where $\sum_{\lambda \neq 0} {a_j v_j} \in \range T$ and $\sum_{\lambda = 0} {a_j v_j} \in \ns T$.
Thus we follow that $V = \ns T + \range T$.

Since $v_1, ..., v_n$ is a basis, we follow that it is linearly independent, and therefore
$V = \ns T \oplus \range T$, as desired.


\subsection{}

\textit{Prove the converse of the statement in the exercise above or give a couneterexample
  to the converse.}

Suppose that $V = \ns T \oplus \range T$. I think that we're assumnming once again that
$V$ is finite-dimentional.

We know that there exist bases of both $\ns T$ and $\range T$ that add up to a basis of $V$.
Every member of basis of $\ns T$ is an eigenvector for an eigenvalue of $0$.

If $F = R$, then we can follow that there exists a map such that it has no eigenvalues,
therefore making the whole thing not diagonalizable. However if $F = C$, then we follow that
$T|_{\range T}$ has an eigenvalue (which will be nonzero, since all zeroes are in the basis
of $\ns T$) with a corresponding eigenvector. If we take one of those eigenvectors away,
we can follow that the span of rest of the vectors is invariant, because if it isn't, then
for vector $u \in \Span(v_k)$ and $w \in \Span(v_{k + 1}, ..., v_n)$
$$T(u) = T(u)$$
$$T(u - w) = 0$$
for which $u \neq w$, $u - w \in \range T$ and $u - w \in \ns T$, and therefore it violates
the fact that $V = \ns T \oplus \range T$. Thus the process can be repeated and $T$ is
diagonalizable.

Therefore we conclude that if in addition to the fact that $V = \ns T \oplus \range T$ we've
got that $V$ is a finite-dimentional vector space, we can follow that $T$ is
diagonalizable.

\subsection{}

\textit{Suppose $V$ is finite-dimentional and $T \in \map(V)$. Prove that the following are
  equivalent}

\textit{(a) $V = \ns T \oplus \range T$}

\textit{(b) $V = \ns T + \range T$}

\textit{(c) $\ns T \cap \range T = \{0\}$}

(a) implies (b) and (c) from the definition of $\oplus$. 

If $\ns T + \range T = V$, then combined bases of $\ns T$ and $\range T$ span $V$. Since
$$\dim V = \dim \ns T + \dim \range T$$
we follow that list that consists of combined bases of $\ns T$ and $\range T$ is
a list of length $\dim V$, that spans $V$, making it a basis of $V$. Thus we can follow that
this list is linearly independent, and therefore
$$\ns T \cap \range T = \{0\}$$
Thus (b) implies (c).

If
$$\ns T \cap \range T = \{0\}$$
then we follow that list, that consists of combined  bases of $\ns T$ and $\range T$ is
linearly independent. Therefore by FTLM  we've got linearly independent list of length $\dim V$,
thus making it a basis of $V$. Therefore we can follow that
(c) implies (b)

(b) and (c) implies (a), and since (b) and (c) are equivalent, we follow that both (b) and (c)
are eqivalent to (a).

\textit{Give an example to show that the exercise above is false without the hypothesis that
  $V$ is finite-dimentional.}

$$U = (0, 0, 0, x_1, ...)$$
$$W = (0, 0, x_1, 0 , ...)$$
$$U \cap W = \{0\}$$
which doesn't imply anything.


\subsection{}

\textit{Suppose $V$ is finite-dimentional complex vector space and $T \in \map(V)$. Prove that
  $T$ is diagonalizable if and only if }
$$V = \ns (T - \lambda I) \oplus \range (T - \lambda I)$$
\textit{for every $\lambda \in C$.}

Suppose that $T$ is diagonalizable. Then we follow that $V$ has a basis of eigenvectors of
$T$. For any of this vectors it is true that 
$$Tv = \kappa v$$
for some $\kappa \in F$. Thus we follow that 
$$(T - \lambda I)v = \kappa v - \lambda v = (\kappa - \lambda)v$$
and therefore we follow that $v$ is also an eigenvector for $(T - \lambda I)$. Thus
we conclude that $V$ has a basis, that consists of eigenvectors of $(T - \lambda I)$. Therefore
$(T - \lambda I)$ is also diagonalizable.

By equivalence that we've proven in exercises 1 and 2, and presented proof we have the desired
result.

\subsection{}

\textit{Suppose $V$ is finite-dimentional, $T \in \map(V)$ has $\dim V$ distinct
  eigenvalues and $S \in \map(V)$ has the same eigenvectors as $T$ (not necessarily with the
  same eigenvalues). Prove that $ST = TS$}

Because $T$ has $\dim V$ distinct eigenvalues we follow that it is diagonalizable. Thus
$V$ has a basis, that consists of eigenvectors of $T$. Since
$S$ has the same eigenvectors, we follow that $V$ has a basis, consisting of eigenvectors of $S$,
thus making $S$ diagonalizable. Let $v_1, ..., v_n$ be this basis, $\lambda_1, ..., lambda_n$
be the corresponding eigenvalues of $T$ and $\kappa_1, ..., \kappa_n$ be corresponding
eigenvalues of $S$. Let $v \in V$. Then we follow that
$$v = \sum{a_j v_j}$$
$$ST v = ST \sum{a_j v_j} = \sum{a_j ST v_j} = \sum{a_j \kappa_j \lambda_j  v_j} =
\sum{a_j  \lambda_j \kappa_j  v_j} = \sum{a_j  TS v_j} = TS \sum{a_j v_j} = TSv$$
thus $ST = TS$, as desired.

\subsection{}

\textit{Suppose $T \in \map(V)$ has a diagonal matrix $A$ with respect to some basis of $V$
  and that $\lambda \in F$. Prove that $\lambda$ appears on the diagonal of $A$ precisely
  $\dim E(\lambda,  T)$ times.}

This diagonal basis will consist of eigenvectors of $T$. Thus we follow that since
$$V = E(\lambda_1, T) \oplus ... \oplus E(\lambda_m, T)$$
and for $j \neq m$, 
$$v \in E(\lambda_j, T) \to v \notin E(\lambda_m, T)$$
there will exist $\dim E(\lambda_j, T)$ vectors, that correspond to $\lambda_j$.

\subsection{}

\textit{Suppose $T \in \map(F^5)$ and $\dim E(8, T) = 4$. Prove that $T - 2I$ or $T - 6I$
  is invertible.}

In order for $T - 6I$ to be non-injertible, we've got to have
$$\dim E(6, T) \geq 1$$
Since
$$\dim V = \sum{\dim E(\lambda_j T)} = 5$$
$$\sum{\dim E(\lambda_j T)} + \dim E(8, T) = 5$$
$$\sum{\dim E(\lambda_j T)}  = 1$$
thus we follow that there exists at most one more non-invertible $T - \lambda I$. Thus one of the
provided maps is invertible, as desired.

\subsection{}

\textit{Suppose $T \in \map(V)$ is invertible. Prove that
  $E (\lambda T) = E(\frac{1}{\lambda} T \inv)$ for every $\lambda \in F$ with $\lambda \neq 0$.}

From exercise 5.1.21 we follow that if $T$ is invertible and $\lambda$ is an eigenvalue for $T$
with corresponding eigenvector $v$,  then $v$ is an eigenvector of $T \inv$ for
value $1/\lambda$  ($\lambda \neq 0$ follows from invertability of $T$). Thus we can follow that
presented sets are equal by definition of eigenspace.

\subsection{}

\textit{Suppose that $V$ is finite-dimentional and $T \in \map(V)$. Let $\lambda_1, ..., \lambda_n$
  denote distinct nonzero eigenvalus of $T$. Prove that }
$$\sum \dim E(\lambda_j, T) \leq \dim \range T$$


$$\dim V = \dim \range T + \dim \ns T$$
since $\ns T = E(0, T)$ (follows from definitions and a moments' thought) we follow that
$$\dim V = \dim \range T + \dim E(0, T)$$
thererfore since
$$\sum \dim E(\lambda_j, T) \leq V$$
we follow that 
$$\sum \dim E(\lambda_j, T) \leq \dim \range T + \dim E(0, T)$$
$$\dim E(0, T) + \sum_{\lambda_j \neq 0} \dim E(\lambda_j, T) \leq \dim \range T + \dim E(0, T)$$
$$\sum_{\lambda_j \neq 0} \dim E(\lambda_j, T) \leq \dim \range T$$
as desired.

\subsection{}

\textit{Verify the assertion in Example 5.40}

$$
\begin{pmatrix}  41 & 7 \\
  -20 & 74
\end{pmatrix}
\begin{pmatrix}
  1 \\
  4
\end{pmatrix}
=
\begin{pmatrix}
  69 \\
  276
\end{pmatrix}
=
69
\begin{pmatrix}
  1 \\
  4
\end{pmatrix}
$$
$$
\begin{pmatrix}
  41 & 7 \\
  -20 & 74
\end{pmatrix}
\begin{pmatrix}
  7 \\
  5
\end{pmatrix}
=
\begin{pmatrix}
  322 \\
  230
\end{pmatrix}
=
46
\begin{pmatrix}
  7 \\
  5
\end{pmatrix}
$$

thus we get that for this basis  everything holds.

\subsection{}

\textit{Suppose $R, T \in \map(F^3)$ each have 2, 6, 7 as eigenvalues. Prove that there exists
  an invertible operator $S \in \map(F^3)$ such that $R = S \inv T S$}

Since $R$ and $T$ have $\dim F^3 = 3$ distinct eigenvalues, we follow that they are diagonalizable.
Thus we can follow that there exist a basis $v_1, v_2, v_3$ of $V$, that consists of eigenvalues
of $R$, and basis $u_1, u_2, u_3$ of $T$ that also consists of eigenvalues of $T$, both of
which correspond to eigenvalues 2, 3 and 7 respectively.

Define map
$$S(\sum a_j v_j) = \sum a_j u_j$$
which is invertible, because $\dim \range T = 3$. Thus we follow that for $v \in V$
$$v = a_1 v_1 + a_2 v_2 + a_3 v_3 $$ 
$$Rv = 2 a_1 v_1 + 6 a_2 v_2 + 7 a_3 v_3 $$ 
$$S \inv T S v = S \inv T S (\sum a_j v_j) =  S \inv T (\sum a_j u_j) = S \inv (2 a_1 u_1 +
6 a_2 u_2 + 7 a_3 u_3) = $$
$$ = (2 a_1 S \inv u_1 + 6 a_2 S \inv u_2 + 7 a_3 S \inv u_3) = 2 a_1 v_1 + 6 a_2 v_2 + 7 a_3 v_3 =
R(v)$$
as desired.

\subsection{}

\textit{Find $T \in \map(C^3)$ such that $6$ and $7$ are eigenvalues of $T$ and such that
  $T$ does not have a diagonal matrix with respect to any basis of $C^3$}

In order for it now to be diagonalizable, we need for $T$ not to have 3 distinct eigenvalues. Thus
let us frick around the matrix of the standard diagonale matrix.

After some deloberation I came up with
$$
\begin{pmatrix}
  6 & 0 & 0 \\
  0 & 7 & 7 \\
  0 & 0 & 7
\end{pmatrix}
$$
which seems to satisfy the desired properties.

\subsection{}

\textit{Suppose $T \in \map(C^3)$ is such that 6 and 7 are eigenvalueus of $T$. Furthermore,
  suppose $T$ does not have a diagonal matrix with respect to any basis of $C^3$. Prove that
  there exists $(x, y, z) \in F^3$ such that
  $$T(x, y, z) = (17 + 8x, \sqrt{5} + 8y, 2\pi + 8z)$$}

Since $T$ is not diagonalizable, we follow that 6 and 7 are the only eigenvalues of $T$. Thus we
follow that $8$ is not an eigenvalue of $T$. Therefore $T - 8I$ is invertibe and surjective.
Threrefore we follow that by surjectivity of $T - 8I$, that there exists $(x', y', z') \in C^3$
such that
$$(T - 8I)(x', y', z') = (17, \sqrt{5}, 2\pi)$$
thus
$$(T - 8I)(x', y', z') = (17, \sqrt{5}, 2\pi)$$
$$T(x', y', z') - 8(x', y', z') = (17, \sqrt{5}, 2\pi)$$
$$T(x', y', z') = (17, \sqrt{5}, 2\pi) + 8(x', y', z')$$\
$$T(x', y', z') = (17 + 8x', \sqrt{5} + 8y', 2\pi + 8z')$$
as desired.

\subsection{}

\textit{The Fibonacci sequence $F_1, F_2, ...$ is defined by}
$$F_0 = 0$$
$$F_1 = 1$$
$$F_2 = 1$$
$$F_n = F_{n - 2} + F_{n - 1}$$
\textit{Define $T \in \map(R^3)$ by $T(x, y) = (y, x + y)$}

I've extended the sequence back a bit becasuse I like zeroes.

\textit{(a) Show that $T^n(0, 1) = (F_n, F_{n + 1})$ for each positive integer $n$.}

This kinda follows from the definition, but we're going to use induction just in case. For
the case $n = 1$ we've got
$$T^1(0, 1) = T(0, 1) = (1, 1) = (F_1, F_2)$$

For our hypothesis we've got that
$$T^{n - 1}(0, 1) = (F_{n - 1}, F_n) $$
and for our step we've got
$$T^n(0, 1) = T(T^{n - 1}(0, 1)) = T(F_{n - 1}, F_n) = (F_n, F_{n - 1} + F_n) =
(F_n, F_{n + 1})$$
thus we follow that our inductive hypothesis is true for $n \in N$.

\textit{(b) Find the eigenvalues of $T$.}

Let $\lambda$ be an eigenvalue of $T$. Then we follow that

$$
\begin{cases}
  \lambda x = y \\
  \lambda y = x + y
\end{cases}
$$

$$\lambda (\lambda x) = x + \lambda x$$
$$\lambda^2  x = (1 + \lambda )x$$
$$\lambda^2 = (1 + \lambda )$$
$$\lambda^2 - \lambda - 1 = 0$$
$$\lambda = \frac{1 \pm \sqrt{5}}{2}$$

\textit{(c) Find a basis of $R^2$ consisting of eigenvalues of $T$}

Let $c_1 = \frac{1 + \sqrt{5}}{2}$ and $c_2 = \frac{1 - \sqrt{5}}{2}$

Suppose that $v_1, v_2$ are such vectors. Then we follow that

$$Tv_1 = T(x, y) = c_1(x, y) = (y, x + y)$$
thus if we fix $x = 1$, then we'll get that 
$$c_1(1, y) = (y, x + y)$$
$$c_1(1, y) = (c, 1 + c)$$
$$c_1(1, c) = (c, 1 + c)$$
thus one of our vectors is $v_1 = (1, c_1)$ and the other is $v_2 = (1, c_2)$.
Octave seems to concur.

\textit{(d) Use the solution to part (c) to compute $T^n(0, 1)$. Conclude that}
$$F_n = \frac{1}{\sqrt{5}}\left[\left(\frac{1 + \sqrt{5}}{2}\right)^n -
  \left(\frac{1 - \sqrt{5}}{2}\right)^n \right]$$

We follow that
$$v_1 - v_2 = (0, c_1 - c_2)$$
thus
$$\frac{1}{c_1 - c_2}(v_1 - v_2) = (0, 1)$$
therefore
$$T^n(0, 1) = T^n(\frac{1}{c_1 - c_2}(v_1 - v_2)) =
\frac{1}{c_1 - c_2} T^n(v_1 - v_2) = \frac{1}{c_1 - c_2} (c_1^n v_1 - c_2^n v_2)$$
thus
$$F_n = \frac{1}{c_1 - c_2} c_1^n  - c_2^n $$
since
$$c_1 - c_2 = \frac{1 + \sqrt{5}}{2} - \frac{1 - \sqrt{5}}{2} =
\frac{1 + \sqrt{5} - 1 + \sqrt{5}}{2} = \frac{2\sqrt{5}}{2} = \sqrt{5}$$
thus by expanding $c_1$ and $c_2$ we've got 
$$F_n = \frac{1}{c_1 - c_2} (c_1^n  - c_2^n) =
\frac{1}{\sqrt{5}}\left[\left(\frac{1 + \sqrt{5}}{2}\right)^n -
  \left(\frac{1 - \sqrt{5}}{2}\right)^n \right]$$
as desired.

\textit{(e) Use part (d) to conclude that for each positive integer $n$, the Fibonacci
  number $F_n$ is the integer that is closest to}
$$\frac{1}{\sqrt{5}}\left(\frac{1 + \sqrt{5}}{2}\right)$$

$$F_n = \frac{1}{\sqrt{5}}\left[\left(\frac{1 + \sqrt{5}}{2}\right)^n -
  \left(\frac{1 - \sqrt{5}}{2}\right)^n \right] =
\frac{1}{\sqrt{5}}\left(\frac{1 + \sqrt{5}}{2}\right)^n -
\frac{1}{\sqrt{5}} \left(\frac{1 - \sqrt{5}}{2}\right)^n  = $$

Since $|\frac{1}{\sqrt{5}} \left(\frac{1 - \sqrt{5}}{2}\right)| < 0.5$ and
$|\left(\frac{1 - \sqrt{5}}{2}\right)| < 1$ we follow that
$$|\frac{1}{\sqrt{5}} \left(\frac{1 - \sqrt{5}}{2}\right)^n| < 0.5 $$
therefore we've got our desired conclusion.

\chapter{Inner Product Spaces}

\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:

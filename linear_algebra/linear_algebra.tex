\documentclass[11pt,oneside,titlepage]{book}
\title{My linear algebra exercises}
\usepackage{amsmath, amssymb}
\usepackage{geometry}
\usepackage{hyperref}
\author{Evgeny Markin}
\date{2023}

\DeclareMathOperator \map {\mathcal {L}}
\DeclareMathOperator \ns {null}
\DeclareMathOperator \range {range}
\DeclareMathOperator \inv {^{-1}}
\DeclareMathOperator \Span {span}


\begin{document}
\maketitle
\tableofcontents

\chapter*{Preface}

Exercises are from "Linear algebra done right" by Sheldon Axler, 3rd ed.
I've already read this book before and completed some exercises from it.
Right now I want to brush up the material once again, put all the
proofs on a more durable material than paper and to prepare myself to
what's gonna happen afterwards.

\chapter*{Glossary}

FTLM - Fundamental Theorem of Linear Maps 

\chapter{Vector Spaces}
\section{$R^n$ and $C^n$}

\subsection{}
\textit{Suppose $a$ and $b$ are real numbers, not both $0$. Find real nuber
  $c$ and $d$ such that }
$$1/(a + bi) = c + di$$

$$\frac{1}{a + bi} = c + di$$
$$\frac{1}{a + bi} - c - di = 0$$
$$\frac{a - bi}{(a + bi)(a - bi)} = c + di$$
$$\frac{a - bi}{(a^2 + b^2)} = c + di$$
$$\frac{a}{a^2 + b^2} - \frac{b}{a^2 + b^2}i = c + di$$
Thus $c = \frac{a}{a^2 + b^2}$ and $d = -\frac{b}{a^2 + b^2}$

\subsection{}
\textit{Show that }
$$\frac{-1  + \sqrt{3}i}{2}$$
\textit{is a cube root of $1$ (meaning that its cube equals 1)}
$$(\frac{-1  + \sqrt{3}i}{2})^3 =
\frac{(-1  + \sqrt{3}i)^3}{8} =
\frac{(-1  + \sqrt{3}i)(-1  + \sqrt{3}i)^2}{8} =
\frac{(-1  + \sqrt{3}i)(1  - 2\sqrt{3}i - 3)}{8} =
$$
$$
=\frac{(-1  + \sqrt{3}i)(-2  - 2\sqrt{3}i)}{8} =
\frac{2 + 2\sqrt{3}i - 2\sqrt{3}i + 6}{8} =
\frac{8}{8} = 1
$$
as desired.

\subsection{}
\textit{Find two distinct square roots of $i$}

Square root of $i$, I assume, is a number, whose square is equal to $i$.
Suppose that $(a + bi)^2 = i$. It follows that
$$(a + bi)^2 = a^2 + 2abi - b^2$$
So if we set $$a = b = 1/\sqrt{2}$$ this equation holds. Also it holds for
$$a = b = -1/\sqrt{2}$$
maxima seems to agree with me on this one

\subsection{}
\textit{Show that $\alpha + \beta = \beta + \alpha$ for all
  $\alpha, \beta \in \textbf{C}$}

Let $\alpha = a_1 + b_1 i$ and $\beta = a_2 + b_2 i$. It follows
$$\alpha + \beta = a_1 + b_1 i + a_2 + b_2 i = a_2 + b_2 i + a_1 + b_1 i =
\beta  + \alpha$$
as desired.

\subsection{}
\textit{Show that $(\alpha + \beta) + \lambda = \alpha + (\beta + \lambda)$
  for all  $\alpha, \beta, \lambda \in \textbf{C}$}

Let $\alpha = a_1 + b_1 i$ , $\beta = a_2 + b_2 i$, $\lambda = a_3 + b_3 i$.
It follows that 
$$\alpha + (\beta + \lambda)  = a_1 + b_1 i + (a_2 + b_2 i + a_3 + b_3 i) =
(a_1 + b_1 i + a_2 + b_2 i) + a_3 + b_3 i = 
(\alpha + \beta) + \lambda$$

\subsection{}
\textit{Show that $(\alpha \beta) \lambda = \alpha( \beta \lambda)$}

$$\alpha + (\beta + \lambda)  = (a_1 + b_1 i) ((a_2 + b_2 i) + (a_3 + b_3 i)) =
((a_1 + b_1 i)(a_2 + b_2 i)) + (a_3 + b_3 i) = 
(\alpha \beta) \lambda$$

\subsection{}
\textit{Show that for every $\alpha \in \textbf{C}$ there exists a unique
  $\beta \in \textbf{C}$ such that $\alpha + \beta = 0$}

Suppose that there exist two different $\beta_1 \neq \beta_2$ such that
$\alpha + \beta_1 = 0$ and $\alpha + \beta_2 = 0$. It follows that
$$ \beta_1 = \beta_1 + 0 =  \beta_1 + \alpha + \beta_2 = \alpha + \beta_1  + \beta_2 = 0  + \beta_2 = \beta_2$$
which is a contradiction. Therefore there exists only one unique $\beta$.

\subsection{}
\textit{Show that for every $\alpha \in \textbf{C}$ with $\alpha \neq 0$
  there exists a unique $\beta \in \textbf{C}$ such that $\alpha \beta = 1$}

Suppose that it is not true and there exist two different
$\beta_1 \neq \beta_2$ such that
$$\alpha \beta_1 = 1 \textit{ and } \alpha \beta_2 = 1$$
it follows then that 
$$\beta_1 = 1 *  \beta_1 = \alpha \beta_2 \beta_1  =
\alpha \beta_1 \beta_2 = 1 * \beta_2 = \beta_2$$
which is a contradiction. Therefore there exists only one unique $\beta$.

\subsection{}
The rest of the section is the repetition of this kind of stuff.
That is a lot of writing, and not a lot of thinking, so I'll skip it.
I don't ususally like to skip sections, but I have  aa feeling, that I've
completed this thing on paper somewhere, and there is not much reason to
rewrite it here.

\section{Definition of Vector Space}

\subsection{}
\textit{Prove that $-(-v)= v$ for every $v \in V$.}

For $v$ there exists only one $-v$. For $-v$ there exists only one $-(-v))$.

Thus
$$v = v + 0 = v + (-v) + (-(-v)) = 0 + (-(-v)) = -(-v)$$
as desired (idk if it's true, I'm not good at axioms and stuff)

\subsection{}
\textit{Suppose $a \in F, v \in V$, and $av = 0$. Prove that
  $a = 0$ or $v = 0$.}

Suppose that $a \neq 0$, $v \neq 0$ but $av = 0$. It follows that there
exist $1/a$ - multiplicative inverse of $a$. It follows that
$$1/a * av = 1/a * 0$$
$$1v = 0$$
$$v = 0$$
which is a contradiction. Thus either $a = 0$ or $v = 0$.

\subsection{}
\textit{Suppose $v, w \in V$. Explain why there exists a unique $x \in V$
  such that $v + 3x = w$.}

Suppose that there exists $x_1 \neq x_2$ such that
$v + 3x_1 = w$ and $v + 3x_2 = w$. Thus
$$3x_1 = w - v = 3x_2$$
$$x_1 = \frac{1}{3}(w - v) = x_2$$
which is a contradiction.

Same can be stated from the fact that $x$ is a unique additive inverse of
$\frac{1}{3}(v - w)$.

\subsection{}
\textit{The empty set is not a vector space. The empty set fails to satisfy
  only one of the requirements listed in 1.19. Which one?}

Additive indentity. Empty set does not have zero element in it.
BTW $\{0\}$ is a vector space.

\subsection{}
\textit{Show that n the defintition of a vector space (1.19), the additive
  inverse condition can be replaced with the condition that}
$$0v = 0 \textit{ for all } v \in V$$
\textit{Here the 0 on the left side is the number 0, and the 0 on the right
  side is the additive identity of $V$.}

$$0v = 0$$
$$(1 - 1)v = 0$$
$$1v - 1v = 0$$
$$v - v= 0$$
$$v + (- v)= 0$$

\subsection{}
\textit{Let $\infty$ and $-\infty$ denote two distinct object, neither of
  which is in $R$. Define an addition and multiplication on
  $R \cup \{\infty\} \cup \{-\infty\}$ as you could guess from the notation.
  Specifically, the sum and the product of two real numbers is as usual,
  and for $t \in R$ define
}

$$
t\infty =
\begin{cases}
  -\infty \text{ if } t < 0 \\
  0 \text{ if } t = 0 \\
  \infty \text{ if } t > 0 \\
\end{cases}
$$

$$
t(-\infty) =
\begin{cases}
  \infty \text{ if } t < 0 \\
  0 \text{ if } t = 0 \\
  -\infty \text{ if } t > 0 \\
\end{cases}
$$

$$t + \infty = \infty + t = \infty$$
$$t + (-\infty) = (-\infty) + t = (-\infty)$$
$$\infty + \infty = \infty$$
$$(-\infty) + (-\infty) = (-\infty)$$
$$\infty + (-\infty) = 0$$

\textit{Is $R \cup \{\infty\} \cup \{-\infty\}$ a vector space over
  $R$? Explain.}

I don't think that it is.

$$(t + \infty) - \infty = \infty - \infty = 0$$
$$t + (\infty - \infty) = t + 0 = t$$
thus
$$t + (\infty - \infty) \neq (t + \infty) - \infty$$
thus 
$R \cup \{\infty\} \cup \{-\infty\}$ is not associative, therefore it is
not a vector space.

\section{Subspaces}

\subsection{}
\textit{For each of the following subsets of $F^3$, determine whether it is a
  subspace of $F^3$:}

\textit{(a) $\{(x_1, x_2, x_3) \in F^3: x_1 + 2x_2 + 3x_3 = 0\}$}

Yes, it is. $0$ is contained within it.
$$(x_1, x_2, x_3) + (y_1, y_2, y_3) = (x_1 + y_1, x_2 + y_2, x_3 + y_3)$$
therefore
$$x_1 + y_1 + 2(x_2 + y_2) +  3(x_3 + y_3) =
x_1 + 2x_2 + 3x_3 + y_1 + 2y_2 + 3y_3 = 0+ 0 = 0$$
therefore it is closed under addition
$$n(x_1, x_2, x_3) = (nx_1, nx_2, nx_3)$$
$$nx_1 + 2nx_2 + 3nx_3 = n(x_1 + 2x_2 + 3x_3 ) = 0n = 0$$
therefore it is closed under multiplication.

\textit{(b) $\{(x_1, x_2, x_3) \in F^3: x_1 + 2x_2 + 3x_3 = 4\}$}

It's not a subspace, because it does not contain zero.


\textit{(c) $\{(x_1, x_2, x_3) \in F^3: x_1 x_2 x_3 = 0\}$}

It's not a subspace, because
$$(0, 1, 1) + (1, 0, 0) = (1, 1, 1)$$
therefore it's not closed under addition.

\textit{(d) $\{(x_1, x_2, x_3) \in F^3: x_1  = 5x_3\}$}

It's a subspace, proof is the same as in (a), can be seen more clearly when we
rewrite constraint as
$$x_1 = 5x_3 \to x_1 + 0x_2 -5x_3 = 0$$

\subsection{}
\textit{Verify all the assertions in Example 1.35}

\textit{(a) if $b \in F$, then}
$$\{(x_1, x_2, x_3, x_4) \in F^4: x_3 = 5x_4 + b\}$$
\textit{is a subspace of $F^4$ if and only if $b = 0$}

If $b \neq 0$, then $0$ is not an element of this set.

Proving that it's a subspace when $b = 0$ is trivial

\textit{(b) The set of continous real-valued functions on the interval $[0, 1]$
  is a subspace of $R^{[0, 1]}$.}

$(kf) = kf$ by algebraic properties of continous functions.
If $f$ and $g$ are continous, then $(f + g)$ is continous as well by the same
property.
$f(x) = 0$ is continous because it's a constant functions.

By the way, same (probably) applies to a set of uniformly continous functions.

\textit{(c) The set of differentiable real-valued functions on $R$ is a
  subspace of $R^R$.}

Same deal, algebraic proerties imply linearity, adn zero is included.

\textit{(d) The set of differentiable real-valued functions $f$ on the
  interval $(0, 3)$ such that $f'(2) = b$ is a subspace of $R^{(0, 3)}$
  if and only if $b = 0$.}

Same deal as in previous one, $f'(2)$ needs to be equal to zero in order
to include zero. Previous part does not include it, because it does not
have specific restrictions on derivatives being particular values at particular places.

\textit{(e) The set of all sequences of complex numbers with limit $0$ is a
  subspace of $C^{\infty}$.}

Here we can take zero to be $(x_n) = 0$. Linearity is implied by aldebraic
properties of limits of sequences.

\subsection{}
\textit{Show that the set of differentiable real-valued functions $f$ on the
  interval $(-4, 4)$ such that $f'(1) = 3f(2)$ is a subspace of $R^{[-4, 4]}$.}

Zero is included here. Suppose that $f$ and $g$ are functions in given set.
It follows that
$$f'(1) + g'(1) = 3f(2) + 3g(2)$$
$$f'(1) + g'(1) = 3(f(2) + g(2))$$
$$(f + g)'(1) = 3(f + g)(2)$$
thus it's closed under addition.

$$(kf)'(1) = 3(kf)(2)$$
implies
$$kf'(1) = 3kf(2)$$
therefore it's closed under multiplication by scalar.
Therefore we can state that given subset is a vector subspace.

\subsection{}
analogous to previous

\subsection{}
\textit{Is $R^2$ a subspace of the complex vector space $C^2$?}

No, it's not closed under scalar multiplication.

\subsection{}
\textit{(a) Is }
$$\{(a, b, c) \in R^3: a^3 = b^3\}$$
\textit{a subspace if $R^3$?}

Yes. it is. $a^3 = b^3 \to a = b \to a - b = 0$, the rest of proof is trivial.

\textit{(b) Is }
$$\{(a, b, c) \in C^3: a^3 = b^3\}$$
\textit{a subspace if $C^3$?}

I want to say no to this one, example is
$$(1/2 + i\frac{\sqrt{3}}{2}, -1, 0) +
(1/2 - i\frac{\sqrt{3}}{2}, -1, 0) =
(1, -1, 0)$$
thus it's not closed under additon.

\subsection{}
\textit{Give an example of a nonemplty subset $U$ of $R^2$ such that $U$ is
  closed under addition and under additive inverses (meaning $-u \in U$
  whenever $u \in U$), but $U$ is not a subspace of $R^2$}

$Q^2$. On the other thouhgh, $Z$ will do as well.

\subsection{}
\textit{Give an example of a nonempty subset $U$ of $R^2$ such that $U$ is
  closed under scalar multiplication, but $U$ is not a subspace of $R^2$.}

Two lines through origin.

\subsection{}
\textit{A function is called periodic if there exists a positive number
  $p$ such that $f(x) = f(x + p)$ for all $x \in R$. Is the set of
  periodic functions from $R$ to $R$ a subspace of $R^R$? Explain. }

Zero is a periodic function. Set is
certainly closed under scalar multiplication.

Suppose that $f$ and $g$ are both periodic and $f$ has a period of $p1$
and $g$ has a period of $p2$. Thus if $p2/p1 \in I$,
then functions will be constantly out of phase, therefore the set is not
closed under addition. Thus this subset is not a subspace.

\subsection{}
\textit{Suppose $U_1$ and $U_2$ are subspaces of $V$. Prove that the
  intersection $U_1 \cap U_2$ is a subspace of $V$.}

Zero is included in any subspace, therefore zero is included.

Suppose that $u_1, u_2 \in U_1 \cap U_2$. It follows that for $z \in F$
$zu_1 \in U_1$ and $zu_1 \in U_2$ by closure of those two subspaces.
Therefore $zu_1 \in U_1 \cap U_2$ for any scalar, thus the set is
closed under scalar multiplication.

$u_1 + u_2 \in U_1$ and $u_1 + u_2 \in U_2$ by closure under addition for
both subspaces. Thus $u_1 + u_2 \in U_1 \cap U_2$ for any such vectors.
Therefore the set is closed under addition.

Thus the set satisfies all requirements to be a subspace. Therefore it is a
subspace.

\subsection{}
\textit{Prove that the interseection of every collection of subspace of $V$ is
  a subspace of $V$}

Intersection of two subspaces  is a subspace. Therefore by induction
intersection of any finite collection of subspaces is a subspace.

Suppose that $\Lambda$ is an arbitrary collection of subspaces.
Every subspace contains a zero element, therefore
$$0 \in \cap \Lambda$$

Any vector in $\cap \Lambda$ will be closed under scalar multiplication
for every $U \in \Lambda$. Thus, it will be contained in every
$U \in \Lambda$. Therefore it is contained in $\cap \Lambda$.


Any two  vectors in $\cap \Lambda$ will be closed under addition,
for every $U \in \Lambda$. Thus, their sum  will be contained in every
$U \in \Lambda$. Therefore it is contained in $\cap \Lambda$.

Thus $\cap \Lambda$ is a vector space.

\subsection{}
\textit{Prove that the union of two subspaces of $V$ is a subspace of $V$
  if and only if one of the subspaces is contained in the other.}

Suppose that a union of two subspaces $U_1 \cup U_2$
is a subspace of $V$.

Zero is included in eveery subspace, so in case of the union we don't worry
about it.
Scalar multiplication is also trivial, as we are working only with one vector.

Now for the interesting part: addition. Let $u_1, u_2 \in U_1 \cup U_2$.
In case when $u_1, u_2$ are contained only in one subspace we've got
a trivial case. Interesting part comes when $u_1 \in U_1$ and $u_2 \in U_2$.

What we want to prove is that it is impossible to have
$u_1 \in U_1 \setminus U_2$ and $u_2 \in U_2 \setminus U_1$ and we're going to
use contradiction. Suppose that
$u_1 \in U_1 \setminus U_2$, $u_2 \in U_2 \setminus U_1$ and
$u_1 + u_2 \in U_1 \cup U_2$. Thus it must be the case that
$u_1 + u_2 \in U_1$ or $u_1 + u_2 \in U_2$. Suppose that the former is true;
then it follows that $u_1 + u_2 - u_1 = u_2 \in U_1$, which is a contradiction
(same thing happens if we assume the latter). Thus given case is impossible.
Therefore there cannot exist $u_1 \in U_1 \setminus U_2$ and
$u_2 \in U_2 \setminus U_1$. Thus
$U_1 = U_1 \cup U_2$ or $U_2 = U_1 \cup U_2$.

The reverse case is trivial: if we have two subspaces and
one of it is a subset of another, then larger subspace is is subspace.

\subsection{}
\textit{Prove that the union of three subspaces of $V$ is a subspace of
  $V$ if and only if one of the subspaces contains the other two.}

Same thing applies as in previous exercise: zero and multiplication are
trivial.

We are going to proceed with a proof by contradiction, but firstly we want
to state precisely what we want to prove in a first place. We want to state,
that if a union of three subspaces is a subspace, then this union is equal to
one of the subpsaces. So let us start: suppose that the union of three subspaces is not equal to one of the subspaces.

Firstly, we can eliminate the case, when one of the subspaces is a
subset of another subspace, but third isn't, because it will mean that
union of first two subspaces constitues a subspace, and thus we'll default
to result in the previous exercise.

Thus let us assume that none of the subspaces is a subset of another subspace.
Now we've got two cases to sort out: suppose that if we take $u_2 \in U_2$
and $u_3 \in U_3$ we get that
$$u_2 + u_3 \in U_1$$
for every $u_2 \in U_2$ and $u_3 \in U_3$. Then we can follow, by setting
$u_2 = 0$ to the case that
$$\forall u_3 \in U_3 \to u_3 + u_2 \in U_1 \to u_3 + 0 \in U_1 \to
u_3 \in U_1$$
thus $U_3$ is a subset of $U_1$, which raises a contradiction (in our
assumptions that $U_3$ is not a subset of $U_1$  and by extension
for the default 2-subspace case).

The case when $u_2 \in U_2$, $u_3 \in U_3$ and $u_2 + u_3 \notin
U_1 \cup U_2 \cup U_3$ implies that $U_1 \cup U_2 \cup U_3$ is
not a vector space, thus it cannot happen.

The case when $u_2 \in U_2$, $u_3 \in U_3$ and $u_2 + u_3 \notin U_1$
implies that $u_2 + u_3$ is  in $U_2 \cup U_3$. This raises the case that
$U_2$ is a subspace of $U_3$, which is a contradiction.

Thus we can follow that there exists $u_1 \in U_1$ such that it
cannot be represented in terms of vectors from $U_2$ and $U_3$.
Thus we can follow that analogous  vectors   $u_2 \in U_2$ and $u_3 \in U_3$
also exist.

Because we are still assuming that $U_1 \cup U_2 \cup U_3$ we can follow that
$$u_1 + u_2 + u_3 \in U_1 \cup U_2 \cup U_3$$
Thus this sum is bound to be located in one of the $U_1$, $U_2$ or $U_3$.
Let us assume for simplicity of notation that it is located in $U_1$. Then
we can follow that
$$u_1 + u_2 + u_3 - u_1 = u_2 + u_3 \in U_1$$

Suppose that we take $u_2 \in U_2 \setminus (U_3 \cup U_1)$ and
$u_3 \in U_3 \setminus (U_1 \cup U_2)$. It follows that
$u_2 + u_3$ cannot be in either $U_2$ nor in $U_3$ because in this
case we have that
$$u_2 + u_3 - u_2 = u_3 \in U_2$$
which is a contradiction. Thus
$$u_2 + u_3 \in U_1 \setminus (U_2 \cup U_3)$$
let us call it $u_1'$. In the same fashion we can define $u_2'$ and $u_3'$.

Thus $u_1' + u_2' + u_3' \in U_1 \cup U_2 \cup U_3$. Thus it needs to
be in one of $U_1$, $U_2$ or $U_3$. Suppose that it is included in
$U_1$. Then we can follow that
$$u_1' + u_2' + u_3' \in U_1$$
$$u_2' + u_3' \in U_1$$
$$u_1 + u_3 + u_1 + u_2 \in U_1$$
$$2u_1 + u_3  + u_2 \in U_1$$
$$u_3  + u_2 \in U_1$$

TODO

\subsection{}
\textit{Verify the assertion in Example 1.38}

1.38 states that

\textit{Suppose that $U = \{(x, x, y, y) \in F^4: x, y \in F\}$ and
  $W = \{(x, x, x, y) \in F^4: x, y \in F\}$. Then }
$$U + W = \{(x, x, y, z) \in F^4: x, y, z \in F\}$$
\textit{as you should verify}

Let $u \in U$ and $w \in W$. It follows that
$$u = (x_1, x_1, x_1, y_1)$$
$$w = (x_2, x_2, y_2, y_2)$$

Suppose that $q \in U + W$.
It follows that
$$q = (x_1 + x_2, x_1 + x_2, x_1 + y_2, y_1 + y_2)$$
thus we can set $x = x_1 + x_2$, $y = x_1 + y_2$ and $z = y_1 + y_2$
and call it a day.

\subsection{}
\textit{Suppose $U$ is a subspace of $V$. What is $U + U$.}

By properties of vector space, if we take $u_1, u_2 \in U$ then
$$u_1 + u_2 \in U$$
for every $u_1, u_2 \in U$. Thus we can follow that
$$U + U = U$$

\subsection{}
\textit{Is the operation of addition on the subspaces of $V$ commutative? In
  other words, if $U$ and $W$ are subspaces of $V$, is $U + W = W + U$?}

If $q \in U + W$ it follows that there exists $u \in U$ and $w \i W$
such that
$$q = v + w = w + v = q'$$
where $q' \in W + U$. Thus we can follow that $W + U = U + W$.

\subsection{}

\textit{Is the operation of addition on the subspaces of $V$ associative? In
  other words, if $U_1$, $U_2$, $U_3$ are subspaces of $V$, is}
$$(U_1 + U_2) + U_3 = U_1 + (U_2 + U_3)?$$

Yes it is. We can apply the same logic as in the previous exercise and it'll do
the job.

\subsection{}

\textit{Does the operation of addition on the subspaces of $V$ have an additive
  identity? Which subspace have additive inverces?}

Every subspace contains zero, therefore
$$U + 0 = U$$
thus we've got additive identity.

By adding two subspaces together we get a larger subspace, thus we can follow
that the only way to get 0 vector space as the result of addition of two
subspaces is to add
$$0 + 0 = 0$$
thus the only subspace that contains additive inverse is $0$.

\textit{Prove or give counterexample: if $U_1$, $U_2$, $W$ are subspaces of
  $V$, such that }
$$U_1 + W = U_2 + W$$
\textit{then $U_1 = U_2$}

This is wrong: suppose that $U_2$ is a nonzero subspace of $W$ and $U_1 = 0$.
Then it follows that
$$U_1 + W = 0 + W = W = W + U_2$$
and
$$U_1 \neq U_2$$
as desired.

\subsection{}

\textit{Suppose}
$$U = \{(x, x, y, y) \in F^4: x, y \in F\}$$
\textit{Find a subspace $W$ of $F^4$ such that $F^4 = U \bigoplus W$ }

$$W = \{(0, x, y, 0) \in F^4: x, y \in F\}$$


\subsection{}

\textit{Suppose}
$$U = \{(x, y, x + y, x - y, 2x) \in F^5: x, y \in F\}$$
\textit{Find a subspace $W$ of $F^5$ such that $F^5 = U \oplus W$ }

$$W = \{(0, 0, x, y, z) \in F^5: x, y, z \in F\}$$

\subsection{}

\textit{Suppose}
$$U = \{(x, y, x + y, x - y, 2x) \in F^5: x, y \in F\}$$
\textit{Find a thee subspaces $W_1$, $W_2$, $W_3$ of $F^5$
  such that $F^5 = U \oplus W_1 \oplus W_2 \oplus W_3$ }

$$W_1 = \{(0, 0, x, 0, 0) \in F^5: x \in F\}$$
$$W_2 = \{(0, 0, 0, y, 0) \in F^5: y \in F\}$$
$$W_3 = \{(0, 0, 0, 0, z) \in F^5: z \in F\}$$

\subsection{}

\textit{Prove or give a counterexample: if $U_1$, $U_2$, $W$ are subspaces
  of $V$ such that }
$$ V = U_1 \oplus W \text{ and } V = U_2 \oplus W $$
\textit{then $U_1 = U_2$}

This one is false;
$$U_1 = \{(x, x) \in F^2: x \in F\}$$
$$U_2 = \{(x, 0) \in F^2: x \in F\}$$
$$W = \{(0, y) \in F^2: y \in F\}$$

\subsection{}

\textit{A function $f: R \to R$ is called even if }
$$f(-x) = f(x)$$
\textit{for all $x \in R$. A function $f: R \to R$ is called odd if }
$$f(-x) = -f(x)$$
\textit{for all $x \in R$. Let $U_e$ denote the set of real-valued
  even functions on $R$ and let $U_o$ denote the set of real-valued odd
  functions on $R$. Show that }
$$R^R = U_e \oplus U_o$$



Let $f: R \to R$ be arbitrary. It follows that

$$f_e(x) =
\begin{cases}
  2 f(x) - f(-x) \text{ if } x \geq 0 \\
  f(x) \text{ if } x = 0 \\
  2 f(-x) - f(x) \text{ if } x < 0
\end{cases}
$$

Every odd function satisfies $f(0) = 0$.
Therefore for even function we've got to have $f_e(0) = f(0)$


$$
f_e(x) =
\begin{cases}
  a_1 f(x) + b_1 f(-x) \text{ if } x > 0 \\
  a_1 f(-x) + b_1 f(x) \text{ if } x < 0
\end{cases}
$$

$$
f_o(x) =
\begin{cases}
  a_2 f(x) + b_2 f(-x) \text{ if } x > 0 \\
  -a_2 f(-x) - b_2 f(x)  \text{ if } x < 0
\end{cases}
$$

$$
\begin{cases}
  a_1 + a_2 = 1 \\
  b_1 + b_2 = 0 \\
  a_1 - a_2 = 0 \\
  b_1 - b_2 = 1 \\
\end{cases}
$$

$$
\begin{cases}
  a_1  = 0.5 \\
  b_1 = 0.5 \\
\end{cases}
$$

$$
f_e(x) =
\begin{cases}
  1/2 f(x) + 1/2 f(-x) \text{ if } x > 0 \\
  f(x) \text{ if } x = 0 \\
  1/2 f(x) + 1/2 f(-x)  \text{ if } x < 0
\end{cases}
$$

$$
f_o(x) =
\begin{cases}
  1/2 f(x) - 1/2 f(-x) \text{ if } x > 0 \\
  0 \text{ if } x = 0 \\
  -1/2 f(-x) + 1/2 f(x)  \text{ if } x < 0
\end{cases}
$$

Thus
$$f_e(x) = f_e(-x)$$
$$f_o(-x) = -f_o(x)$$
and
$$f_e(x) + f_o(x) = f(x)$$
as desired.

Also, the only function that is odd and even at the same time is $0$,
therefore we've got a direct sum, as desired.


\chapter{Finite-Dimentional Vector Spaces}

\section{Span and Linear Independence}

\subsection{}
\textit{Suppose $v_1, v_2, v_3, v_4$ spans $V$. Prove that the list }
$$v_1 - v_2, v_2 - v_3, v_3 - v_4, v4$$
\textit{also spans $V$.}

Let $v \in V$ be represented as
$$v = a_1 v_1 + a_2 v_2 + a_3 v_3 + a_4 v_4$$

then we can follow that
$$v = a_1 (v_1 - v_2) + (a_2 + a_1) (v_2 - v_3)+ (a_3 + a_2 + a_1) (v_3 - v_4) + (a_1 + a_2 + a_3 + a_4) v_4$$
therefore any $v \in V$ can be represented using given list, therefore
given list spans $V$, as desired.

\subsection{}
\textit{Verify the assertion in Example 2.18}

Suppose that $v \in V$. Then it follows from some exercise in previous
chapter that $a_1 v = 0$ iff $a_1 = 0$ or $v = 0$. Thus if $v \neq 0$ we
can follow that the only way to represent zero is to set $a_1$ to 0. Thus
list is linearly independent.


Suppose that we've got linearly independent list of two vectors. We therefore
can follow that the only way to represent 0 is to set
$a_1 = 0$ and $a_2 = 0$. Thus vectors are not a scalar multiples of each other.
In other directon we've got a trivial case.

For the list
$$v_1 = (1, 0, 0, 0), v_2 = (0, 1, 0, 0), v_3 = (0, 0 1, 0)$$
we've got that
$$v = a_1 v_1 + a_2 v_2 + a_3 v_3 = (a_1, a_2, a_3, 0)$$
therefore the only way to represent zero is to set all of a's into 0.

Same case applies for the last one.

\subsection{}
\textit{Find a number $t$ such that}
$$(3, 1, 4), (2, -3, 5), (5, 9, t)$$
\textit{is not linearly inependent in $R^3$}

The only way that this list is not linearly independent is if 
we can represent last vector as a linear combiination of the other two. Thus
$$
\begin{cases}
  3 a_1 + 2 a_2 = 5
  1 a_1 - 3 a_2 = 9
\end{cases}
$$
$$
\begin{cases}
  3 a_1 + 2 a_2 = 5
  a_1= 9 +  3 a_2
\end{cases}
$$
$$   3 (9 +  3 a_3) + 2 a_2 = 5 $$
$$   27 +  9 a_2 + 2 a_2 = 5 $$
$$ 11 a_2= -22$$
$$ a_2 = -2$$
thus
$$a_1 = 3$$
therefore
$$3 * 4 - 5 * 2 = t$$
$$t = 2$$

\subsection{}
\textit{Verify the assertion in the second bullet point in Example 2.20}

$c = 8$ is the only solution such that third vector is a
scalar multipe of first vector plus scalal multiple of second. Thus we
can follow that the last vector is not in the span of first two, therefore
the list is linearly independent.

\subsection{}
\textit{(a) Show that if we think of $C$ as a vector space over $R$, then the
  list $(1 + i, 1 - i)$ is linearly independent.}

$$(1 + i  + 1 - i)/2 = 1$$
$$(1 + i - 1 + i)/2 = i$$
thus the only way to represent $0$ is to set all of a's to zero

\textit{(b) Show that if we think of $C$ as a vector space over $C$, then
  the list $(1 + i, 1 - i)$ is linearly dependent}

List $(1)$ spans $C$, and its length is less that
the length of given set. THus given set is linearly dependent.


\subsection{}
\textit{Suppose $v_1, v_2, v_3, v_4$ is linearly independent.
  Prove that the list }
$$v_1 - v_2, v_2 - v_3, v_3 - v_4, v4$$
\textit{is also linearly independent.}

As we've shown before, spans of two sets are equal, therefore the only
way to represent $0$ is to put all a's to 0.



\subsection{}
\textit{Prove or give counterexample: If $v_1, v_2, ... v_m$ is a linearly
  independent list of vectors in $V$, then}
$$5v_1 - 4v_2, v_2, v_3, ... v_m$$
\textit{is linearly independent}

Both sets span the same space and have the same length, therefore they are
both linearly independent.

\subsection{}

Trivial, equivalent to previous

\subsection{}
\textit{Prove or give counterexample: If $v_1, ... v_m$ and $w_1, ..., w_m$ are
  linearly independent lists of vectors in $V$, then
  $v_1 + w_1, ..., v_m + w_m$ is linearly independent.}

False: set $w_1 = - v_1$ and get the desired result.

\subsection{}
\textit{Suppose $v_1, ..., v_m$ is linearly independent in $V$ and $w \in V$.
  Prove that if $v_1 + w, v_2 + w, ... v_m + w$ is linearly dependent,
  then $w \in span(v_1, v_2, ... v_m)$.}

Suppose that resulting list is linearly dependent. It follows that there
exists a way to represent 
$$\sum_{n = 1}^m{a_n (v_n + w)} = 0$$
such that not all a's are zeroes. Thus
$$\sum_{n = 1}^m{a_n (v_1 + w)} = \sum_{n = 1}^m{(a_n w + a_n v_n)} =
\sum_{n = 1}^m{a_n w} + \sum_{n = 1}^m{a_n v_n} =
w \sum_{n = 1}^m{a_n} + \sum_{n = 1}^m{a_n v_n} = 0
$$
$$- w \sum_{n = 1}^m{a_n} =  \sum_{n = 1}^m{a_n v_n}$$
$\sum_{n = 1}^m{a_n} \neq 0$, because otherwise left side is zero and
therefore right side is zero, which is not assumed.
$$w  =  \sum_{n = 1}^m{ - \frac{a_n}{\sum_{j = 1}^m{a_j}} v_n}$$
thus $w \in span(v_1, v_2, ... v_m)$, as desired.


\subsection{}
\textit{Suppose $v_1, ..., v_m$ is linearly independent in $V$ and $w \in V$.
  Show that $v_1, ..., v_m, w$ is linearly independent if and only if }
$$w \notin span(v_1, ..., v_m)$$

Because otherwise we've got a bigger linearly independent list, that spans
$V$.

\subsection{}
\textit{Explain why there does not exist a list of six polinomials that is
  linearly independent of $\mathcal{P_4}(F)$.}

Because the list of length 5 spans this space.

\subsection{}
\textit{Explain why no list of four polynomials spans $\mathcal{P_4}(F)$.}

Because the list of length 5 spans this space.

\subsection{}
\textit{Prove that $V$ is infinite-dimentional if and only if there is a
  sequence $v_1, v_2, ... $ of vectors in $V$ such that $v_1, ... v_m$ is
  linearly independent for every possible integer $m$.}

Forward is coming from the fact that we can always add new vectors
to a given linearly independent list of vectors, that are outside of span
of given list.

Because there always exists list that is bigger than
given list and is linearly independent in $V$ we can follow that
no final list of vectors spans $V$, therefore it is infinite-dimentional.


\subsection{}
\textit{Prove that $F^{\infty}$ is infinite-dimentional.}

Infinite list
$$(1, 0, ....), (0, 1, 0, ...), .... $$
is all linearly indepenent, therefore no finite set spans the space.


\subsection{}
\textit{PRove that the real vector space of all continous real-valued
  functions on the interval $[0, 1]$ is infinite-dimentional.}

We can create a countable sequence $(r_1, r_2, ... )$ of rationals in this
space, and correspod each one of them with some number, thus creating a
infinite linearly inedependent list.

\subsection{}

\textit{Suppose $p_0, p_1, ... p_n$ are
  polynomials in $\mathcal{P_m}(F)$ such that
  $p_j(2) = 0$ for each $j$. Prove tat $p_0, p_1, ... p_m$ is not linearly
  independent in $\mathcal{P_m}(F)$.}

Because it has the same length as $1, x, x^2 ... $, but doesn't span the same
space.

\section{Bases}

There are no challenging exercises in this section, just a recap of
the material. Looked them over, brushed up the material, not gonna waste
my time writing them down.

\section{Dimention}

\subsection{}
\textit{Suppose $V$ is finite-dimentional and $U$ is a subspace of $V$ such
  that $\dim U = \dim V$. Prove that $U = V$}

They have the same length of basis, thus basis of $U$ is a basis of $V$.

\subsection{}
\textit{Show that the subspaces of $R^2$ are precisely $\{0\}, R^2$ and all
  lines through the origin}

For 0 dimention we've got null

For dimention 1 we've got scalar multiple of any vector, which are lines
through the origin

For dimention 2 we've got the space itself

\subsection{}
\textit{Show that the subspaces of $R^3$ are precisely $\{0\}, R^3$, all
  lines through the origin, and all planes through the origin}

Same idea as in previos exericise, but list of length 2 defines a plane
through the origin and 3 defined space itself

\subsection{}
\textit{(a) Let $U = \{p \in P_4(F): p(6) = 0$. Find a basis of $U$.}

$$(x - 6), (x - 6)^2, (x - 6)^3, (x - 6)^4$$

\textit{(b) Extend the basis in part (a) to a basis of $P_4(F)$}

$$1, (x - 6), (x - 6)^2, (x - 6)^3, (x - 6)^4$$

\textit{Find a subspace $W$ of $P_4(F)$ such that $P_4(F) = U \oplus W$}

$$\{c: c \in F\}$$

\subsection{}
\textit{(a) Let $U = \{p \in P_4(F): p''(6) = 0$. Find a basis of $U$.}

$$1, (x - 6), (x - 6)^3, (x - 6)^4$$

\textit{(b) Extend the basis in part (a) to a basis of $P_4(F)$}

$$1, (x - 6), (x - 6)^2, (x - 6)^3, (x - 6)^4$$

\textit{Find a subspace $W$ of $P_4(F)$ such that $P_4(F) = U \oplus W$}

$$ (x - 6)^2$$


\subsection{}
\textit{(a) Let $U = \{p \in P_4(F): p(2) = p(5)$. Find a basis of $U$.}

$$1, (x - 2)(x - 5), (x - 2)^2(x - 5), (x - 2)^2(x - 5)^2$$

\textit{(b) Extend the basis in part (a) to a basis of $P_4(F)$}

$$1, x, (x - 2)(x - 5), (x - 2)^2(x - 5), (x - 2)^2(x - 5)^2$$

\textit{Find a subspace $W$ of $P_4(F)$ such that $P_4(F) = U \oplus W$}

$$ x $$

\subsection{}
\textit{(a) Let $U = \{p \in P_4(F): p(2) = p(5) = p(6)$. Find a basis of $U$.}

$$1, (x - 2)(x - 5)(x - 6), (x - 2)^2(x - 5)(x - 6)$$

\textit{(b) Extend the basis in part (a) to a basis of $P_4(F)$}

$$1, x, x^2,  (x - 2)(x - 5)(x - 6), (x - 2)^2(x - 5)(x - 6)$$

\textit{Find a subspace $W$ of $P_4(F)$ such that $P_4(F) = U \oplus W$}

$$x, x^2$$

\subsection{}
\textit{(a) Let $U = \{p \in P_4(F): \int_-1^1{p} = 0 \}$.
  Find a basis of $U$.}

$$x, x^3$$

\textit{(b) Extend the basis in part (a) to a basis of $P_4(F)$}

$$1, x, x^2, x^3, x^4$$

\textit{Find a subspace $W$ of $P_4(F)$ such that $P_4(F) = U \oplus W$}

$$1, x^2, x^4$$

\subsection{}
\textit{Suppose $v_1, ... v_m$ is linearly independent in $V$ and $w \in V$.
  Prove that}

$$\dim span(v_1 + w, ..., v_m + w) \geq m - 1$$

Because $v_1, ... v_m$ is linearly independent we can follow that $w$ is either
in $span(v_1, ..., v_m)$ or not. In the latter case we've got that the
case that we increase the span. In the former we've got by linear independence
of $v_1, ... v_m$ that the maximum decline of degree is $1$. Thus
$$\dim span(v_1 + w, ..., v_m + w) \geq m - 1$$
as desired.

\subsection{}

\textit{Suppose $p_0, p_1, ..., p_m \in P(F)$ are such taht each $p_j$ has
  degree $j$. Prove that $p_0, ... p_m$ is a basis of $P_m(F)$. }

Suppose that $p \in P_m(f)$. Because each $p_n$ has a degree of $n$ we can
follow that there exists only 1 $a_m \in F$ such that 
of $p_m$ such that
$$p - a_m p_m \in P_{m - 1}(F)$$.

Bu applying the same procedure  again repeatedly  we get unique
$a_m, ..., a_0$ such that
$$\sum{a_n p_m} = p$$
for every $p \in P_m(f)$. Thus we can follow that given list spans $P_m(F)$
and by unique representation we get that this list is linearly independent.
Thus we can follow that given list is a basis of $P_m(F)$, as desired.


\subsection{}
\textit{Suppose that $U$ and $W$ are subspaces of $R^8$ such that $\dim U = 3$,
  $\dim W = 5$, and $U + W = R^8$. Prove that $R^8 = U \oplus W$.}

We know that
$$\dim (U_1 + U_2) = \dim U_1 + \dim U_2 - \dim (U_1 \cap U_2)$$

Thus we can follow that in this particular case
$$\dim (R^8) = \dim U + \dim W - \dim (U \cap W)$$
$$8 = 3 + 5 - \dim (U \cap W)$$
$$\dim (U \cap W) = 0$$
thus we can follow that $U \cap W = \{0\}$. Therefore
$$U + W = U \oplus W = R^8$$
as desired.

\subsection{}
\textit{Suppose that $U$ and $W$ are both five-dimentional subspaces of $R^9$.
  Prove that $U \cap W \neq \{0\}$}

Once again we get that
$$\dim R^9 = \dim U + \dim W - \dim (U \cap W)$$
$$9 = 5 + 5- \dim (U \cap W)$$
$$\dim (U \cap W) = 1$$
thus
$$U \cap W \neq 0$$
as desired.

\subsection{}
\textit{Suppose $U$ and $W$ are both 4-dimentional subspaces of $C^6$. Prove
  that there exists two vectors in $U \cap W$ such that neither of these
  vectors is a scalar multiple of the other}

Goto previous exercise for concretee explanation if needed, but  we can 
conclude that
$$\dim U \cap W = 2$$
thus there exists a linearly independent list of length 2 in $U \cap W$ (basis)
so that neither of them is a scalar multiple of another by some exercise in 2.A

\subsection{}
\textit{Suppose $U_1, ... U_m$ are finite-dimentional subspaces of $V$.
  Prove that $U_1 + ... + U_m$ is finite-dimentional and }
$$\dim(\sum U_n) \leq \sum{\dim U_n}$$

We know that 
$$\dim (U_1 + U_2) = \dim U_1 + \dim U_2 - \dim (U_1 \cap U_2)$$
given that $\dim W \geq 0$ for any vector space $W$ we follow that
$$\dim (U_1 + U_2) \leq \dim U_1 + \dim U_2$$
Thus by induction
$$\dim(\sum U_n) \leq  \sum \dim U_n $$
which in presented case get us desired result.

\subsection{}
\textit{Suppose $V$ is finite-dimentional, with $\dim V = n \geq 1$. Prove
  that there exist 1-dimentional subspaces $U_1, ... U_n$ of $V$ such that  
}
$$V = U_1 \oplus ... \oplus U_n$$

For $V$ there exists a basis of length $n$. Thus by setting
$$U_j = \{c v_j: c \in F\}$$
we get desired result.

\subsection{}
\textit{Suppose $U_1, ..., U_m$ are finite-dimentional subspaces of $V$ such
  that $U_1 + .. + U_m$ is a direct sum. Prove that $U_1 + ... + U_m$ is
  finite dimentional and that}
$$\dim \sum U_n = \sum \dim U_n$$

We can just go by induction on the case that
$$\dim (U \oplus W) = \dim U + \dim W + \dim (U \cap W) =
\dim U + \dim W + 0$$
Or we can use the fact, that we can combine all bases
of subspaces together in one
mega-basis for their sum. Both will suffice.

\subsection{}
\textit{You might guess, by analogy with the formula for the number of elements
  in the union of three subsets of a finite set, that if $U_1, U_2, U_3$ are
  subspaces of finite-dimentional vector space, then}
$$\dim (U_1 + U_2 + U_3) =
\dim U_1 + \dim U_2 + \dim U_3 - \dim (U_1 \cap U_2) - \dim (U_1 \cap U_3) - $$
$$
- \dim (U_2 \cap U_3)
+ \dim (U_1 \cap U_2 \cap U_3)$$

We know that 
$$\dim (U_1 + U_2) = \dim U_1 + \dim U_2 - \dim (U_1 \cap U_2)$$
and
$$U_1 + U_2 + U_3 = (U_1 + U_2) + U_3$$
thus
$$\dim (U_1 + U_2 + U_3) = \dim ((U_1 + U_2) + U_3) =
\dim (U_1 + U_2) + \dim U_3 - \dim ((U_1 + U_2) \cap U_3) =$$
$$ =
\dim U_1 + \dim U_2 - \dim U_1 \cap U_2 + \dim U_3 -
\dim ((U_1 + U_2) \cap U_3) = $$
here we get a little problem because we don't know how to reduce
$(U_1 + U_2) \cap U_3$ to some managable pieces. After this discovery
one might even glance over
the equation once again in order to try to disprove the theorem.
And indeed we've found a counterexample: suppose that $U_1, U_2, U_3$ are
lines through the origin in $R^3$ such that they are located on the same
plane. Then it follows that left-hand side becomes 2, and the right side is
equal to 3. Thus we've got a contradiction (which is a shame, because
the formula looks nice :( ).

\chapter{Linear maps}

\section{The Vector Space of Linear Maps}

\subsection{}
\textit{Suppose $b, c \in R$. Define $T: R^3 \to R^2$ by }
$$T(x, y, z) = (2x - 4y + 3z + b, 6x + cxyz)$$
\textit{Show that $T$ is linear if and only of $b = c = 0$.}

Suppose that $T$ is linear. Then it follows that
$$T(0) = 0 = (0 + b, 0)$$
thus we can follow that $b = 0$.

Also,
$$T((1, 1, 1) + (2, 2, 2)) = (6 - 12 + 9, 18 + 27c) = (3, 18 + 27c) =
$$
$$
= T((1, 1, 1)) + T(2, 2, 2) = (2 - 4 + 3, 6 + c) + (4 - 8 + 6, 12 + 8c) =
(1, 6 + c) + (2, 12 + 8c) = (3, 18 + 9c)$$

Thus
$$27c = 9c$$
$$3c = c$$
$$c = 0$$
as desired.

Reverse implication is trivial, thus we get the desired result.

\subsection{}
\textit{Suppose $b, c \in R$. Define $T: \mathcal(P)(R) \to R^2$ by}
$$Tp = \left(3p(4) + 5p'(6) + bp(1)p(2), \int_{-1}^2{x^3 p(x) dx} + c \sin{p(0)}\right)$$
\textit{Show that $T$ is linear if and only if $b = c = 0$.}

Suppose that $T$ is linear. Then it follows that if $p(0) = \pi/2$, then latter
part of resulting vector has additive property only when $c = 0$. For the former
we've got result that
$$\lambda^2 b = b$$
for all $\lambda \in R$, which happens only if $b = 0$. Thus $b = c = 0$.

Reverse implication is trivial, thus we have the desired result.

\subsection{}
\textit{Suppose $T \in  \mathcal{L}(F^n, F^m)$. Show that there exists scalars
  $A_{j, k} \in F$ for $j = 1, ..., m$ and $K = 1, ..., n$ such that}
$$T(x_1, ..., x_n) = (A_{1, 1}x_1 + ... + A_{1, n} x_n, ..., A_{m, 1} x_1 + ... + A_{m, n} x_n)$$
\textit{for every $(x_1, ..., x_n) \in F^n$.}

Because $(1, 0, ...), (0, 1, ...), ... $ is a basis of $F^n$ we can follow that
there vector in $F^m$, such that $T(v) \in F^m$. Thus let us denote 
$$T(1, 0, ...) = (A_{1, 1}, A_{2, 1}, ..., A_{m, 1})$$
$$T(0, 1, ...) = (A_{1, 2}, A_{2, 2}, ..., A_{m, 2})$$
$$...$$
Thus given given arbitrary vector $v = (x_1, x_2, ..., x_n)\in T^n$ we get that 
$$T(v) = T(x_1, x_2, ...) = T(x_1, 0, 0, ...) + T(0, x_2, 0, ...) + ... =
x_1T(1, 0, 0, ...) + x_2 T(0, 1, 0, ...) + ... = $$
$$ = (x_1 A_{1_1}, x_1 A_{2, 1}, ... ) +
(x_2 A_{1_2}, x_2 A_{2, 2}, ... ) = (x_1 A_{1, 1} + x_2 A_{1, 2} + ..., x_1 A_{2, 1} + x_2 A_{2, 2} + ... )$$
as desired.

\subsection{}
\textit{Suppose $T \in \mathcal{L}(V, W)$ and $v_1, v_2, ... v_m$ is a list of vectors in $V$
  such that $T v_1, ...,, T v_m$ is a linearly inndependent list in $W$. Prove that
$v_1, v_2, ..., v_m$ is linearly independent.}

Suppose that it isn't. Then we can follow that there exist $w_1 \in W$ such that
$$w_1 = \sum a_j v_j = 0$$
and not all of $a_j$'s are zeroes. Thus we can follow that
$$T(w) = T(\sum a_j v_j) = \sum T (a_j v_j) = \sum a_j T (v_j) = 0$$
But $T(v_j)$ is a list of linearly independent vectors, and therefore their sum is
equal to zero iff all $a_j$'s are zeroes, which is false. Thus we've got a contradiction.

\subsection{}
\textit{Prove the assertion in 3.7}

Let $T_1 = T, T_2 = S, T_3 \in L(V, W)$. Then it follows that

(1) $$(T_1 + T_2)(v) = T_1(v) + T_2(v) = T_2(v) + T_1(v) = (T_2 + T_1)(v)$$

(2) $$(T_1 + (T_2 + T_3))(v) = T_1(v) + (T_2 + T_3)(v) = T_1(v) + T_2(v) + T_3(v) =$$
$$ = (T_1 + T_2)(v) + T_3(v) = ((T_1 + T_2) + T_3)(v)$$

(3) $$\lambda((S + T)(v)) = \lambda ( S(v) + T(v)) = \lambda S(v) + \lambda T(v)
= (\lambda S + \lambda T)(v)$$

(4) $$T + 0 = T$$

(5) $$1T = T$$

(6) $$T + -1T = (1 - 1)T = 0T = 0$$

Thus $L(V, W)$ satisfies all reqirements of a vector space, as desired.

\subsection{}
\textit{Prove the assertion in 3.9}

Let $v \in V$.

(1) Then it follows that
$$((T_1 T_2)T_3)(v) = (T_1 T_2)( T_3(v)) =  T_1 (T_2( T_3(v))) = T_1 ((T_2 T_3)(v))
= (T_1(T_2 T_3))(v)$$
directly from definition. (I wonder if it's  true in general for all functions; it probably
is).

(2)
$$ TI v = T(I(v)) = T(v) = I (T(v))$$

(3)

$$(S_1 + S_2)T(v) = (S_1 + S_2)(T(v)) = S_1(T(v)) + S_2(T(v)) = S_1 T v + S_2 T v$$
$$S(T_1 + T_2)(v) = S((T_1 + T_2)(v)) = S(T_1(v) + T_2(v)) = S(T_1(v)) + S(T_2(v)) = S T_1 v +
S T_2 v$$

as desired.

\subsection{}
\textit{Show that every linear map from a 1-dimentional vector space to itself is
  multiplication by some scalar. More precisely, prove that if $\dim V = 1$ and
  $T \in L(V, V)$, then there exists $\lambda \in F$ such that $Tv = \lambda v$ for
  all $v \in V$.}

Because we've got a 1-dimentional space, it follows that there exists a basis of $V$ - $v_1$.
For this vector we've got that
$$T v_1 = v_2 = \lambda v_1$$
Thus we can follow that if $u \in V$ then
$$T u = T \sigma v_1 =  \sigma T v_1 = \sigma \lambda v_1 = \lambda \sigma v_1 = \lambda u$$
as desired.

\subsection{}
\textit{Give an example of a function $\phi: R^2 \to R$ such that }
$$\phi (av) = a\phi(v)$$
\textit{for all $a \in R$ and all $v \in R^2$ but $\phi$ is not linear.}

$$\phi(x, y) =
\begin{cases}
  x \text{ if } x \neq y \\
  0 \text{ otherwise}
\end{cases}
$$

\subsection{}
\textit{Give an example of a function $\phi: C \to C$ such that }
$$\phi (w + z) = \phi(w) + \phi(z)$$
\textit{for all $w, z \in C$ but $\phi$ is not linear.}

Let us define 
$$\phi(a + bi) = b + ai$$
Thus 
$$\phi(a + bi + c + di) = ai + ci + b + d = \phi(a + bi) + \phi(c + di)$$
but
$$i \phi(a + bi) = -a + bi$$
$$\phi(i(a + bi)) = \phi(ai - b) = -bi + a \neq i \phi(a + bi)$$

\subsection{}
\textit{Suppose $U$ is a subspace of $V$ with $U \neq V$. Suppose $S \in L(V, W)$ and
  $S \neq 0$. Define $T: V \to W$ by}
$$Tv =
\begin{cases}
  Sv \text{ if } v \in U \\
  0 \text{ if } v \in V \text{ and } v \notin U
\end{cases}
$$
\textit{Prove that $T$ is not a linear map on $V$.}

Let $u \neq 0 \in U$ such that $Su \neq 0$ and $v \in V \setminus U$. Then it follows that
$$v + u \notin U$$
(because otherwise $-(v + u)$ is in $U$, therefore $u - (v + u) = -v \in U$ and
thus $v \in U$, which is a contradiction)
Thus we can follow that
$$T(v + u) = 0$$
but
$$T(v) + T(u) = 0 + Su = Su \neq 0 = T(v + u)$$
therefore the function is not linear, as desired.

\subsection{}
\textit{Suppose $V$ is finite-dimentional. Prove that every linear map on a subspace of $V$
  can be extended to a lineaer map on $V$. In other words, show that if $U$ is a subspace of $V$
  and $S$ is a subspace of $V$ and $S = L(V, W)$, then there exists $T \in L(V, W)$ such that
  $Tu = Su$ for all $u \in U$.}

Because $V$ is finite-dimentional and $U$ is a subspace of $V$, we can follow that $U$
is finite-dimentional as well. Thus we can follow that there exists
$u_1, ..., u_m$ - basis of $U$. As we know, we can extend this basis to a basis of $V$ -
$u_1, ..., u_m, v_1, ... v_n$. Therefore we can define a map $P \in L(V, U)$ by 
$$P(x_1, x_2, ...) = (x_1, x_2, ... x_m, 0, 0, ...)$$
(basically trim every element of basis that is not in $U$). Thus we can follow that
$P(u) = u$ if $u \in U$. Proof that $P$ is linear is trivial.
Thus if $S \in L(U, W)$, then $T = SP \in (V, W)$ with the
desired properties.

\subsection{}

\textit{Suppose $V$ is finite-dimentional with $\dim V > 0$, and suppose $W$ is
  infinite-dimentional. Prove that $L(V, W)$ is infinite-dimentional.}

Let $v_1, ..., v_m$ be a basis of $V$ and
let $w_1, w_2, ...$ be a list of linearly independent vectors in $W$. Now
let us look at $T_n: V \to W$
$$T_n((x_1, x_2, ...) ) = x_1 w_n$$
Then it follows that by linear independence of $w_n$ there does not exist a linear
combination of $T_m$ such that
$$\sum_{m \neq n} a_m T_m \neq T_n$$
Thus we can follow that list $T_n$ is linearly independent.
Because list is not finite we can follow that the space $L(V, W)$ is infinite-dimentional,
as desired.

\subsection{}

\textit{Suppose $v_1, ..., v_m$ is a linearly dependent list of vectors in $V$. Suppose
  also that $W \neq \{0\}$. Prove that there exist $w_1, ... w_m \in W$ such that no
  $T \in L(V, W)$ satisfies $Tv_k = w_k$ for each $k = 1, ..., m$.}

Because $v_1, ..., v_m$ is linearly dependent we can reduce it to a linearly independent list
$v_1', ..., v_n'$. Thus resulting list will span some subspace of $V$ and will be its basis.

Thus we can take vector $v_j$ from the original list,
that does not appear in basis.
Then take some vectors $w_1, ... w_n$ in $W$. We know that there exists a unique map
$$Tv_n' = w_n$$
thus by adding to list $w_1, ... w_n$ any vectors from $W$, apart from $T(v_j)$ we create desired
list.

\subsection{}
\textit{Suppose $V$ is finite-dimentional with $\dim V \geq 2$. Prove that there
  exists $S, T, \in L(V, V)$ such that $ST \neq TS$ }

Let $v_1, v_2$ be a basis of $V$ and let
$$ S(x, y) = (y, x)$$
$$ T(x, y) = (x, 0)$$
Then
$$ST = (0, x)$$
and
$$TS = (y, 0)$$
as desired.


\section{Null Spaces and Ranges}

\subsection{}

\textit{Give an example of a linear map $T$ such that $\dim null T = 3$ and
  $\dim range T = 2$.}

$T(x, y, z) = (x, y)$

\subsection{}

\textit{Suppose $V$ is a vector space and $S, T \in L(V, V)$ are such that }
$$range S \subset null T$$
\textit{Prove that $(ST)^2 = 0$.}

Let $v \in V$. Then it follows that $S(T(v)) \in range S$. Thus $ST(v) \in null T$.
Therefore $TST(v) = 0$. And thus  $STST = (ST)^2 = 0$, as desired.

\subsection{}
\textit{Suppose $v_1, ...,  v_m$ is a list of vectors in $V$. Define $T \in L(F^m, V)$ by}
$$T(z_1, ..., z_m) = z_1 v_1 + ... + z_m v_m$$

\textit{(a) What property of $T$ corresponds to $v_1, ..., v_m$ spanning $V$?}

Surjectivity

\textit{(b) What property of $T$ corresponds to $v_1, ..., v_m$ being linearly independent?}

Injectivity

\subsection{}

\textit{Show that }
$$ \{T \in L(R^5, R^4): \dim null T > 2 \}$$
\textit{is not a subspace of $L(R^5, R^4)$.}

We can set

$$T_1(x, y, z, w, q) = (x, 0, 0, 0)$$
$$T_2(x, y, z, w, q) = (0, y, 0, 0)$$
$$T_3(x, y, z, w, q) = (0, 0, z, 0)$$
$$T_4(x, y, z, w, q) = (0, 0, 0, w)$$

all of which are in the desired subset, but their sum is
$$T(x, y, z, w, q) = (x, y, z, w, 0)$$
which has $\dim null = 1$. Thus this subset is not closed under addition and therefore it
is not a subspace.

\subsection{}

\textit{Give an example of a linear map $T: R^4 \to R^4$ such that }
$$range T = null T$$

$$T(x, y, z, w) = (z, w, 0, 0)$$.

\subsection{}

\textit{Prove that there does not exist a linear map $T R^5 \to R^5$ such that }
$$range T = null T$$

$\dim$ is always an integer, therefore for $\dim range T = \dim null T = n$ and
$$\dim T = 2n = 5$$
which is impossible.

\subsection{}

\textit{Suppose $V$ and $W$ are finite-dimentional with $2 \leq \dim V \leq \dim W$.
  Show that $\{T \in L(V, W): T \text{ is not injective}\}$ is not a subspace of $L(V, W)$.}

Suppose that $v_1, ..., v_m$ is a basis for $V$ and $w_1, ..., w_n$ is a basis of $W$.
We can follow that there exist, which maps $v_1$ to $w_1$ and so on. By adding all of
those maps together we get an injective map. Thus we can follow that given set is not
closed under addition and therefore is not a subspace.

\subsection{}

\textit{Suppose $V$ and $W$ are finite-dimentional with $2 \leq \dim W \leq \dim V$.
  Show that $\{T \in L(V, W): T \text{ is not surjective}\}$ is not a subspace of $L(V, W)$.}

By following the simular logic as in previous exercise, we get a desired result.

\subsection{}

\textit{Suppose $T \in L(V, W)$ is injective and $v_1, ..., v_n$ is linearly independent
  in $V$. Prove that $Tv_1, ..., Tv_n$ is linearly independent in $W$.}

Suppose that it is not the case. Then it follows that there exists $a_1, ... a_n \in F$ such
that not all of them are equal to zero and 
$$\sum a_n T v_n = 0$$
Thus we can follow that
$$T \sum a_n v_n = 0$$
Thus $\sum a_n v_n \in null T$. Because $T$ is injective we can follow that
$$\sum a_n v_n = 0$$
and some of $a_n$'s are not equal to zero. But $v_1, ..., v_n$ is linearly independent, thus
we get a contradiction.

\subsection{}

\textit{Suppose $v_1, ..., v_n$ spans $V$ and $T \in L(V, W)$. Prove that the list
  $T v_1, .... Tv_n$ spans range $T$.}

Suppose $w \in range T$. Thus we can follow that there exists $v \in V$ such that
$$Tv = w$$
Given that $v_1, ..., v_n$ spans $V$ we can follow that there exists $a_1, ... a_n$ such that
$$v = \sum a_n v_n$$
and thus
$$w = T \sum a_n v_n$$
$$w =  \sum T a_n v_n$$
thus we can follow that $v_1, ..., v_n$ spans the range of $T$, as desired.

\subsection{}
\textit{Suppose $S_1, ..., S_n$ are injective linear maps such that $S_1 S_2 ... S_n$ makes sense.
  Prove that $S_1 S_2 ... S_n$ is injective. }

Suppose that $T$ and $S$ are injective such that $ST$ makes sence. Suppose that
$$STv = 0$$
Then by injectivity of $S$ we get that $Tv \in null S$ and thus $Tv = 0$. Thus, by injectivity of
$T$ we get that $v$ = 0. Therefore $null ST = 0$. Therefore $ST$ is injective.

The case in the exercise is derived from induction on presented argument.

\subsection{}

\textit{Suppose that $V$ is finite-dimentional and that $T \in L(V, W)$. Prove that there exists
  a subspace $U$ of $V$ such that $U \cap null T = 0$ and $range T = \{Tu: u \in U\}$.}

Let $N$ be a nullspace of $T$. It follows that it is a subspace of $V$. Now let
$n_1, ..., n_m$ be a basis of $N$ and extend it to a basis of $V$: $n_1, ..., n_m, v_1, ..., v_n$.
Then if follows that $span(v_1, ... v_n) \cap N = 0$ (because otherwise the vector is in nullspace)
and if $w \in rangeT$, then there exists $u \in span(v_1, ... v_n)$ such that
$Tu = w$. Thus $span(v_1, ... v_n)$ is the desired subspace.

\subsection{}

\textit{Suppose $T$ is a linear map from $F^4$ to $F^2$ such that}
$$null T = \{(x_1, x_2, x_3, x_4) \in F^4: x_1 = 5x_2, x_3 = 7x_4\}$$
\textit{Prove that $T$ is surjective.}

$\dim null T = 2$, thus $\dim range T = 2$, therefore $T$ is surjective, as desired.

\subsection{}
\textit{Suppose $U$ is a 3-dimentional subspace of $R^8$ and that $T$ is a linear map from
  $R^8$ to $R^5$ such that $null T = U$. Prove that $T$ is surjcetive.}

We can follow that $\dim range T = 5$, and therefore $T$ is surjective, as desired.

\subsection{}

Very similar to previous one

\subsection{}

Same

\subsection{}

Same

\subsection{}

Same

\subsection{}

Same

\subsection{}

\textit{Suppose $W$ is finite-dimentional and $T \in L(V, W)$. Prove that $T$ is injective
  if and only if there exists $S \in L(W, V)$ such that $ST$ is the identity map on $V$.}

I don't know why it isn't stated explicitly, but by existence of injective $T$ we can
follow that $\dim V \leq \dim W$, and thus $V$ is finite-dimentional.

\textbf{In forward  direction}:

Suppose that $T$ is injective. Now let $v_1, ..., v_m$ be a basis of $V$. Then we can follow
that $Tv_1, ..., Tv_n$ is a basis of range $T$. Thus, extend this basis to a basis of $W$:
$Tv_1, ..., Tv_n, w_1, ..., w_m$. Now let us define $S \in L(W, V)$ such that
$$S Tv_n = v_n$$ 
$$S w_n = 0$$
Which will exist, and by the way, will be unique
because we're pairing basis of $W$ with a list of vectors in $V$. Thus we
can follow that if $v \in V$ then
$$ST v = ST \sum a_n v_n = S \sum T a_n v_n = S \sum a_n T  v_n = \sum a_n v_n = v$$
thus $ST = I$, as desired.

\textbf{In reverse dierction: }

Suppose that there exists $S \in L(W, V)$ such that $ST$ is an identity map on $V$. Suppose that
$T$ is not injective.  Then we follow that $null T \neq 0$.
Then let $v_1 \in null T \neq 0$. Then we can follow that
$$ST v_1 = S(T v_1) = S(0) = 0 \neq Iv_1$$
which is a contradiction. Thus we can conclude that $T$ is injective, as desired.

\subsection{}

\textit{Suppose $W$ is finite-dimentional and $T \in L(V, W)$. Prove that $T$ is surjective
  if and only if there exists $S \in L(W, V)$ such that $TS$ is the identity map on $W$.}

\textbf{In forward direction:}

Suppose that $T$ is surjective and let $w_1, ..., w_n$ be a basis of $W$. Then we can follow
that there exists $v_1, ..., v_m$ such that $T v_1 = w_1, ... T v_m = w_m$.
Thus we can follow that there exists a map in $L(W, V)$ such
that it maps
$$S w_1 = v_1$$
$$S w_n = v_n$$
Thus if $w \in W$, then we can follow that
$$TS w = TW(\sum a_n w_n) = T(\sum a_n W  w_n) =  T(\sum a_n v_n) =  \sum a_n T v_n =
\sum a_n w_n = w$$
for every $w \in W$. Thus we can follow that $TS = I$, as desired.

\textbf{In reverse direction:}

Suppose that there exists a map $S \in L(W, V)$ such that $TS$ is an identity map on $W$. 

Suppose now that $T$ is not surjective. Then we can follow that there exists $w \in W$ such that
there is no $v \in V$ such that $Tv = w$. But we've got that
$$TS w = T(Sw) = w$$
thus we've got a contradiction.

\subsection{}

\textit{Suppose $U$ and $V$ are finite-dimentional vector spaces and $S \in L(V, W)$ andd
  $T \in L(U, V)$. Prove that }
$$\dim null ST \leq \dim null S + \dim null T.$$

We know that if $T$ maps a vector to zero, then $STv = S(Tv) = S0 = 0$. Thus we can follow that
$$\text{null } T \subseteq \text{null } ST$$
Suppose that $STv = 0$. Then we can follow that $Tv \in null S$. Thus $ null ST $ exhaustively
decomposes into two sets: $null T$ and $\{u \in U : Tu \in range T \cap null S\}$. We know that
$$\dim (range T \cap null S) \leq \dim null S$$.
thus we can follow that
$$\dim null ST = \dim null T + \dim (range T \cap null S) \leq   \dim null S + \dim null T$$
as desired.

\subsection{}

\textit{Suppsoe $U$ and $V$ are finite-dimentional vector spaces and $S \in L(V, W)$ and
  $T \in L(U, V)$. Prove that}
$$\dim range ST \leq \min\{\dim range S, \dim range T\}$$

Given that $range ST \subseteq range S$ we can follow that
$$\dim range ST \leq \dim range S$$
Suppose that $U'$ is a preimage of range of $ST$. Then we can follow that if  $u' \in U'$, then
$u'$ is also in preimage of range of $T$. Thus we can follow that preimage of $ST$ is
a subset of preimage of $T$, and thus
$$\dim range ST \leq \dim range T$$
Because both equations must hold, in follows that  we  get out desired inequality.

\subsection{}
\textit{Suppose $W$ is finite-dimentional and $T_1, T_2 \in L(V, W)$. Prove that
  $null T_1 \subset null T_2$ if and only if there exists $S \in L(W, W)$ such that
  $T_2 = ST_1$.}

Firstly I should state that proposition in the exercise holds if we state that
$\subset$ does not donote a proper subset, but a regular subset. 

\textbf{In forward direction}: 
Suppose $null T_1 \subset null T_2$. This implies that $\dim range T_1 \geq \dim range T_2$.
Let $v_1, ..., v_n, u_1, ... u_n, r_1, ..., r_n$ be a basis of $V$ such that
$r_1, ... r_n$ is a basis of $null T_1$,  $u_1, ..., u_n, r_1, ... r_n$ is a basis of $T_2$.
Then we can follow that
$T_2 v_1, ..., T_2 v_n$ is a basis of range of $T_2$ and $T_1 v_1, ... T_1 v_n$ is a
basis of a subspace of range of $T_1$.
Thus we can create a map $S: W \to W$ such that
$S T_1 v_n = T_2 v_n$
Suppose that $v \in V$. Then it follows that
$$S T_1 v = S T_1 \sum a_n v_n =   \sum a_n ST_1 v_n = \sum a_n T_2 v_n = T_2 v$$
Thus we get our desired result.

\textbf{In reverse direction:}
Suppose that there exists $S \in L(W, W)$ such that $T_2 = S T_1$. Suppose that $v \in null T_1$.
Thus $T_1 v = 0 = S T_1 v = T_2 v$. Thus $v \in  null T_2$. Therefore $null T_1 \subset \null T_2$,
as desired.

\subsection{}

\textit{Suppose $W$ is finite-dimentional and $T_1, T_2 \in L(V, W)$. Prove that
  $range T_1 \subset range T_2$ if and only if there exists $S \in L(V, V)$ such that
  $T_2 = T_1S $.}

\textbf{In forward direction:}

Suppose that $range T_1 \subset range T_2$. Then let $q_1, ... q_n$ be a basis of range of $T_1$.
Thus we can extend it to be a basis of range of $T_2$ be adding $w_1, ..., w_m, q_1, ..., q_n$.
Thus we can follow that there exist $v_1, ... v_k \in V$ such that
$$T_1 v_n = w_n$$
and $v_1', ..., v_k' \in V$ such that
$$T_2 v_n' = w_n$$
Thus we can create a map $S \in L(V, V)$ such that
$$S v_n = v_n'$$
and thus
$$T_2 S v = T_2 S \sum a_n v_n = T_2  \sum a_n S v_n = T_2  \sum a_n  v_n'  = \sum a_n T_1 v_n 
=T_1 \sum a_n  v_n = T_1 v$$
as desired.

\textbf{In reverse direction: }

Suppose that there exists $S$ such that $T_1 = T_2 S$. Then it follows that if $u \in range T_1$,
then $u \in T_2$ as well. Thus $range T_1 \subset T_2$, as desired.


\subsection{}

\textit{Suppose $D \in L(P(R), P(R))$ is such that $\deg Dp = (deg p) -1$ for every nonconstant
  polynomial $p \in P(R)$. Prove that $D$ is surjective.}

Let us define a list of polynomials $p_n$ such that $\deg(p_n) = n$. Then it follows that
the list $D(p_n)$ is a list of polynomials such that $\deg (D(p_n)) = n - 1$, thus it
spans the space of polynomials. Thus we can follow that $D$ is surjective.

\subsection{}
\textit{Suppose $p \in P(R)$. Prove that there exists a polynomial $q \in P(R)$ such that
  $5q'' + 3q' = p$.}

By the exercise above we can state that differentiation is surjective. Thus double differentiation
is also surjective. Thus there exists $k \in P(R)$ such that $q'' = k'$, therefore
$5q'' = 5k'$. Thus by surjectivity of differentiation we've got the desired result.

\subsection{}
\textit{Suppose $T \in L(V, W)$. and $w_1, ..., w_m$ is a basis of range $T$. Prove that there
  exist $\phi_1, ... \phi_m \in L(V, F)$ such that }
$$T(v) = \phi_1(v) w_1 + ... \phi_n(v) w_n$$
\textit{for every $v \in V$}

Suppose that $v_1, ..., v_n$ is a basis of $V$. It follows that we can get coefficients
$$T v_j = A_{j,1} w_1 + ... + A_{j,n} w_n$$
thus if we set
$$\phi_j(v) = \phi_j(\sum a_n v_n) = \sum a_j A_{j, n}$$
then we get that
$$T v = T \sum a_n v_n = \sum a_n T v_n = \sum {a_n \sum A_{n, j} w_j} =
\sum {\sum a_n A_{n, j} w_j} = \sum {\phi_n(v) w_n} $$\
as desired.

\subsection{}

\textit{Suppose $\phi \in L(V, F)$. Suppose $u \in B$ is not in null $\phi$. Prove that}
$$V = null \phi \oplus \{au: a \in F\}$$

$\phi$ maps into a space of dimention one. Thus we can follow that its range is either
1 or 0. In this case there exists $u \in V$, such that it is not in null space of $\phi$,
therefore we can follow that $\dim range \phi = 1$. Thus the space, that is not in $null T$
has dimention $1$. Thus we can follow that this space is scalar multiples of $u$. Therefore
$$null \phi + \{au: a \in F\} = V$$
because $u \notin null \phi$ we follow that 
$$null \phi \cap \{au: a \in F\} = 0$$
and thus we can state that
$$null \phi \oplus \{au: a \in F\} = V$$
as desired.

\subsection{}

\textit{Suppose $\phi_1$ and $\phi_2$ are linear maps from $V$ to $F$ that have the same
  null space. Show that there exists a constant $c \in F$ such that $\phi_1 = c \phi_2$.}

If $\dim range \phi = 0$, then the case is trivial. Thus let us assume that $\dim range \phi = 1$.
Because they have the same null space we can follow that they have the same preimage of
the range. Thus we follow that if $v_1, ..., v_n$ is a basis of nullspace, then
$v_1, ..., v_n, w$ is a basis of $V$, therefore $w$ is a basis of a preimage. Thus
$$\phi_1 v = a_{n + 1}  \phi_1 w = a_{n + 1}c_1 $$
$$\phi_2 v = a_{n + 1}  \phi_2 w = a_{n + 1}c_2 $$
thus
$$\phi_1 = c_2 / c_1 \phi_2$$
as desired.

\subsection{}

\textit{Give an example of two linear maps $T_1$ and $T_2$ from $R^5$ to $R^2$ that have the same
  null space but are such that $T_1$ is not a scalar multiple of $T_2$}

$$T_1(x, y, z, w, q) = (x, y)$$
$$T_2(x, y, z, w, q) = (y, x)$$

\section{Matrices}

\subsection{}

\textit{Suppose $V$ and $W$ are finite-dimentional and $T \in L(V, W)$. Show that with respect
  to each choice of bases of $V$ and $W$, the matrix of $T$ has at least $\dim range T$ nonzero
  entries.}

Suppose that we've there exists a choice of bases of $V$ and $W$, such that matrix of this
linear map has less nonzero entries, then $\dim range T$. Then it follows, that
$\dim range T$ is spanned by list of vectors, that has length less than $\dim range T$,
which is impossible.


\subsection{}

\textit{Suppose $D \in L(P_3(R), P_2(R))$ is the differentiation map defined by $Dp = p'$. Find
  a basis of $P_3(R)$ and a basis of $P_2(R)$ such that the matrix of $D$ with resprect to these
  bases is }
$$
A =
\begin{pmatrix}
  1 & 0 & 0 & 0 \\
  0 & 1 & 0 & 0 \\
  0 & 0 & 1 & 0 \\
\end{pmatrix}
$$

I think that we can use standart basis for $P_3(R)$, and for $P_2(R)$ we gotta use
basis $1, 2x, 3x^2$.

\subsection{}

\textit{Suppose $V$ and $W$ are finite-dimentional and $T \in L(V, W)$. Prove that there exist
  a basis of $V$ and a basis of $W$. such that with respect to these bases, all entries of $M(T)$
  are 0 except that the entries in row $j$, column $j$, equal $1$ for
  $1 \leq j \leq \dim range T$.}

We can create a basis out of preimage of range of $T$. Thus if we set $v_1, ..., v_n$ to be a
basis of preimage and $T v_1, ..., T v_n$ to be the basis of range. Thus if we extend those
lists to be a bases of $V$ and $W$ respectively, we get the desired result.

\subsection{}

\textit{Suppose $v_1, ..., v_m$ is a basis of $V$ and $W$ is finite-dimentional. Suppose
  $T \in L(V, W)$. Prove that there exists a basis $w_1, ..., w_n$ of $W$ such that
  all the entries in the first column of $M(T)$ (with respect to the bases $v_1, ..., v_m$ and
  $w_1, ..., w_m$) are 0 except for possibly a 1 in the first row, first column.}

We can plug in $v_1$ into $T$ to get $T v_1$. If $T v_1 = 0$, then $v_1$ is in the nullspace
and any basis will do. Otherwise we can extend $T v_1$ to a basis of $W$ and get the desired
result.

\subsection{}

\textit{Suppose $w_1, ..., w_n$ is a basis of $W$ and $V$ is finite-dimentional. Suppose
  $T \in L(V, W)$. Prove that there exists a basis $v_1, ..., v_m$ of $V$ such that
  all the entries in the first row of $M(T)$ (with respect to the bases $v_1, ..., v_m$ and
  $w_1, ..., w_n$) are 0 except for possibly a 1 in the first row, first column.}

Suppose that we've got a random basis $v_1, ..., v_n$  of $V$ and then
map it through $T$. Then pick a vector $v_j$ such that $T v_j = a_1 w_1 + ... + a_m w_m$
such that $a_1 \neq 0$. If there is no such vector, then we're set. Otherwise go
through all the other vecrors $v_k$ and look at the representation
$$v_k = a_1' w_1 + ... + a_m' w_m$$
and set 
$$v_k'  = v_k - b_n v_n$$
where $b_n$ satisfies $a_1/a_1'$ (or vice versa) such that $v_k'$ represented as
$$T v_k' = 0 w_1 + ... + a_m w_m$$
Thus by linear independence of $v_1, ..., v_n$ we've got that $v_j, ..., v_1', ..., v_n'$ is
also linearly independent. Then, by plugging this vector into a matrix in this order, we
get the desired result.

\subsection{}

\textit{Suppose $V$ and $W$ are finite-dimentional and $T = L(V, W)$. Prove that
  $\dim range T = 1$ if and only if there exist a basis of $V$ and a basis of $W$
  such that with respect to these bases, all entries of $M(T)$ equal 1.}

\textbf{In forward direction: }
Suppose that we've got $T$ and $\dim range T = 1$.
Suppose that $v_1, ..., v_n$ is the resulting basis of $V$. Thus we can follow that
$T v_1 = T v_2 = ... = T v_n = w \neq 0$. Thus we can follow that $w_1 + ... + w_m = w$ is
the basis of range of $T$.

Thus we can create a vector $v_1$ such that $T v_1 = w$, then expand it to a basis of
$V$ $v_1, ..., v_n$ and then it'll follow that $v_2, ..., v_n$ is a basis of a nullspace.
Thus we can make a list $v_1, v_2 + v_1, v_3 + v_1, ..., v_n + v_1$, that will also a
basis of $V$ and for it it'll follow that
$$T v_j' = T (v_j + v_1) = 0 + w = w$$
Thus the only thing that is left is to create a basis of $W$ such that
$$w_1 + ... + w_n = w$$
we can actually do it by expanding $w$ to a basis of $W$, and getting $w, w_1, ..., w_n$. Then
we can set the first vector to be $w - w_1 - w_2 - ... - w_n$ and we'll get the desired
property. Thus we can construct the bases such that we have the desired property.

\textbf{In reverse direction: }
Suppose that there exist a basis of $V$ and basis of $W$ such that all entries of $M(T)$  are
equal to 1. Then we can follow that if we plug any vector into $T$, then we'll get the
constant multiple of the vector $w_1 + ... + w_n$. Thus we can follow that
$\dim range T = 1$, as desired.

\subsection{}
\textit{Verify 3.36}

3.36 states that if $S, T \in L(V, W)$, then $M(S + T) = M(S) + M(T)$.

Suppose that $S, T \in L(V, W)$, $v_1, ..., v_n$ is a basis of $V$ and
$w_1, ..., w_m$ is a basis of $W$. Then we can follow that
values at $j$'th column of $M(S + T)$ are obtained through
$$(S + T)(v_j) =  S(v_j) + T(v_j) = (a_1 + a_1') w_1 + ... + (a_m + a_m')w_m$$
where $a_1, ... a_m$ will be numbers in $j$'th row of $M(S)$ and $a_1', ..., a_m'$ will
be numbers in $j$'th row of $M(T)$. Thus we can follow that $M(S + T) = M(S) + M(T)$.

\subsection{}

\textit{Verify 3.38}

3.38 states that if $\lambda \in F$ and $T \in L(V, W)$, then $M(\lambda T) = \lambda M(T)$
$$(\lambda T) v_j = \lambda a_1 w_1 + ... + \lambda a_m w_m =
\lambda (a_1 w_1 + ... + a_m w_m) = 
\lambda(T v_j)$$
thus by the same reasoning as in previous exercise we've got that $M(\lambda T) = \lambda M(T)$,
as desired.

\subsection{}

\textit{Verify 3.52}

Follows directrly from the definition.

\subsection{}

\textit{Suppose $A$ is an $m$-by-$n$ matrix and $C$ is an $n$-by-$p$ matrix. Prove that }
$$(AC)_{j, \cdot} = A_{j, \cdot}C$$
\textit{for $1 \leq j \leq m$. In other words, show that row $j$ of $AC$ equals (row $j$ of $A$)
  times $C$.}

$$(AC)_{j, k} = A_{j, \cdot} C_{k, \cdot}$$
thus
$$(AC)_{j, \cdot} = (A_{j, \cdot} C_{1, \cdot}, A_{j, \cdot} C_{2, \cdot}, ..., A_{j, \cdot} C_{k, \cdot}) =
A_{j, \cdot} C
$$

\subsection{}

\textit{Suppose $a = (a_1, ..., a_n)$ is a $1$-by-$n$ matrix and $C$ is an $n$-by-$p$ matrix.
  Prove that }
$$aC = a_1 C_{1, \cdot} + ... + a_n C_{n, \cdot}$$
\textit{In other words, show that $aC$ is a linear combination of the rows of $C$,
  with the scalars that multiply the rows coming from $a$.}

This follows directly from a definition of matrix multiplication.

\subsection{}

\textit{Give an example with 2-by-2 matrices to show that matrix multiplication is not
  commutative. In other words, find 2-by-2 matrices $A$ and $C$ such that
  $AC \neq CA$}

$$A =
\begin{pmatrix}
  1 & 0 \\
  0 & 0
\end{pmatrix}
$$

$$C =
\begin{pmatrix}
  0 & 1 \\
  1 & 0
\end{pmatrix}
$$

$$AC =
\begin{pmatrix}
  0 & 1 \\
  0 & 0
\end{pmatrix}
$$

$$CA =
\begin{pmatrix}
  0 & 0 \\
  1 & 0
\end{pmatrix}
$$

\textit{The rest of the exercises are just basic applications of definitions, and equating them
  rigorously. Nothing interesting in there}

\section{Invertibility and Isomorphic Vector Spaces}

\subsection{}

\textit{Suppose $T \in \map (U, V)$ and $S \in \map (V, W)$ are both invertible linear maps.
  Prove that $ST \in \map(U, W)$ is invertible and that $(ST)^{-1} = T^{-1} S^{-1}$}

Let $u \in U$. Then
$$u = Iu = (T \inv T) u = T \inv (T u) = T \inv I (T u) = T \inv (S \inv S) (T u) =
(T \inv S \inv) (S T) u $$
thus we can follow that $(ST)$ is invertible and $(T \inv S \inv) = (ST) \inv$, as desired.

\subsection{}

\textit{Suppose $V$ is finite-dimentional and $\dim V > 1$. Prove that the set of
  noninvertible operators on $V$ is not a subspace of $\map(V)$.}

We can have
$$S_1(a_1 v_1 + ... a_n + v_n) = a_1 v_1 + ... a_j + v_j$$
$$S_2(a_1 v_1 + ... a_n + v_n) = a_{j + 1} v_{j + 1} + ... a_n + v_n$$
for some $1 \leq j < n$. (you can interpret it as an upper part of the identity and a lower
part). Both of them are non-invertible (by non-surjectivity), but their sum is the identity,
which is invertible. Thus the subset is not closed under addition and therefore it is not a
subspace.

\subsection{}

\textit{Suppose $V$ is finite-dimentional, $U$ is a subspace of $V$, and $S \in \map (U, V)$.
  Prove that there exists an invertible operator $T \in \map(V)$ such that $T(u) = S(u)$ for
  every $u \in U$ if and only if $S$ is injective.}

\textbf{In forward direction: }
Suppose that there exists such an operator and $S$ is not injective. Thus there exists
$u_1$ such that $u_1 \neq 0$ and $S u_1 = 0$. Thus $T u_1 = S u_1 = 0$, therefore
$T$ is not injective, therefore it is not invertible, which is a contradiction.

\textbf{In reverse direction: }
Suppose that $S$ is injective. Let $u_1, ..., u_n, v_1, ..., v_m$ be a basis of $V$ such
that $u_1, ..., u_n$ is a basis of $U$. Thus, by FTLM we've got that 
$$\dim U  = \dim \range S + \dim \ns S$$
Given that $S$ is injective, we can follow that $\dim \ns S = 0$. Thus
$$\dim U  = \dim \range S$$
Thus, because $\range S$ is a subspace,  we can create a basis of it
$u_1', ..., u_n'$, which will have the same length as the basis of $U$. By expanding this basis
to a basis of $V$ we can create $u_1', ..., u_n', v_1', ..., v_m'$.
Then if we map $u_1, ..., u_n, v_1, ..., v_m$ to $u_1', ..., u_n', v_1', ..., v_m'$, we'll
get $T \in \map(V)$, which  by uniqueness of representation that will have
$$Tu = Su$$
and because $u_1', ..., u_n', v_1', ..., v_m'$ is a basis of $V$ we'll get that $T$ is
surjective, and thus invertible, as desired.

\subsection{}

\textit{Suppose $W$ is finite-dimentional and $T_1, T_2 \in \map(V, W)$. Prove that
  $\ns T_1 = \ns T_2$ if and only if there exists an invertible operator $S \in \map(W)$ such that
  $T_1 = S T_2$.}

\textbf{In forward direction: }
Suppose that $\ns T_1 = \ns T_2$. Then we can follow that $\dim \range T_1 = \dim \range T_2$.
We can also follow that there exists $v_1, ... v_n$ - basis of preimage of $T_1$ and $T_2$.
Thus, $T_1 v_1, ... T_1 v_n$ is a basis of $\range T_1$ and $T_2 v_1, ..., T_2 v_n$ is a
basis of $\range T_2$. Thus we can create a unique map $S \in \map(\range T_2, \range T_1)$
$$S'(a_1 T_2 v_1 +  ... +  a_n T_2 v_n) = a_1 T_1 v_1 +  ... +  a_n T_1 v_n$$
Thus, if $v \in V$, then
$$S T_2 v = S (a_1 T_2 v_1 + ... + a_n T_2 v_n) = a_1 T_1 v_1 + ... + a_n T_1 v_n =
T_1(a_1 v_1 + ... + a_n v_n) = T_1 v$$

Given that $S'$ is injective and $S' \in (\range T_2, \range T_1) \to S' \in (\range T_2, W)$
(because $\range T_2 \subseteq W$) , we can follow by results of our previous exercise, that there
exists invertible  $S \in \map (W)$ such that
$$S(w) = S'(w)$$
and therefore
$$T_1 = S T_2$$
as desired.

\textbf{In reverse direction: }
Suppose that there exists invertible $S \in \map(W)$ such that $T_1 = S T_2$.
Thus if $v \in \ns T_1$, then we can follow that $S T_2 v = 0$. By invertability of $S$ we've got
that there exists $S \inv$ and therefore
$$S T_2 v = 0$$
$$S \inv S T_2 v = S \inv 0$$
$$T_2 v = 0$$
Thus, $v \in \ns T_2$. Therefore we can follow that $\ns T_1 \subseteq \ns T_2$. By the same
logic, but in other direction we've got that $\ns T_2 \subseteq \ns T_1$, thus
$$\ns T_1 = \ns T_2$$
as desired.


\subsection{}

\textit{Suppose that $V$ is finite-dimentional and $T_1, T_2 \in \map(V, W)$. Prove that
  $\range T_1 = \range T_2$ if and only if there exists an invertible operator $S \in \map (W)$
  such that $T_1 = T_2 S$.}

\textbf{In forward direction: }
Let $w_1, ... w_n$ be a basis of $\range T_1 = \range T_2$. Thus we can follow that there exists
basis $v_1, ... v_n \in V$ of preimage of $T_1$  and $v_1', ... v_n' \in V$  - basis of preimage
of $T_2$ such that $T_1 v_j = w_j = T_2 v_j'$.
Because those lists are linearly independent and have the same length, we
can follow that there exists an isomorphy $S \in \map (V)$ such that
$$S(a_1 v_1 + ... a_n v_n) = a_1 v_1' + ... a_n v_n'$$
Thus we can follow that for $v \in V$
$$T_2 S v = T_2 S (a_1 v_1 + ... a_n v_n) = a_1 w_1 + ... + a_n w_n = T_1 v$$
as desired.

\textbf{In reverse direction: }
Suppose that there exists $S \in \map (W)$ such that $T_1 = T_2 S$. Then it is obvious that
$\range T_1 = \range T_2$.

\subsection{}

\textit{Suppose that $V$ and $W$ are finite-dimentional and $T_1, T_2 \in \map (V, W)$. Prove that
  there exists invertible operators $R \in \map L(V)$ and $S \in L(W)$ such that
  $T_1 = S T_2 R$ if and only if $\dim \ns T_1 = \dim \ns T_2$.}

\textbf{In forward direction: }
By resullts of previous exercise we can follow that $\ns T_1 = \ns T_2 R$.
Thus by injectivity of $R$ we've got that $\dim \ns T_1 = \dim \ns T_2$.

\textbf{In reverse direction: }
Suppose that $\dim \ns T_1 = \dim \ns T_2$. We can follow that there exists isomorphism
$S \in \map (W)$ such that $\ns T_1 = \ns T_2 R$. Thus, by results of previous exercises we
can folllow that there exists $S \in \map (V)$ such that $T_1 = S T_2 R$, as desired.

\subsection{}

\textit{Suppose $V$ and $W$ are finite-dimentional. Let $v \in V$. Let }
$$E = \{T \in \map(V, W): Tv = 0 \}$$

\textit{(a) Show that $E$ is a subspace of $\map (V, W)$.}

Suppose that $S, T \in E$. Then for $v \in V$ we've got that 
$$(S + T)v = S(v) + T(v) = 0$$
thus $S + T \in E$ and $E$ is closed under addition.

Let $\lambda \in F$.  Then
$$(\lambda T)v = \lambda (T v) = \lambda 0 = 0$$
thus $(\lambda T) \in E$, therefore $E$ is closed under scalar multiplication. Given that
$\map (V, W)$ is a vector space we can follow that $E$ is a subspace, as desired.

\textit{(b) Suppose $v \neq 0$. What is $\dim E$?}

Extend $v$ to a basis $v, v_1 ... v_{n - 1}$  of $V$ and let $w_1, ..., w_n$ be
arbitrary basis of $W$. Because $Tv = 0$, we require that first column  of $M(T)$ will be
zeroes. Then we can follow that $\mathcal M$ is an
isomorphism between $E$ and $F^{m, (n - 1)}$. Thus
$$\dim E = (\dim V - 1)(\dim W)$$

\subsection{}

\textit{Suppose $V$ is finite-dimentional and $T: V \to W$ is a surjective linear map
  of $V$ onto $W$. Prove that there is a subspace $U$ of $W$ such that
  $T|_U$ is an isomorphism of $U$ onto $W$. }

Let $v_1, ..., v_n$ be a basis of $\ns V$. Then we can extend it to  $v_1, ..., v_n, u_1, ..., u_m$,
which will be a basis of $V$. Let $U = \Span(u_1, ..., u_m)$. We can follow by FTLM that
$\dim U = \dim \range T$. Also, because $U$ is surjective, we can follow that it is invertible
and therefore is isomorphism, as desired.

\subsection{}

\textit{Suppose $V$ is finite-dimentional and $S, T \in \map(V)$. Prove that $ST$ is invertible
  if and only if both $S$ and $T$ are invertible.}

\textbf{In forward direction: }

Suppose that $ST$ is invertible. Suppose that $T$ is not invertible. Then it is not injective.
Therefore theree exists $v \in V \neq 0$ such that $Tv = 0$. Therefore $STv = S0 = 0$, which
is a contradiction. Thus $T$ is injective and therefore invertible.

Suppose that $S$ is not invertible. Then we can follow that there exists $v \in V \neq 0$ such
that $Sv = 0$. Given that $T$ must be invertible we can follow that there exists $w \in V \neq 0$
such that $Tw = v$. Thus $STw = Sv = 0$. Therefore $ST$ is not injective, which is a contradiction.

Thus we can follow that in order for $ST$ to be invertible, both $S$ and $T$ must be invertible
as well, as desired.

\textbf{In reverse direction: }
Suppose that $S$ and $T$ are invertible. Then we can follow that both of them are injective.
Thus by some exercise 
in this chapter (looked it up, it's 3.2.11) $ST$ is injective as well. Thus $ST$ is invertible, as
desired.


\subsection{}

\textit{Suppose $V$ is finite0dimentional and $S, T \in \map(V)$. Prove that $ST = I$ if
  and only if $TS = I$.}

From previous exercise we can follow that both $S$ and $T$, as well as $ST$ and $TS$ are
invertible.

All of the following are equivalences and not  implications, therefore we can prove everything
in one go.
$$ST = I$$
$$STS = IS$$
$$STS = SI$$
$$TS = I$$

\subsection{}

\textit{Suppose $V$ is finite-dimentional and $S, T, U \in \map(V)$ and $STU = I$.
  Show that $T$ is invertible and that $T \inv = US$.}

Firstly I should state that we assume here that $S$ and $U$ are invertible, otherwise we default
to one of the previous exrecises. 

Suppose that $T$ is not invertible. Then it isn't injective, and therefore there exists
$v \in V \neq 0$ such that $Tv = 0$. Because $U$ is invertible we can follow that there exists
$w \in V \neq 0$ such that $Uw = v$. Thus
$$STUw = STv = S0 = 0 \neq w$$
Therefore $STU \neq I$, which is a contradiction.

Now we can use some algebra in here
$$STU = I$$
$$TU = S \inv I$$
$$TU = S \inv U \inv$$
$$T = S \inv U \inv$$
and by first exercise in this chapter
$$T \inv = (S \inv U \inv) \inv = US$$
as desired.

\subsection{}

\textit{Show that the result in the previous exercise can fail withoud the hypothesis that $V$
  is finite-dimentional}

We can set $U$ to be $I$,
$$T(x_1, x_2, ...) = (0, x_1, x_2, ...)$$
$$S(x_1, x_2, ...) = (x_2, x_3, x_4, ...)$$
and so on. Then $T$ will not be surjective, and therefore will not be invertible (which doesn't
follow from our usual equivalence, but by the fact that there does not exist a map such
that $TS = I$, because there is no way to represent (1, 1, ...))

\subsection{}

\textit{Suppose $V$ is a finite-dimentional vector space and $R, S, T \in \map(V)$. are
  such that $RST$ is surjective. Prove that $S$ is injective.}

Suppose that it isn't. We can follow that $RST$ is invertible.
Then we can follow that there exits $v \in V \neq 0$ such that
$Sv = 0$. By invertability of $T$ we follow that there exists $w \in V \neq 0$
such that $Tw = v$. thus
$$RSTw = RSv = R0 = 0$$
Thus $RST$ is not invertible and isn't surjective, which is a contradiction.

\subsection{}

\textit{Suppose $v_1, ..., v_n$ is a basis of $V$. Prove that the map $T: V \to F^{n, 1}$
  defined by }
$$Tv = \mathcal M (v)$$
\textit{is an isomorphism of $V$ onto $F^{n, 1}$.}

Only way to represent 0 in $M(v)$ is that if $v = 0$, therefore the map is injective. Also,
by common sense, map is surjective. Thus it is invertible and therefore it is an isomorphism.

\subsection{}

Trivial, seen simular in previous chapter

\subsection{}

\textit{Suppose $V$ is finite-dimentional and $T \in \map(V)$. Prove that $T$ is a scalar
  multiple of the identity if and only if $ST = TS$ for every $S \in \map (V)$.}

Forward direction is trivial.

Suppose that $ST = TS$ for every $S \in \map(V)$. Suppose that $Tv \neq \lambda v$ for some
$v \neq 0$ and $\lambda \in F$. Then we can follow that there exsits $S$ such that
$$S(Tv) = v$$
and
$$S(v) = 0$$
Thus
$$ S (Tv) = v \neq 0 = S(v) = T(Sv)$$

Thus we can follow that for every $v \in V$ there exists $a \in F$ such that $Tv = av$.

Now suppose that $v, w \in V$. If $v \neq \lambda w$, then
$$T(v + w) = a_{v + w}(v + w) = a_{v + w}v + a_{v + w}w = T(v) + T(w) = a_v v + a_w w$$
thus
$$a_v = a_w = a_{v + w}$$
by the unique representation of zero.

If $v = \lambda w$,  then
$$Tv = a_v v = T(\lambda w) = a_w \lambda w$$
$$a_v v =  a_w \lambda w$$
$$a_v v =  a_w v$$
thus $a_v = a_w$. Therefore for any given $v, w$ we follow that $a_v = a_w$. Thus, $T$ is a
scalar multiple of $I$, as desired. (Proof aquired after reading another proof in supplimentary
material).

\subsection{}

\textit{Suppose $V$ is finite-dimentional and $\mathcal E$ is a subspace of $\map(V)$ such
  that $ST \in  \mathcal E$ and $TS \in \mathcal E$ for all $S \in \map(V)$ and all
  $T \in \mathcal E$. Prove that $\mathcal E  = \{0\}$ or $\mathcal E = \map(V)$.}

It's trivial to proof that $\{0\}$ and $\map(V)$ are such supspaces. We also should
mention that in order for it not to be a trivial case we assume that $\dim V > 1$ (otherwise
we don't have any other subspaces other than $\map (V)$ and $\{0\}$).

Our strategy with this proof is to show that there exists an invertible map in $\mathcal E$.
If we've got that, then the rest of the proof is trivial.

Thus suppose that
$E \neq \{0\}$ and $E \neq \map(V)$. Then we can follow that there exists a map
$T \in E$ such that $T \neq 0$. Because we want to prove that there exists an invertible map
in $E$, suppose that $T$ is not invertible (otherwise $T T \inv = I \in \mathcal E$, and we
can skip to the later part).
Thus there exists a vector $v \in V \neq 0$ such that 
$Tv \neq 0$ and $w \in V \neq 0$ such that $Tw = 0$.
Therefore extend $v$ to a basis $v, v_1, ..., v_n$ of $V$ and make a map
$$S (v) = v$$.
$$S (v_k) = 0$$.
Then it follows that there exist maps $T S \in E_j$ such that
$$TS(a_0 v + a_1 v_1 + ... a_n v_n) = Ta_0 v = a_0 T v$$
Thus there exists a map 
$$Q_j (a_0 v + ... + a_n v_n) = a_j v_j$$
also in $\mathcal E$.
Thus we can follow that $I \in \mathcal E$. Thus we can follow that for every $S \in \map(V)$
we've got that
$$SI = S$$
is also in $\mathcal E$. Thus $\map(V) \subseteq \mathcal E$. Thus  we can follow that
$\map(V) = \mathcal E$, which is a contradiction.

Thus we can follow that $\mathcal E$ is either $\map(V)$ and $\{0\}$, as desired.

\subsection{}

\textit{Show that $V$ and $\map(F, V)$ are isomorphic vector spaces.}

For finite-dimentional spaces we've got that

$$\dim \map (F, V) = (\dim F)(\dim V) = 1 (\dim V) = \dim V$$

Thus they are isomorphic.

Otherwise we suppose that $v_1, ...$ is a basis of $V$. Given that
$$v = \sum a_j v_j$$
we can follow that we've got bijectivity between $\map(F, V)$ and $V$, as desired.

\subsection{}

\textit{Suppose $T \in \map(P(R))$ is such that $T$ is injective and $\deg Tp \leq \deg p$
  for every nonzero polynomial $p \in P(R)$.}

\textit{(a) Prove that $T$ is surjective.}

Because $T$ is injective, we can follow that $\ns T = \{0\}$ (looked it up, this one applies
to any vector space).

Now let's try to prove that $\deg Tp = \deg p$.
We probably don't need the induction here, but we'll use it anyways.

For $p = 0$ we've got that $Tp = 0$. Let us prove that this is the case also
for $\deg p = 0$, just in case. Suppose that $\deg p = 0$ and $\deg Tp < 0$, then $\ns T$ is
not equal to zero, which is a contradiction.

For inductive step let us assume that $\deg Tp = \deg p$ for $p$ such that  $\deg p = n - 1$.
Now suppose that $\deg p = n$ and $\deg Tp < n$. By our assumption,  it follows that there exists
a basis $p_0, ..., p_{n - 1}$ such that $\deg T p_j = j$. Thus we can follow that $T p_0, ... T p_n$
spans $P_{n - 1}(R)$. Thus we follow that there exists $a_0, ... a_{n - 1}$ such that
$$a_0 T p_0 + ... a_{n - 1} T p_{n - 1} = T p$$
then it follows that $T$ is not injective, which is a contradiction.

This concludes the proof that $\deg p = \deg Tp$. By this we can follow that
given $p \in P(R)$ with $\deg p = n$ there exists $p_0, ... p_n$ and by extension
$T p_0, ... Tp_n$ such that $p \in \range(T p_0, ... Tp_n)$. Thus there exists
$p' \in P(R)$ such that $Tp' = p$. Thus $T$ is surjective, as desired (this dragged
on for waaay too long).

\subsection{}

not gonna repeat the text of the exercise, but it basically reduces to our usual eqivalence
of surjectivity/injectivity on finite-dimentional operators.

\section{Products and Quotients of Vector Spaces}

\subsection{}

\textit{Suppose $T$ is a function from $V$ to $W$. The graph of $T$ is the subset of
  $V \times W$ defined by }
$$\text{graph of } T = \{(v, Tv) \in V \times W: v \in V\}$$
\textit{Prove that $T$ is a linear map if and only if the graph of $T$  is a subspace
  of $V \times W$.}

Firstly, $G(T)$ denotes the graph of $T$. 

\textbf{In forward direction: }
Suppose that $T$ is a linear map. Suppose that $v, w \in G(T)$. Then it follows that
$$v + w = (v + w, T(v + w))$$
Thus $G(T)$ is closed under addition.
$$\lambda v = (\lambda v, \lambda Tv)$$
thus it is also closed under scalar multiplication. Given that $G(T)$ is a subset of
a product of vector spaces, which is a vector space, we follow that $G(T)$ is a subspace,
as desired.

\textbf{In reverse direction: }

Suppose that $G(T)$ is a subspace of $V \times W$. Suppose that $v, w \in V$. Then
$$T(v + w) = T(v) + T(w)$$
and
$$\lambda T(v) = T(\lambda v)$$
by properties of a product of vector spaces. Thus we can follows that $T$ is linear, as desired.

\subsection{}

\textit{Suppose $V_1, ..., V_m$ are vector spaces such that $V_1 \times ... \times V_m$ is
  finite-dimentional. Prove that $V_j$ is finite-dimentional for each $j = 1, ..., m$.}

We can follow that the combined list of bases of $V_n$'s spans $V_1 \times ... \times V_m$
and is linearly independent. Given that this list is finite, we
can follow that  $V_1 \times ... \times V_m$ is finite-dimentional, as desired.


\subsection{}

\textit{Give an example of a vector space $V$ and subspaces $U_1, U_2$ of $V$ such that
  $U_1 \times U_2$ is isomorphic to $U_1 + U_2$, but $U_1 + U_2$ is not a direct sum.}

$$U_1 = (0, x_1, 0, x_2, 0, x_3, ...)$$
$$U_2 = (x_1, x_2, x_3, 0, x_4 ...)$$

\subsection{}

\textit{Suppose $V_1, ... V_m$ are vector spaces. Prove that $\map(V_1 \times ... \times V_m, W)$
  and $\map(V_1, W) \times ... \times \map(V_m, W)$ are isomorphic vector spaces.}

For finite-dimentional spaces we can just follow the standard equations for calculating the
dimentions of given spaces and produce the desired result.


Conversely, if any of the spaces is infinite-dimentional, then we gotta produce the bijective
map between the two spaces.


Our strategy here will be to prove that there exists a bijective map between the two.
Let $T: \map(V_1 \times ... \times V_m, W) \to \map(V_1, W) \times ... \times \map(V_m, W)$
be defined by

$$T(S) = S_1 \times ... \times S_n$$
where
$$S_j(v_j) = S(0, ..., v_j, .., 0) = w_j$$
thus
$$T(\lambda S) = \lambda S_1 \times ... \times \lambda S_n$$
and
$$T(S + M) = (S + M)_1 \times ... \times (S + M)_n = S_1 \times ... \times S_n +
M_1 \times ... \times M_n$$
thus it is linear.

Injectivity and surjectivity are given with this one. Thus we follow that there exists a linear
bijectivity between the two, thus they are isomorphic.

We can actually follow here that if the space is infinite-dimentional, then they are isomorhic
by default.

\subsection{}

\textit{Suppose $W_1, ... W_m$ are vector spaces. Prove that
  $\map(V, W_1 \times  ... W_n)$ and $\map(V, W_1) \times ... \times \map(V, W_n)$ are isomorphic }

Same logic as in previous ones works on this oen too.

\subsection{}

\textit{For a posisitivee integer $n$, define $V^n$ by}
$$V^n = V \times ...\text{ n times }... V$$
\textit{Prove that $V^n$ and $\map(F^n, V)$ are isomorphic vector spaces.}

$$\dim \map(F^n, V) = (\dim F^n) ( \dim V) =  n \dim V = \dim V^n$$
as desired.

\subsection{}

\textit{Suppose $v, x$ are vectors in $V$ and $U, W$ are subspaces of $V$ such that
  $v + U = x + W$. Prove that $U = W$.}

Let $v'= x - v$. It follows that $x = v + v'$. Given that $v + U = x + W$ we follow
that there exists $-w \in W$ such that $x - w = v + 0$. Thus
$$x - w = v + 0$$
$$v + v' - w = v$$
$$v' - w = 0$$
$$v' = w$$
Thus we follow that $v' \in W$. Therefore we've got that
$$x + W = v + v' + W = v + W = v + U$$
thus we follow that
$$W = U$$
as desired.

\subsection{}

\textit{Prove that a nonempty subset $A$ of $V$ is an affine subset of $V$ if and only if
  $\lambda v + (1 - \lambda)w \in A$ for all $v, w \in A$ and all $\lambda \in F$.}

\textbf{In forward direction: }

Suppose that $A$ is an affine subset of $V$. Then we follow that there exists vector
$x \in V$ and subspace $U \subseteq V$ such that
$$A = x + U$$
Now let $v, w \in A$. Then we can follow that there exist $v', w' \in U$ such
that
$$v = x + v'$$
$$w = x + w'$$

Thus we follow that

$$\lambda v + (1 - \lambda) w = \lambda x + \lambda v' + x + w' - \lambda x - \lambda w' =
\lambda v' + x + w' - \lambda w' $$
Because $v', w' \in U$ we follow that
$$\lambda v' + w' - \lambda w' \in U$$
thus
$$\lambda v' + x + w' - \lambda w' \in A$$
as desired.

\textbf{In reverse direction: }

Suppose that for every $\lambda \in F$ and $v, w \in A$ we've got that
$$\lambda v + (1 - \lambda) w \in A $$

Fix $x \in A$ and suppose that $v, w \in A$. Then it follows that
$$ \lambda(v  - x) = \lambda v  - \lambda x =
\lambda v  - \lambda x + x - x =
(\lambda v  - (1 - \lambda) x) - x
$$
Thus space $A - x$ is closed under scalar multiplication. And
$$ (v - x) + (w - x) = 2(v/2 + w/2 - x) = 2(\frac{1}{2} v + (1 - \frac 1 2) w - x) $$
by the fact that $v, w \in A \to \frac{1}{2} v + (1 - \frac 1 2) w \in A$ we follow that
$(\frac{1}{2} v + (1 - \frac 1 2) w - x) \in A - x$. Thus, by the fact that
$A - x$ is close under scalar multiplication we follow that
$$2(\frac{1}{2} v + (1 - \frac 1 2) w - x) \in A - x$$
thus we can conclude that $A - x$ is a subspace of $V$. Thus, $A - x + x = A$ is an affine
subset of $V$, as desired.

\subsection{}

\textit{Suppose $A_1$ and $A_2$ aare affine subsets of $V$. Prove that the intersection
  $A_1 \cap A_2$ is either an affine subset of $V$ or the empty set.}

Fisrtly, let us denote that
$$A_1 = x_1 + U_1$$
$$A_2 = x_2 + U_2$$

Firstly, if $A_1$ and $A_2$ are parallel, then they are either equal to each other (in which
case their intersection is an affine subset), or empty. Thus we can follow that
intersection of two affine subsets can be empty.

Suppose that their intersection is nonempty and let $x \in A_1 \cap A_2$. Then it follows
that $U_1 = A_1 - x$ and $U_2 = A_2 + x$. Intersection of two vector spaces is a vector
space, thus we follow that
$$x + U_1 \cap U_2 = A_1 \cap A_2$$
is an affine space, as desired.

\subsection{}

\textit{Prove that the intersection of every collection of affine subsets of $V$ is either
  an affine subset of $V$ or the empty set}

It follows by induction from the previous exercise.

\subsection{}

\textit{Suppose $v_1, ..., v_m \in V$. Let }
$$A = \{\lambda_1 v_1 + ... + \lambda_m v_m: \lambda_{[1, m]} \in F \text{ and }
\sum \lambda_j = 1\}$$

\textit{(a) Prove that $A$ is an affine subset of $V$.}

Let $v, m \in A$. Then it follows that
$$\kappa v + (1 - \kappa) w =
\kappa (\lambda_1 v_1 + ... + \lambda_m v_m) + (1 - \kappa)
(\lambda_1' v_1 + ... + \lambda_m' v_m) =
$$
$$=
\kappa (\lambda_1 v_1 + ... + \lambda_m v_m) + (1 - \kappa)
(\lambda_1' v_1 + ... + \lambda_m' v_m) = (\kappa \lambda_1 + (1 - \kappa) \lambda_1') v_1 + ...
(\kappa \lambda_n + (1 - \kappa) \lambda_n') v_n $$
Thus the sum of the coefficients is equal to
$$\sum{(\kappa \lambda_n + (1 - \kappa) \lambda_n')} =
\sum{(\kappa \lambda_n)} + \sum{[(1 - \kappa) \lambda_n']} =
\kappa \sum{( \lambda_n)} + (1 - \kappa) \sum{[ \lambda_n']} =
\kappa  + (1 - \kappa)  = 
$$
$$
= 1
$$
Thus we follow that $A$ is an affine subset by equivalence, that was proven in a couple
of exercises above.

\textit{(b) Prove that every affine subspae of $V$ that contains $v_1, ..., v_m$ also
  contains $A$}

Suppose that some affine subset $B$ contains $v_1, ..., v_m$.
Let $a \in A$. 

We know that there exists $U \subseteq V$ such that $v_1 + U = A$ and $W \in V$ such
that $B = v_1 + W$.
Thus we follow that $v_2 - v_1 \in B - v_1$, ..., $v_n - v_1 \in B - v_1$. Thus
$$\Span(v_2 - v_1, ... v_n - v_1) \subseteq B - v_1$$
Suppose that $a \in A$. Then
$$a - v_1 \in \Span(v_2 - v_1, ... v_n - v_1)$$
Thus
$$a - v_1 \in B$$
as desired.

\textit{(c) Prove that $A = v + U$ for some $v \in V$ and some subspace $U$ of $V$
  with $\dim U \leq m - 1$}

We know, that the list $(v_2 - v_1, ... v_m - v_1)$ spans $A - v_1$. Therefore we can follow
that $\dim U \leq m - 1$.

\subsection{}

\textit{Suppose $U$ is a subspace of $V$ such that $V/U$ is finite-dimentional. Prove that
  $V$ is isomorphic to $U \times (V / U)$.}

For finite-dimentional case is trivial, for infinite-dimentional we have isomorphism by default.

\subsection{}

\textit{Suppose $U$ is a subspace of $V$ and $v_1 + U, ..., v_m + U$ is a basis of $V/U$
  and $u_1, ..., u_n$ is a basis of $U$. Prove that $v_1, ..., v_m, u_1, ..., u_m$ is a basis
  of $V$.}

Because $v_1 + U, ..., v_m + U$ is a basis of $V/U$, we can follow that $v_1, ..., v_m$ is
linearly independent in $V$.

If $v_j \in U$, then $v_j + U = U = 0 + U$, therefore we follow that $v_j \notin U$.

Thus we can follows that $v_1, ..., v_n, u_1, ..., u_m$ is a linearly independent list of
vectors.

Given that $v_1, ..., v_n, u_1, ..., u_m$ is a list of vectors in $V$, we can follow that
$\Span(v_1, ..., v_n, u_1, ..., u_m)$ is a subspace of $V$.

Now suppose that $v \in V$ and $v \notin \Span(v_1, ..., v_n, u_1, ..., u_m)$. We can follow that
$v \notin U$. Thus we can follow that $v + U$ is an element of $V/U$. In this case we
follow that $v \in \Span(v_1, ..., v_n)$, which is a contradiction. Thus we conclude that
such a vector does not exist and therefore $V = \Span(v_1, ..., v_n, u_1, ..., u_m)$.
Given that $v_1, ..., v_n, u_1, ..., u_m$ is linearly independent, we follow that
it is a basis of $V$, as desired.

\subsection{}

\textit{Suppose
  $U = \{(x_1, x_2, ...) \in F^{\infty}: x_j \neq 0 \text{ for only finitely many }j\}$ }

\textit{(a) Show that $U$ is a subspace of $F^{\infty}$}

Let $v, u \in U$. We can follow, that sinse finitely many x's are not zero, then we
follow that for $v + u$
$$\{j_1, ..., j_n\} \cup \{j_1', ..., j_n'\} $$
is a set of positions, in which the $v + u$ might not be zero. Since union of finite sets is
finite, we follow that $U$ is closed undeer addition. The same reasoning, but applied to
intersection, rather then union, can be applied to  get closure for multiplication.
Thus we follow that $U$ is a subspace of $F^\infty$.

\textit{(b) Prove that $F^\infty/U$ is infinite-dimentional}

We can follow that there exist
$$x_j = (0, 0, 0, ...\text{ j times }... 0, 1, 1, 1...)$$
such that each $x_j$ is linearly independent from one another. Since $x_j \notin U$, we can
follow that there is no basis of $F^\infty/U$, therefore it is infinite-dimentional, as desired.


\subsection{}

\textit{Suppose $\phi \in \map(V, F)$ and $\phi \neq 0$. Prove that $\dim V / (\ns \phi) = 1$}

Because $\phi \neq 0$, we can follow that there exists $v \in V$ such that
$$\phi v \neq 0$$
thus we follow that $\dim \range \phi = 1$. Given that $V / \ns \phi$ is isomorphic to
$\range \phi$, we follow that its dimention is also one, as desired.

\subsection{}

\textit{Suppose $U$ is a subspace of $V$ such that $\dim V/U = 1$. Prove that there exists
  $\phi \in \map(V, F)$ such that $\null \phi = U$.}

Given that $\dim V/U = 1$ we can follow that there exists $v \in V$ such that $v \notin U$.
Thus $v$ is a basis for $V/U$. Thus we can define $g: V/U \to F$
$$g(kv + U) = k$$

Then, by plugging $\pi: V \to V/U$ such that  $\pi(v) = v + U$ and
making
$$\phi(v) = g(\pi(v))$$
we get $\phi: V \to F$ such that $u \in U \to \phi(u) = 0$. Thus $U \subseteq \ns \phi$.

Then suppose that $v \neq 0$ and $v \notin U$. Then it follows that $\phi(v) \neq 0$. Thus
we can conclude that $\ns \phi = U$, as desired.

\subsection{}

\textit{Suppose $U$ is a subspace of $V$ such that $V/U$ is finite-dimentional. Prove that
  there exists a subspace $W$ of $V$ such that $\dim W = \dim V/U$ and
  $V = U \oplus W$.}

Because $V/U$ is finite-dimentional, we can follow that there exists a basis of $V$.
$$v_1 + U, ..., v_n + U$$
We can follow that $v_1, ..., v_n$ is linearly independent. It is also will be helpful
later to mention that none of $v_j$'s are in $U$. Then, define
$$W = \Span(v_1, ..., v_n)$$

Suppose that $v \in V$. Then we can follow that $v \in v' + U$ for some $v$.
Thus we follow that $v' = a_1 v_1 + ... a_n v_n$ and therefore
there exists $u \in U$ and $v' \in W$ such that $v = u + v'$. Thus $V = W + U$.

Suppose that $w \in W$ and $w \in U$. Then it follows that
$$w = a_1 v_1 + ... a_n v_n$$
given that none of  $v_1, ... v_n$ are in $U$, we follow that the only way that it is
possible is when $a_1, ..., a_n = 0$. Thus we follow that
$$W \cap U = \{0\}$$.
Thus we can conclude that
$$V = W \oplus V$$
as desired.




\subsection{}

\textit{Suppose $T \in \map(V, W)$ and $U$ is a subspace of $V$. Let $\pi$ denote the
  quotient map from $V$ onto $V/U$. Prove that there exists $S \in (V/U, W)$
  such that $T = S \circ \pi$ if and only if $U \subset \ns T$. }

\textbf{In forward direction: }

Suppose that there exists $S$ such that $T = S \circ \pi$.
Let $u \in U$. Then it follows that
$$\pi(u) = u + U = 0 + U$$

Because $S$ is a linear function, we follow that $S(0) = S(U) = 0$. Thus we follow that
$u \in \ns T$. Thus we can conclude that $U \subseteq T$

\textbf{In reverse direction: }

Suppose that $U \subseteq \ns T$. Then we follow that $\pi(U) = 0$. Create a map
$S': V/U \to V/U$ such that $\ns (S' \circ \pi ) = \ns T$. Then there exists
an invertible operator $R$ such that $R S' \circ \pi = T$. Thus we follow that
there exists a map $RS' = S$ such that $S \circ \pi = T$, as desired.

\subsection{}

\textit{Find a correct statement analogous to 3.78 that is applicable to finite sets, with
  unions analogous to sums of subspaaces and disjoint unions analagous to direct sums }

Suppose $A$ is a finite set and $U_1, ..., U_m$ are subsets of $A$. Then
$$U_1 \cap ... \cap U_n = A$$
is a disjoint union if and only if
$$\sum |U_j| = |A|$$

\subsection{}

\textit{Suppose $U$ is a subspace of $V$. Define $\Gamma: \map(V/U, W) \to \map(V, W)$ by}
$$\Gamma(S) = S \circ \pi$$

\textit{(a) Show that $\Gamma$ is a linear map}

Let $S, T \in \map(V/U, W)$. Then
$$\Gamma(S + T) = (S + T) \circ \pi = S \circ \pi + T \circ \pi = \Gamma(S) + \Gamma(T)$$

If $\lambda \in F$, them
$$\Gamma(\lambda S) = (\lambda S) \circ \pi = \lambda S \circ \pi = \lambda \Gamma(S)$$
Therefore we can follow that $\Gamma$  is linear

\textit{(b) Show that $\Gamma$ is injective}

Let $\Gamma(T) = 0$. It follows that
$$T \circ \pi = 0$$
Then we can follow that for $v \in V$
$$T \circ \pi(v) = 0 = T(v + U)$$
Thus $T = 0$. Therefore we follow that $\ns \Gamma = \{0\}$, therefore it is injective.

\textit{(c) Show that $\range \Gamma = \{T \in \map(V, W): Tu = 0 \text{ for every } u \in U \}$}

Suppose that $T \in \range \Gamma$. Then it follows that there exists $S \in \map(V/U, W)$
such that
$$S \circ \pi = T$$
thus if $u \in U$ then
$$S \circ \pi (u) = S(u + U) = S(U) = 0$$

Suppose that $S \in \{T \in \map(V, W): Tu = 0 \text{ for every } u \in U \}$. Then
it follows by the results in exercise 18, that there exists $T \in \map(V/U, W)$ such
that
$$T = S \circ \pi$$
therefore by double inclusion we've got that 
$\range \Gamma = \{T \in \map(V, W): Tu = 0 \text{ for every } u \in U \}$
as desired.


\section{Duality}

TBD

\chapter{Polynomials}




\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
